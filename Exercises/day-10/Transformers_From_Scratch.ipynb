{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4rPQ573DD1/7pIsGd5Jdi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/appliedcode/mthree-c422/blob/mthree-422-salleh/Exercises/day-10/Transformers_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem Statement: Sentiment Classification on IMDb Movie Reviews Using BERT\n",
        "Objective\n",
        "\n",
        "Build and fine-tune a Transformer-based model (BERT) to classify movie reviews from the IMDb dataset into positive or negative sentiment. This task involves data cleaning, tokenization, model training, evaluation, and analysis, following a similar pipeline demonstrated in the transformer tweet sentiment example.\n",
        "Dataset\n",
        "\n",
        "IMDb Movie Reviews\n",
        "\n",
        "    Publicly accessible via the Hugging Face Datasets library (no manual download or sign-in required).\n",
        "    Loading code snippet:\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"imdb\")\n",
        "train = dataset[\"train\"]\n",
        "test = dataset[\"test\"]\n",
        "\n",
        "    Dataset size: 25,000 training samples and 25,000 testing samples.\n",
        "    Structure: Each example contains a text field (the movie review) and a label field (0 = negative, 1 = positive).\n",
        "\n",
        "Learning Objectives\n",
        "\n",
        "    Clean and preprocess natural language movie reviews (remove HTML tags, special characters, unwanted whitespace).\n",
        "    Tokenize and encode text using BertTokenizer.\n",
        "    Fine-tune BertForSequenceClassification for binary sentiment classification.\n",
        "    Evaluate model with classification metrics (precision, recall, F1-score).\n",
        "    Analyze model predictions, including inspection of correctly and incorrectly classified samples.\n",
        "\n",
        "Tasks\n",
        "\n",
        "    Data Loading & Exploration\n",
        "        Load the IMDb dataset directly using Hugging Faceâ€™s load_dataset(\"imdb\") function.\n",
        "        Analyze dataset distribution and sample texts to understand the data.\n",
        "    Data Cleaning\n",
        "        Clean the review texts to remove noise such as HTML tags and punctuation.\n",
        "        Prepare the cleaned text for tokenization.\n",
        "    Dataset Preparation\n",
        "        Implement a PyTorch Dataset class similar to TweetDataset, which performs tokenization, padding, and truncation using BertTokenizer.\n",
        "        Ensure token sequences have a max length (e.g., 128) for efficient batching.\n",
        "    Model Setup and Training\n",
        "        Load the pretrained BERT base uncased model configured for sequence classification with two output labels.\n",
        "        Define training parameters such as batch size, epochs, and logging setup.\n",
        "        Use the Hugging Face Trainer API to train and validate the model on the IMDb data.\n",
        "    Evaluation and Reporting\n",
        "        Generate a detailed classification report with precision, recall, and F1-score.\n",
        "        Create a DataFrame comparing review texts, actual labels, and predicted labels for sample inspection.\n",
        "\n",
        "Deliverables\n",
        "\n",
        "    Python notebook or script containing fully documented code for the entire pipeline.\n",
        "    Classification report and insights into model performance and errors.\n",
        "    Examples of correct and incorrect predictions with analysis.\n",
        "\n",
        "Getting Started Example\n",
        "\n"
      ],
      "metadata": {
        "id": "_FtSLr3R5K7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load IMDb dataset\n",
        "dataset = load_dataset(\"imdb\")\n",
        "train = dataset[\"train\"]\n",
        "test = dataset[\"test\"]\n",
        "\n",
        "print(f\"Number of training samples: {len(train)}\")\n",
        "print(f\"Number of test samples: {len(test)}\")\n",
        "\n",
        "# Sample review and label\n",
        "print(\"Sample text:\", train[0][\"text\"][:200])\n",
        "print(\"Sample label:\", train[0][\"label\"])\n",
        "'''\n",
        "\n"
      ],
      "metadata": {
        "id": "BVEqgTJQ5ik6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This problem statement ensures you use a reliable, easy-to-access dataset with no external sign-in or manual downloads, perfectly fitting into a Transformer fine-tuning workflow.\n"
      ],
      "metadata": {
        "id": "UU6jbdXK5oa6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ozwDbubN6HML"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}