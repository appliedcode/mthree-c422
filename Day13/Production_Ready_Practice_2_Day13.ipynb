{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Operationalizing Ethics in AI Development & Deployment — Ride‑Hailing Driver Evaluation"
      ],
      "metadata": {
        "id": "6ITpoIGJDxRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Dataset Creation\n"
      ],
      "metadata": {
        "id": "XwgGsiSTD4jF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIL2t3CBDwRp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(77)\n",
        "\n",
        "# Synthetic dataset\n",
        "n = 1500\n",
        "genders = [\"Male\", \"Female\", \"Other\"]\n",
        "cities = [\"Metro\", \"Urban\", \"Town\"]\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    \"Driver_ID\": range(1, n+1),\n",
        "    \"Age\": np.random.randint(21, 65, n),\n",
        "    \"Gender\": np.random.choice(genders, n, p=[0.6, 0.35, 0.05]),\n",
        "    \"City_Type\": np.random.choice(cities, n, p=[0.4, 0.45, 0.15]),\n",
        "    \"Trips_Completed\": np.random.randint(50, 2000, n),\n",
        "    \"Avg_Rating\": np.round(np.random.uniform(3.0, 5.0, n), 2),\n",
        "    \"Complaints\": np.random.randint(0, 20, n),\n",
        "    \"Accidents\": np.random.randint(0, 5, n)\n",
        "})\n",
        "\n",
        "# Introduce base label\n",
        "data[\"Flagged_For_Review\"] = np.where(\n",
        "    (data[\"Complaints\"] > 5) | (data[\"Accidents\"] > 0),\n",
        "    np.random.choice([1, 0], n, p=[0.7, 0.3]),\n",
        "    np.random.choice([1, 0], n, p=[0.15, 0.85])\n",
        ")\n",
        "\n",
        "# Add bias: higher flag rates for Metro drivers\n",
        "mask_metro = data[\"City_Type\"] == \"Metro\"\n",
        "data.loc[mask_metro, \"Flagged_For_Review\"] = np.where(\n",
        "    mask_metro,\n",
        "    np.random.choice([1, 0], mask_metro.sum(), p=[0.4, 0.6]),\n",
        "    data[\"Flagged_For_Review\"]\n",
        ")\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part A — Operationalizing Ethics in Development\n"
      ],
      "metadata": {
        "id": "wwjwp1qJD802"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1 — Bias Assessment Before Deployment\n"
      ],
      "metadata": {
        "id": "bedn-F5zD-j-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Prepare data (one-hot encoding for categorical vars)\n",
        "X = pd.get_dummies(data.drop(columns=[\"Driver_ID\", \"Flagged_For_Review\"]), drop_first=True)\n",
        "y = data[\"Flagged_For_Review\"]\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Baseline Accuracy:\", round(accuracy_score(y_test, y_pred), 3))\n",
        "\n",
        "# Merge sensitive attrs for bias check\n",
        "df_test = X_test.copy()\n",
        "df_test[\"y_pred\"] = y_pred\n",
        "df_test[\"Gender\"] = data.loc[y_test.index, \"Gender\"]\n",
        "df_test[\"City_Type\"] = data.loc[y_test.index, \"City_Type\"]\n",
        "\n",
        "# Bias check\n",
        "print(\"\\nPositive prediction rate by Gender:\")\n",
        "print(df_test.groupby(\"Gender\")[\"y_pred\"].mean())\n",
        "\n",
        "print(\"\\nPositive prediction rate by City_Type:\")\n",
        "print(df_test.groupby(\"City_Type\")[\"y_pred\"].mean())"
      ],
      "metadata": {
        "id": "AI-QbOHDEqLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2 — Ethics‑Aware Fairness Gate\n"
      ],
      "metadata": {
        "id": "pLF6NolLEtgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fairness_check(df, sensitive_attr, threshold=0.08):\n",
        "    rates = df.groupby(sensitive_attr)[\"y_pred\"].mean()\n",
        "    disparity = rates.max() - rates.min()\n",
        "    print(f\"{sensitive_attr} disparity: {round(disparity, 3)}\")\n",
        "    return disparity <= threshold\n",
        "\n",
        "print(\"\\nFairness Gate Results:\")\n",
        "gender_fair = fairness_check(df_test, \"Gender\")\n",
        "city_fair = fairness_check(df_test, \"City_Type\")\n",
        "\n",
        "if not (gender_fair and city_fair):\n",
        "    print(\"❌ Fairness gate FAILED — mitigation needed before deployment.\")\n",
        "else:\n",
        "    print(\"✅ Fairness gate PASSED\")"
      ],
      "metadata": {
        "id": "tPIJ1pjxEq9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3 — Feature Importance Ethics Audit\n"
      ],
      "metadata": {
        "id": "PiDF78J3Evni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "importances = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "print(\"\\nTop Features by Importance:\")\n",
        "print(importances.head(10))"
      ],
      "metadata": {
        "id": "CeSly3cIEvb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 4 — Ethics Development User Stories\n"
      ],
      "metadata": {
        "id": "bJt8GD9wEy0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# US1: As an ethics officer, I want the driver evaluation model to maintain bias disparity below 8% so that reviews are fair across all demographics.\n",
        "# US2: As an operations manager, I want all model predictions logged with context so any disputed driver flagging can be audited."
      ],
      "metadata": {
        "id": "oolmwUz1Eyk3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part B — Operationalizing Ethics in Deployment\n"
      ],
      "metadata": {
        "id": "4BG_bC01E80p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 5 — Accountability via Prediction Logging\n"
      ],
      "metadata": {
        "id": "Vlcs99zXFClU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "prediction_log = []\n",
        "MODEL_VERSION = \"v1.0\"\n",
        "\n",
        "def predict_and_log(driver_features):\n",
        "    pred = model.predict(driver_features)[0]\n",
        "    log_entry = {\n",
        "        \"timestamp\": datetime.datetime.now().isoformat(),\n",
        "        \"model_version\": MODEL_VERSION,\n",
        "        \"inputs\": driver_features.to_dict(orient='records')[0],\n",
        "        \"prediction\": int(pred)\n",
        "    }\n",
        "    prediction_log.append(log_entry)\n",
        "    return pred\n",
        "\n",
        "# Example log\n",
        "sample_driver = X_test.iloc[[0]]\n",
        "predict_and_log(sample_driver)\n",
        "prediction_log[:1]"
      ],
      "metadata": {
        "id": "ogIkcSgyFD43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 6 — Post‑Deployment Monitoring\n"
      ],
      "metadata": {
        "id": "bctB-Os4FF3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Demographic parity after deployment\n",
        "dp_gender = df_test.groupby(\"Gender\")[\"y_pred\"].mean().max() - df_test.groupby(\"Gender\")[\"y_pred\"].mean().min()\n",
        "dp_city = df_test.groupby(\"City_Type\")[\"y_pred\"].mean().max() - df_test.groupby(\"City_Type\")[\"y_pred\"].mean().min()\n",
        "\n",
        "print(f\"Gender disparity: {dp_gender:.3f}\")\n",
        "print(f\"City disparity: {dp_city:.3f}\")\n",
        "\n",
        "if dp_gender > 0.08 or dp_city > 0.08:\n",
        "    print(\"⚠️ ALERT: Bias detected — review required\")\n",
        "else:\n",
        "    print(\"✅ Fairness levels acceptable\")"
      ],
      "metadata": {
        "id": "14HFDh_EFGlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 7 — Privacy Protection\n"
      ],
      "metadata": {
        "id": "qgyo4AUMFJ1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "\n",
        "data[\"Driver_ID\"] = data[\"Driver_ID\"].apply(lambda x: hashlib.sha256(str(x).encode()).hexdigest())\n",
        "data.head()"
      ],
      "metadata": {
        "id": "H8jBwNHhFLXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 8 — Inclusivity Feedback Loop\n"
      ],
      "metadata": {
        "id": "ELA5QUL_FMhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample feedback from underrepresented groups\n",
        "feedback = pd.DataFrame({\n",
        "    \"Age\": [30, 45],\n",
        "    \"Gender\": [\"Other\", \"Other\"],\n",
        "    \"City_Type\": [\"Town\", \"Town\"],\n",
        "    \"Trips_Completed\": [500, 1200],\n",
        "    \"Avg_Rating\": [4.6, 4.3],\n",
        "    \"Complaints\": [1, 0],\n",
        "    \"Accidents\": [0, 0],\n",
        "    \"Flagged_For_Review\": [0, 0]\n",
        "})\n",
        "\n",
        "# Append for retraining\n",
        "data_updated = pd.concat([data, feedback], ignore_index=True)\n",
        "print(\"Updated dataset size:\", data_updated.shape)"
      ],
      "metadata": {
        "id": "sAY7c7y-FOM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 9 — Ethical Incident Simulation\n",
        "Scenario: Post‑deployment, monitoring detects 20% disparity for City_Type.\n",
        "\n",
        "Incident Response Plan:\n",
        "\n",
        "Detection — Monitoring system alerts due to high disparity.\n",
        "Investigation — Audit model logs, check feature importance, review training data.\n",
        "Communication — Notify compliance and ops teams, share summary with stakeholders.\n",
        "Remediation — Retrain with balanced city representation; remove/adjust City_Type weight; tighten fairness gate."
      ],
      "metadata": {
        "id": "z_KkExO6FQWG"
      }
    }
  ]
}