{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/appliedcode/mthree-c422/blob/main/Exercises/day-4/Conversion_techniques/Conversion_types.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversion Techniques: Single-Lab Exercises\n",
        "Below are four self-contained lab exercises‚Äîone each for Bag of Words, N-grams, TF-IDF, and Word2Vec‚Äîcomplete with concise step-by-step instructions and reference Python solutions.\n",
        "All examples run in a standard Jupyter/Python 3 environment and rely only on widely used libraries (nltk, scikit-learn, gensim, pandas)."
      ],
      "metadata": {
        "id": "bOx5MkTlY-ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# Download both old and new tokenizer data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "fCRYJGvtY995",
        "outputId": "3e5dd2c1-d5ae-4b59-ca5b-807e1676e218",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Bag of Words (BoW)\n",
        "### Exercise Brief\n",
        "Convert three short product reviews into a Bag-of-Words matrix and compare a manual implementation with CountVectorizer.\n",
        "\n",
        "### Reviews\n",
        "\n",
        "‚ÄúThis phone has great battery life‚Äù\n",
        "\n",
        "‚ÄúBattery life on this phone is poor‚Äù\n",
        "\n",
        "‚ÄúI love the camera on this phone‚Äù\n",
        "\n",
        "###Tasks\n",
        "\n",
        "- Pre-process each review: lowercase, tokenize, remove stop-words.\n",
        "\n",
        "- Construct a vocabulary of unique words.\n",
        "\n",
        "- Create a frequency vector for every review (manual).\n",
        "\n",
        "- Repeat using sklearn.feature_extraction.text.CountVectorizer.\n",
        "\n",
        "- Compare the two matrices.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2h6SiJl7Vj1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk, string, pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"This phone has great battery life\",\n",
        "    \"Battery life on this phone is poor\",\n",
        "    \"I love the camera on this phone\"\n",
        "]\n",
        "\n",
        "# download once per session\n",
        "nltk.download('punkt'); nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
        "    return [w for w in word_tokenize(text) if w not in stop_words]\n",
        "\n",
        "tokens_list = [preprocess(doc) for doc in corpus]\n",
        "vocab = sorted({w for sent in tokens_list for w in sent})\n",
        "\n",
        "def bow_vector(tokens):\n",
        "    return [tokens.count(term) for term in vocab]\n",
        "\n",
        "manual_bow = [bow_vector(t) for t in tokens_list]\n",
        "manual_df  = pd.DataFrame(manual_bow, columns=vocab)\n",
        "\n",
        "cv = CountVectorizer(lowercase=True, stop_words='english')\n",
        "cv_bow = cv.fit_transform(corpus).toarray()\n",
        "cv_df  = pd.DataFrame(cv_bow, columns=cv.get_feature_names_out())\n",
        "\n",
        "print(manual_df, '\\n'); print(cv_df)"
      ],
      "metadata": {
        "id": "X_DfX-FOVuUb",
        "outputId": "1781a8ef-2ab5-4bdd-d2d7-3e5994f1e81b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   battery  camera  great  life  love  phone  poor\n",
            "0        1       0      1     1     0      1     0\n",
            "1        1       0      0     1     0      1     1\n",
            "2        0       1      0     0     1      1     0 \n",
            "\n",
            "   battery  camera  great  life  love  phone  poor\n",
            "0        1       0      1     1     0      1     0\n",
            "1        1       0      0     1     0      1     1\n",
            "2        0       1      0     0     1      1     0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß™ Exercise Brief: N-grams Generation and Analysis\n",
        "\n",
        "## üìù Sentence\n",
        "> ‚ÄúNatural language processing is fascinating and powerful‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Tasks\n",
        "\n",
        "### 1. Tokenize the Sentence\n",
        "- Use `nltk.word_tokenize` to split the sentence into tokens.\n",
        "\n",
        "### 2. Generate N-grams (n = 1 to 4)\n",
        "- Use `nltk.util.ngrams` to create:\n",
        "  - **Unigrams** (n=1)\n",
        "  - **Bigrams** (n=2)\n",
        "  - **Trigrams** (n=3)\n",
        "  - **Four-grams** (n=4)\n",
        "- Store:\n",
        "  - As **lists of tuples** (e.g., `('natural', 'language')`)\n",
        "  - As **joined strings** (e.g., `\"natural language\"`)\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Frequency Analysis\n",
        "- Use `collections.Counter` to compute:\n",
        "  - **Bigram frequency count**\n",
        "  - **Trigram frequency count**\n",
        "- Report most frequent combinations (if ties, show all with max count)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "NWRytll3Zt6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "sentence = \"Natural language processing is fascinating and powerful\"\n",
        "tokens   = word_tokenize(sentence.lower())\n",
        "\n",
        "for n in range(1, 5):\n",
        "    grams = list(ngrams(tokens, n))\n",
        "    print(f\"{n}-grams:\", grams)\n",
        "\n",
        "bigram_freq  = Counter(ngrams(tokens, 2))\n",
        "trigram_freq = Counter(ngrams(tokens, 3))\n",
        "print(\"Top bigrams:\", bigram_freq.most_common())\n",
        "print(\"Top trigrams:\", trigram_freq.most_common())"
      ],
      "metadata": {
        "id": "juivLkOTZtZ7",
        "outputId": "b737e20c-a754-4bb6-d4d5-4be0efcc82aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1-grams: [('natural',), ('language',), ('processing',), ('is',), ('fascinating',), ('and',), ('powerful',)]\n",
            "2-grams: [('natural', 'language'), ('language', 'processing'), ('processing', 'is'), ('is', 'fascinating'), ('fascinating', 'and'), ('and', 'powerful')]\n",
            "3-grams: [('natural', 'language', 'processing'), ('language', 'processing', 'is'), ('processing', 'is', 'fascinating'), ('is', 'fascinating', 'and'), ('fascinating', 'and', 'powerful')]\n",
            "4-grams: [('natural', 'language', 'processing', 'is'), ('language', 'processing', 'is', 'fascinating'), ('processing', 'is', 'fascinating', 'and'), ('is', 'fascinating', 'and', 'powerful')]\n",
            "Top bigrams: [(('natural', 'language'), 1), (('language', 'processing'), 1), (('processing', 'is'), 1), (('is', 'fascinating'), 1), (('fascinating', 'and'), 1), (('and', 'powerful'), 1)]\n",
            "Top trigrams: [(('natural', 'language', 'processing'), 1), (('language', 'processing', 'is'), 1), (('processing', 'is', 'fascinating'), 1), (('is', 'fascinating', 'and'), 1), (('fascinating', 'and', 'powerful'), 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìò Exercise Brief: TF-IDF Vectorization of News Headlines\n",
        "\n",
        "## üì∞ Headlines\n",
        "1. ‚ÄúStock market crashes amid global uncertainty‚Äù  \n",
        "2. ‚ÄúGlobal leaders discuss climate change solutions‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Tasks\n",
        "\n",
        "### 1. Pre-processing\n",
        "- Convert all text to **lowercase**\n",
        "- **Tokenize** the headlines (split into words)\n",
        "- **Remove stop words** (e.g., \"the\", \"amid\", etc.)\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Compute Term Frequency (TF)\n",
        "- Count term occurrences in each headline\n",
        "- Normalize by total terms in the headline\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Compute Inverse Document Frequency (IDF)\n",
        "- Use the formula:  \n",
        "  \\[\n",
        "  \\text{IDF}(t) = \\log\\left(\\frac{N}{1 + \\text{df}(t)}\\right)\n",
        "  \\]  \n",
        "  Where:\n",
        "  - *N* = total number of documents (2 in this case)\n",
        "  - *df(t)* = number of documents containing term *t*\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Build TF-IDF Matrix (Manual)\n",
        "- Multiply each term‚Äôs TF by its IDF\n",
        "- Result: **2 √ó V matrix**, where *V* is the vocabulary size\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Recreate Matrix with `TfidfVectorizer` (Sklearn)\n",
        "- Use `sklearn.feature_extraction.text.TfidfVectorizer` with:\n",
        "  - `stop_words='english'`\n",
        "  - `lowercase=True`\n",
        "\n",
        "---\n",
        "\n",
        "### 6. Top-Weighted Terms\n",
        "- For each headline:\n",
        "  - Extract top **three terms** with the **highest TF-IDF weights**\n",
        "  - Report terms and their corresponding scores\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Expected Output\n",
        "- **Manual TF-IDF Matrix**\n",
        "- **Sklearn TF-IDF Matrix**\n",
        "- **Top 3 TF-IDF terms per headline**  \n"
      ],
      "metadata": {
        "id": "c_KUxt6-Zy7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import log\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "docs = [\n",
        "    \"Stock market crashes amid global uncertainty\",\n",
        "    \"Global leaders discuss climate change solutions\"\n",
        "]\n",
        "\n",
        "# scikit-learn path\n",
        "vec = TfidfVectorizer(lowercase=True, stop_words='english')\n",
        "tfidf = vec.fit_transform(docs).toarray()\n",
        "print(pd.DataFrame(tfidf, columns=vec.get_feature_names_out()))\n",
        "\n",
        "# manual IDF (for pedagogy)\n",
        "tokens = [word_tokenize(d.lower()) for d in docs]\n",
        "stop = set(stopwords.words('english'))\n",
        "proc   = [[w for w in t if w.isalpha() and w not in stop] for t in tokens]\n",
        "vocab  = sorted({w for doc in proc for w in doc})\n",
        "idf    = {t: log(len(docs) / sum(t in p for p in proc)) for t in vocab}\n",
        "tfidf_manual = []\n",
        "for doc in proc:\n",
        "    length = len(doc)\n",
        "    tfidf_manual.append([doc.count(t)/length*idf[t] for t in vocab])\n",
        "print(pd.DataFrame(tfidf_manual, columns=vocab))\n"
      ],
      "metadata": {
        "id": "vTL56buAZyH7",
        "outputId": "04cf69fd-55a5-49a4-b89c-17fbdac4aac2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      amid   change  climate  crashes  discuss    global  leaders   market  \\\n",
            "0  0.42616  0.00000  0.00000  0.42616  0.00000  0.303216  0.00000  0.42616   \n",
            "1  0.00000  0.42616  0.42616  0.00000  0.42616  0.303216  0.42616  0.00000   \n",
            "\n",
            "   solutions    stock  uncertainty  \n",
            "0    0.00000  0.42616      0.42616  \n",
            "1    0.42616  0.00000      0.00000  \n",
            "       amid    change   climate   crashes   discuss  global   leaders  \\\n",
            "0  0.115525  0.000000  0.000000  0.115525  0.000000     0.0  0.000000   \n",
            "1  0.000000  0.115525  0.115525  0.000000  0.115525     0.0  0.115525   \n",
            "\n",
            "     market  solutions     stock  uncertainty  \n",
            "0  0.115525   0.000000  0.115525     0.115525  \n",
            "1  0.000000   0.115525  0.000000     0.000000  \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}