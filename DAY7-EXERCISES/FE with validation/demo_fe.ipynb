{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DCcYl_sCZwsx"
      },
      "outputs": [],
      "source": [
        "!pip install pandas numpy scikit-learn -q\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.exceptions import DataConversionWarning\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for clearer output\n",
        "warnings.filterwarnings(action='ignore', category=DataConversionWarning)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load and Split the Dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Split into train and validation sets\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['Survived'])"
      ],
      "metadata": {
        "id": "oUyNdxmwaU7y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Define the Pipeline Functions\n",
        "def clean_data(df):\n",
        "    df = df.drop(columns=[\"PassengerId\",\"Ticket\",\"Cabin\"], errors=\"ignore\").copy()\n",
        "    # Impute Age and Embarked\n",
        "    df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].median())\n",
        "    df[\"Embarked\"] = df[\"Embarked\"].fillna(df[\"Embarked\"].mode()[0])\n",
        "    return df\n",
        "\n",
        "def engineer_features(df):\n",
        "    df = df.copy()\n",
        "    # Title extraction\n",
        "    df[\"Title\"] = df[\"Name\"].str.extract(r\",\\s*([^\\.]+)\\.\")\n",
        "    rare_titles = [\"Lady\",\"Countess\",\"Capt\",\"Col\",\"Don\",\"Dr\",\"Major\",\"Rev\",\"Sir\",\"Jonkheer\",\"Dona\"]\n",
        "    df[\"Title\"] = df[\"Title\"].replace(rare_titles, \"Rare\")\n",
        "    # Family size & is alone\n",
        "    df[\"FamilySize\"] = df[\"SibSp\"] + df[\"Parch\"] + 1\n",
        "    df[\"IsAlone\"] = (df[\"FamilySize\"] == 1).astype(int)\n",
        "    # Fare and Age bins\n",
        "    df[\"FareBin\"] = pd.qcut(df[\"Fare\"].fillna(0), 4, labels=False)\n",
        "    df[\"AgeBin\"]  = pd.cut(df[\"Age\"], bins=[0,12,20,40,60,100], labels=False)\n",
        "    # Drop unused columns\n",
        "    df = df.drop(columns=[\"Name\",\"SibSp\",\"Parch\"])\n",
        "    # One-hot encode\n",
        "    df = pd.get_dummies(df, columns=[\"Sex\",\"Embarked\",\"Title\"], drop_first=True)\n",
        "    return df\n",
        "\n",
        "def validate_data(df):\n",
        "    errors = []\n",
        "    # Check for nulls\n",
        "    null_counts = df.isnull().sum()\n",
        "    if null_counts.any():\n",
        "        errors.append(f\"Null values found:\\n{null_counts[null_counts>0]}\")\n",
        "    # Check expected columns\n",
        "    expected_cols = {\"Survived\",\"Pclass\",\"Age\",\"Fare\",\"FamilySize\",\"IsAlone\",\"FareBin\",\"AgeBin\"}\n",
        "    missing = expected_cols - set(df.columns)\n",
        "    if missing:\n",
        "        errors.append(f\"Missing columns: {missing}\")\n",
        "    return errors"
      ],
      "metadata": {
        "id": "awGqpIDpaY9l"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Execute the Pipeline\n",
        "# Clean and feature-engineer training data\n",
        "train_clean = clean_data(train_df)\n",
        "train_feat  = engineer_features(train_clean)\n",
        "train_errors = validate_data(train_feat)\n",
        "print(\"Train validation errors:\", train_errors or \"None\")\n",
        "\n",
        "# Clean and feature-engineer validation data\n",
        "val_clean = clean_data(val_df)\n",
        "val_feat  = engineer_features(val_clean)\n",
        "val_errors = validate_data(val_feat)\n",
        "print(\"Validation validation errors:\", val_errors or \"None\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ApZcDU-aaa2",
        "outputId": "85291008-a43b-4f2d-ae98-f64fd6c081e7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train validation errors: None\n",
            "Validation validation errors: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Save Prepared Data\n",
        "train_feat.to_csv(\"titanic_train_prepared.csv\", index=False)\n",
        "val_feat.to_csv(\"titanic_val_prepared.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "raypOmhpaeT9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        " 1. How did validation checks help catch data quality issues early?\n",
        "Validation checks ensured that:\n",
        "- No null values remained after cleaning and feature engineering.\n",
        "- Key engineered columns (e.g., AgeBin, FareBin, IsAlone) were present.\n",
        "- The dataset structure matched expectations before modeling.\n",
        "\n",
        "This helped prevent runtime errors and inconsistent inputs later in the machine learning pipeline.\n",
        "\n",
        "---\n",
        "\n",
        " 2. Which engineered features contributed most to dataset richness?\n",
        "The following features significantly improved the dataset predictive power:\n",
        "- Title: Extracted from the passenger's name, this adds insight into social status, gender, and age.\n",
        "- FamilySize and IsAlone: Captured social structure and travel patterns, which affected survival likelihood.\n",
        "- FareBin and AgeBin: Transformed continuous variables into categorical bins, capturing non-linear effects more effectively.\n",
        "\n",
        "---\n",
        "\n",
        " 3. How would you extend this pipeline to include scaling, imputation strategies, or integrate with a model training step?\n",
        "To extend the pipeline:\n",
        "- Scaling: Apply StandardScaler or MinMaxScaler to numerical features (e.g., Age, Fare) using ColumnTransformer.\n",
        "- Advanced Imputation: Use SimpleImputer or KNNImputer for more robust handling of missing values.\n",
        "- Model Training Integration: Use sklearn.pipeline.Pipeline to combine feature engineering and model training into one consistent pipeline.\n",
        "- Cross-validation & tuning: Wrap the pipeline in GridSearchCV or RandomizedSearchCV for hyperparameter optimization.\n",
        "\n",
        "This would make the pipeline fully automated and production-ready.\n"
      ],
      "metadata": {
        "id": "nmzB7Wu6aidn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BEU63U7QbMbe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}