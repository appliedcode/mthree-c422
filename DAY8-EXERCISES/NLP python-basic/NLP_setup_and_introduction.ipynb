{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "acae2c73",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acae2c73",
        "outputId": "ba95d64c-200b-4d1f-d07e-05b0d97d7afb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Package installation section complete!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages (run this cell if packages are not already installed)\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package):\n",
        "    \"\"\"Install a package using pip\"\"\"\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "        print(f\"âœ“ Successfully installed {package}\")\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(f\"âœ— Failed to install {package}\")\n",
        "\n",
        "# Core NLP packages\n",
        "packages = [\n",
        "    \"nltk==3.8.1\",\n",
        "    \"spacy==3.7.2\",\n",
        "    \"scikit-learn==1.3.2\",\n",
        "    \"transformers==4.35.2\",\n",
        "    \"datasets==2.14.6\",\n",
        "    \"torch==2.1.1\",\n",
        "    \"pandas==2.0.3\",\n",
        "    \"matplotlib==3.7.2\",\n",
        "    \"seaborn==0.12.2\",\n",
        "    \"wordcloud==1.9.2\",\n",
        "    \"textblob==0.17.1\"\n",
        "]\n",
        "\n",
        "# Uncomment the next lines to install packages\n",
        "# for package in packages:\n",
        "#     install_package(package)\n",
        "\n",
        "print(\"Package installation section complete!\")\n",
        "# print(\"Note: Uncomment the installation loop above if you need to install packages.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bb1da79d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb1da79d",
        "outputId": "cae90d71-b0cb-4648-be52-2fe1adc312f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ All core packages imported successfully!\n",
            "\n",
            "Downloading NLTK data...\n",
            "âœ“ Downloaded punkt\n",
            "âœ“ Downloaded stopwords\n",
            "âœ“ Downloaded vader_lexicon\n",
            "âœ“ Downloaded wordnet\n",
            "âœ“ Downloaded averaged_perceptron_tagger\n",
            "âœ“ Downloaded omw-1.4\n",
            "\n",
            "ðŸŽ‰ Environment setup complete!\n"
          ]
        }
      ],
      "source": [
        "# Verify imports and download NLTK data\n",
        "import nltk\n",
        "import spacy\n",
        "import sklearn\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ“ All core packages imported successfully!\")\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk_downloads = [\n",
        "    'punkt',\n",
        "    'stopwords',\n",
        "    'vader_lexicon',\n",
        "    'wordnet',\n",
        "    'averaged_perceptron_tagger',\n",
        "    'omw-1.4'\n",
        "]\n",
        "\n",
        "print(\"\\nDownloading NLTK data...\")\n",
        "for data in nltk_downloads:\n",
        "    try:\n",
        "        nltk.download(data, quiet=True)\n",
        "        print(f\"âœ“ Downloaded {data}\")\n",
        "    except:\n",
        "        print(f\"âœ— Failed to download {data}\")\n",
        "\n",
        "print(\"\\nðŸŽ‰ Environment setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4c5aceb",
      "metadata": {
        "id": "b4c5aceb"
      },
      "source": [
        "## Section 2: Introduction to Natural Language Processing\n",
        "\n",
        "### What is NLP?\n",
        "Natural Language Processing (NLP) is a branch of artificial intelligence that focuses on the interaction between computers and human language. It enables machines to:\n",
        "\n",
        "- **Understand** human language (reading comprehension)\n",
        "- **Generate** human-like text (text generation)\n",
        "- **Translate** between languages\n",
        "- **Extract insights** from text data\n",
        "- **Classify** and **analyze** text content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5cee59fe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cee59fe",
        "outputId": "e67ca533-8284-4490-fe76-0533f343251f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Texts:\n",
            "==================================================\n",
            "1. I love this product! It's amazing and works perfectly.\n",
            "2. This is the worst purchase I've ever made. Terrible quality!\n",
            "3. The product is okay, nothing special but gets the job done.\n",
            "4. Absolutely fantastic! Would recommend to everyone.\n",
            "5. Not bad, could be better but decent for the price.\n",
            "\n",
            "Basic Statistics:\n",
            "Number of texts: 5\n",
            "Total characters: 273\n",
            "Average text length: 54.6 characters\n",
            "\n",
            "Top 10 Most Common Words:\n",
            "the: 4\n",
            "i: 2\n",
            "this: 2\n",
            "product: 2\n",
            "is: 2\n",
            "but: 2\n",
            "love: 1\n",
            "it: 1\n",
            "s: 1\n",
            "amazing: 1\n",
            "\n",
            "Basic Sentiment Analysis:\n",
            "Text 1: Positive (pos: 3, neg: 0)\n",
            "Text 2: Negative (pos: 0, neg: 2)\n",
            "Text 3: Neutral (pos: 0, neg: 0)\n",
            "Text 4: Positive (pos: 2, neg: 0)\n",
            "Text 5: Negative (pos: 0, neg: 1)\n"
          ]
        }
      ],
      "source": [
        "# Let's start with a simple example to demonstrate basic NLP concepts\n",
        "\n",
        "# Sample text data\n",
        "sample_texts = [\n",
        "    \"I love this product! It's amazing and works perfectly.\",\n",
        "    \"This is the worst purchase I've ever made. Terrible quality!\",\n",
        "    \"The product is okay, nothing special but gets the job done.\",\n",
        "    \"Absolutely fantastic! Would recommend to everyone.\",\n",
        "    \"Not bad, could be better but decent for the price.\"\n",
        "]\n",
        "\n",
        "print(\"Sample Texts:\")\n",
        "print(\"=\" * 50)\n",
        "for i, text in enumerate(sample_texts, 1):\n",
        "    print(f\"{i}. {text}\")\n",
        "\n",
        "# Basic text analysis\n",
        "print(f\"\\nBasic Statistics:\")\n",
        "print(f\"Number of texts: {len(sample_texts)}\")\n",
        "print(f\"Total characters: {sum(len(text) for text in sample_texts)}\")\n",
        "print(f\"Average text length: {sum(len(text) for text in sample_texts) / len(sample_texts):.1f} characters\")\n",
        "\n",
        "# Word frequency analysis\n",
        "all_words = []\n",
        "for text in sample_texts:\n",
        "    # Simple tokenization (split by spaces and remove punctuation)\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    all_words.extend(words)\n",
        "\n",
        "word_freq = Counter(all_words)\n",
        "print(f\"\\nTop 10 Most Common Words:\")\n",
        "for word, count in word_freq.most_common(10):\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "# Basic sentiment indicators\n",
        "positive_words = ['love', 'amazing', 'perfectly', 'fantastic', 'recommend']\n",
        "negative_words = ['worst', 'terrible', 'bad']\n",
        "\n",
        "print(f\"\\nBasic Sentiment Analysis:\")\n",
        "for i, text in enumerate(sample_texts, 1):\n",
        "    text_lower = text.lower()\n",
        "    pos_count = sum(1 for word in positive_words if word in text_lower)\n",
        "    neg_count = sum(1 for word in negative_words if word in text_lower)\n",
        "\n",
        "    if pos_count > neg_count:\n",
        "        sentiment = \"Positive\"\n",
        "    elif neg_count > pos_count:\n",
        "        sentiment = \"Negative\"\n",
        "    else:\n",
        "        sentiment = \"Neutral\"\n",
        "\n",
        "    print(f\"Text {i}: {sentiment} (pos: {pos_count}, neg: {neg_count})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6f89763",
      "metadata": {
        "id": "e6f89763"
      },
      "source": [
        "## Section 3: Loading and Inspecting Datasets\n",
        "\n",
        "Throughout this course, we'll work with several real-world datasets. Let's load and inspect some of the key datasets we'll be using."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8d062425",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d062425",
        "outputId": "78876af8-244c-4ade-ca06-5e9f878f05e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SMS Spam Dataset:\n",
            "  label                                            message\n",
            "0   ham              Hey, are we still on for lunch today?\n",
            "1  spam          URGENT! You've won $1000! Click here now!\n",
            "2   ham             Can you pick up milk on your way home?\n",
            "3  spam         FREE iPhone! Limited time offer! Call now!\n",
            "4   ham                      Meeting moved to 3pm tomorrow\n",
            "5  spam  Congratulations! You've been selected for a sp...\n",
            "6   ham                    Thanks for the birthday wishes!\n",
            "7  spam    SALE ALERT: 90% off everything! Don't miss out!\n",
            "8   ham               Running late, be there in 10 minutes\n",
            "9  spam  You owe $500 in taxes. Pay immediately or face...\n",
            "\n",
            "Dataset shape: (10, 2)\n",
            "Label distribution:\n",
            "label\n",
            "ham     5\n",
            "spam    5\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Movie Reviews Dataset:\n",
            "  sentiment                                             review\n",
            "0  positive  This movie was absolutely fantastic! Great act...\n",
            "1  negative   Boring and predictable. Waste of time and money.\n",
            "2  positive  Brilliant cinematography and outstanding perfo...\n",
            "3  negative  Poor script and terrible direction. Very disap...\n",
            "4  positive  Highly recommend! One of the best films this y...\n",
            "5  negative     Confusing plot and weak character development.\n",
            "6  positive       Amazing visual effects and compelling story.\n",
            "7  negative   Overrated and underwhelming. Expected much more.\n",
            "8  positive  Perfect blend of comedy and drama. Loved every...\n",
            "9  negative  Slow paced and lacks substance. Not worth watc...\n",
            "\n",
            "Dataset shape: (10, 2)\n",
            "Sentiment distribution:\n",
            "sentiment\n",
            "positive    5\n",
            "negative    5\n",
            "Name: count, dtype: int64\n",
            "\n",
            "News Headlines Dataset:\n",
            "                                            headline\n",
            "0  Apple announces new iPhone with revolutionary ...\n",
            "1  Stock market reaches all-time high amid econom...\n",
            "2  Scientists discover new species in Amazon rain...\n",
            "3   Local restaurant wins prestigious culinary award\n",
            "4  Tech company Microsoft invests in renewable en...\n",
            "5  Weather forecast predicts heavy rainfall this ...\n",
            "6  University researchers develop breakthrough me...\n",
            "7    Amazon expands delivery services to rural areas\n",
            "8     New study reveals benefits of regular exercise\n",
            "9  Government announces new environmental protect...\n",
            "\n",
            "Dataset shape: (10, 1)\n",
            "\n",
            "âœ… Sample datasets created and saved to 'data/' directory!\n"
          ]
        }
      ],
      "source": [
        "# Create sample datasets for the course\n",
        "import os\n",
        "\n",
        "# Create data directory\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# 1. Sample SMS Spam Dataset\n",
        "sms_data = [\n",
        "    (\"ham\", \"Hey, are we still on for lunch today?\"),\n",
        "    (\"spam\", \"URGENT! You've won $1000! Click here now!\"),\n",
        "    (\"ham\", \"Can you pick up milk on your way home?\"),\n",
        "    (\"spam\", \"FREE iPhone! Limited time offer! Call now!\"),\n",
        "    (\"ham\", \"Meeting moved to 3pm tomorrow\"),\n",
        "    (\"spam\", \"Congratulations! You've been selected for a special offer!\"),\n",
        "    (\"ham\", \"Thanks for the birthday wishes!\"),\n",
        "    (\"spam\", \"SALE ALERT: 90% off everything! Don't miss out!\"),\n",
        "    (\"ham\", \"Running late, be there in 10 minutes\"),\n",
        "    (\"spam\", \"You owe $500 in taxes. Pay immediately or face legal action!\")\n",
        "]\n",
        "\n",
        "sms_df = pd.DataFrame(sms_data, columns=['label', 'message'])\n",
        "sms_df.to_csv('data/sms_spam_sample.csv', index=False)\n",
        "\n",
        "print(\"SMS Spam Dataset:\")\n",
        "print(sms_df)\n",
        "print(f\"\\nDataset shape: {sms_df.shape}\")\n",
        "print(f\"Label distribution:\\n{sms_df['label'].value_counts()}\")\n",
        "\n",
        "# 2. Sample Movie Reviews Dataset\n",
        "movie_reviews = [\n",
        "    (\"positive\", \"This movie was absolutely fantastic! Great acting and storyline.\"),\n",
        "    (\"negative\", \"Boring and predictable. Waste of time and money.\"),\n",
        "    (\"positive\", \"Brilliant cinematography and outstanding performances.\"),\n",
        "    (\"negative\", \"Poor script and terrible direction. Very disappointed.\"),\n",
        "    (\"positive\", \"Highly recommend! One of the best films this year.\"),\n",
        "    (\"negative\", \"Confusing plot and weak character development.\"),\n",
        "    (\"positive\", \"Amazing visual effects and compelling story.\"),\n",
        "    (\"negative\", \"Overrated and underwhelming. Expected much more.\"),\n",
        "    (\"positive\", \"Perfect blend of comedy and drama. Loved every minute!\"),\n",
        "    (\"negative\", \"Slow paced and lacks substance. Not worth watching.\")\n",
        "]\n",
        "\n",
        "reviews_df = pd.DataFrame(movie_reviews, columns=['sentiment', 'review'])\n",
        "reviews_df.to_csv('data/movie_reviews_sample.csv', index=False)\n",
        "\n",
        "print(f\"\\nMovie Reviews Dataset:\")\n",
        "print(reviews_df)\n",
        "print(f\"\\nDataset shape: {reviews_df.shape}\")\n",
        "print(f\"Sentiment distribution:\\n{reviews_df['sentiment'].value_counts()}\")\n",
        "\n",
        "# 3. Sample News Headlines Dataset\n",
        "news_headlines = [\n",
        "    \"Apple announces new iPhone with revolutionary camera technology\",\n",
        "    \"Stock market reaches all-time high amid economic recovery\",\n",
        "    \"Scientists discover new species in Amazon rainforest\",\n",
        "    \"Local restaurant wins prestigious culinary award\",\n",
        "    \"Tech company Microsoft invests in renewable energy projects\",\n",
        "    \"Weather forecast predicts heavy rainfall this weekend\",\n",
        "    \"University researchers develop breakthrough medical treatment\",\n",
        "    \"Amazon expands delivery services to rural areas\",\n",
        "    \"New study reveals benefits of regular exercise\",\n",
        "    \"Government announces new environmental protection policies\"\n",
        "]\n",
        "\n",
        "news_df = pd.DataFrame({'headline': news_headlines})\n",
        "news_df.to_csv('data/news_headlines_sample.csv', index=False)\n",
        "\n",
        "print(f\"\\nNews Headlines Dataset:\")\n",
        "print(news_df)\n",
        "print(f\"\\nDataset shape: {news_df.shape}\")\n",
        "\n",
        "print(f\"\\nâœ… Sample datasets created and saved to 'data/' directory!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdb59013",
      "metadata": {
        "id": "fdb59013"
      },
      "source": [
        "## Section 4: Quick Exercise - Your First NLP Task\n",
        "\n",
        "Let's practice with a simple exercise to get you comfortable with basic text processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "565bec60",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "565bec60",
        "outputId": "43e08712-219a-459d-a1ad-30eb08d3e1bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“Š Text Analysis Results:\n",
            "==============================\n",
            "Word Count: 34\n",
            "Sentence Count: 3\n",
            "Average Word Length: 6.29\n",
            "Most Frequent Word: language (2 times)\n",
            "Unique Words: 30\n",
            "\n",
            "ðŸŽ¬ Analyzing movie reviews...\n",
            "\n",
            "Review 1:\n",
            "Words: 9, Unique: 9, Avg length: 6.0\n",
            "\n",
            "Review 2:\n",
            "Words: 8, Unique: 7, Avg length: 4.88\n",
            "\n",
            "Review 3:\n",
            "Words: 5, Unique: 5, Avg length: 9.8\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def analyze_text(text):\n",
        "    \"\"\"\n",
        "    Analyze basic statistics of a text string.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to analyze\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing text statistics\n",
        "    \"\"\"\n",
        "    # Extract words using regex (only alphanumeric)\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "\n",
        "    # Extract sentence-ending punctuation as a proxy for sentence count\n",
        "    sentences = re.findall(r'[.!?]+', text)\n",
        "\n",
        "    # Compute stats\n",
        "    num_words = len(words)\n",
        "    num_sentences = len(sentences)\n",
        "    avg_word_length = round(sum(len(word) for word in words) / num_words, 2) if num_words > 0 else 0\n",
        "    word_freq = Counter(words)\n",
        "    most_frequent = word_freq.most_common(1)[0] if word_freq else (\"\", 0)\n",
        "    unique_words = len(set(words))\n",
        "\n",
        "    return {\n",
        "        \"word_count\": num_words,\n",
        "        \"sentence_count\": num_sentences,\n",
        "        \"average_word_length\": avg_word_length,\n",
        "        \"most_frequent_word\": f\"{most_frequent[0]} ({most_frequent[1]} times)\",\n",
        "        \"unique_words\": unique_words\n",
        "    }\n",
        "\n",
        "# Sample text to test the function\n",
        "sample_text = \"\"\"\n",
        "Natural Language Processing is a fascinating field of artificial intelligence.\n",
        "It combines computational linguistics with machine learning and deep learning.\n",
        "NLP enables computers to understand, interpret, and generate human language in a valuable way!\n",
        "\"\"\"\n",
        "\n",
        "# Run and display results\n",
        "result = analyze_text(sample_text)\n",
        "print(\"ðŸ“Š Text Analysis Results:\")\n",
        "print(\"=\" * 30)\n",
        "for key, value in result.items():\n",
        "    print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
        "\n",
        "# Analyze the first 3 reviews from the movie review dataset (assumes reviews_df is loaded)\n",
        "print(f\"\\nðŸŽ¬ Analyzing movie reviews...\")\n",
        "for i, review in enumerate(reviews_df['review'].head(3), 1):\n",
        "    print(f\"\\nReview {i}:\")\n",
        "    stats = analyze_text(review)\n",
        "    print(f\"Words: {stats['word_count']}, Unique: {stats['unique_words']}, Avg length: {stats['average_word_length']}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}