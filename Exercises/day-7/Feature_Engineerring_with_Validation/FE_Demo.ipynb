{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/appliedcode/mthree-c422/blob/main/Exercises/day-6/Feature_Engineerring_with_Validation/FE_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering and Validation Pipeline with the Titanic Dataset\n",
        "Build an automated, scalable pipeline in Google Colab that performs feature engineering, data validation, and outputs a clean dataset ready for modeling.\n",
        "\n",
        "**Objectives**\n",
        "- Load and split the Titanic dataset.\n",
        "\n",
        "- Apply feature engineering (including new features and encoding).\n",
        "\n",
        "- Implement validation checks to ensure data quality.\n",
        "\n",
        "- Modularize the steps into reusable functions.\n",
        "\n",
        "- Demonstrate end-to-end pipeline execution."
      ],
      "metadata": {
        "id": "RpsOG3v9XMxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install and Import Libraries\n",
        "!pip install pandas numpy scikit-learn -q\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.exceptions import DataConversionWarning\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for clearer output\n",
        "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n"
      ],
      "metadata": {
        "id": "2Z8FA56ruhq0",
        "outputId": "a4121f11-aba6-44e5-b61b-a7b24b7c0a61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load and Split the Dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Split into train and validation sets\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['Survived'])\n"
      ],
      "metadata": {
        "id": "CiVU5fePukA2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Define the Pipeline Functions\n",
        "def clean_data(df):\n",
        "    df = df.drop(columns=[\"PassengerId\",\"Ticket\",\"Cabin\"], errors=\"ignore\").copy()\n",
        "    # Impute Age and Embarked\n",
        "    df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].median())\n",
        "    df[\"Embarked\"] = df[\"Embarked\"].fillna(df[\"Embarked\"].mode()[0])\n",
        "    return df\n",
        "\n",
        "def engineer_features(df):\n",
        "    df = df.copy()\n",
        "    # Title extraction\n",
        "    df[\"Title\"] = df[\"Name\"].str.extract(r\",\\s*([^\\.]+)\\.\")\n",
        "    rare_titles = [\"Lady\",\"Countess\",\"Capt\",\"Col\",\"Don\",\"Dr\",\"Major\",\"Rev\",\"Sir\",\"Jonkheer\",\"Dona\"]\n",
        "    df[\"Title\"] = df[\"Title\"].replace(rare_titles, \"Rare\")\n",
        "    # Family size & is alone\n",
        "    df[\"FamilySize\"] = df[\"SibSp\"] + df[\"Parch\"] + 1\n",
        "    df[\"IsAlone\"] = (df[\"FamilySize\"] == 1).astype(int)\n",
        "    # Fare and Age bins\n",
        "    df[\"FareBin\"] = pd.qcut(df[\"Fare\"].fillna(0), 4, labels=False)\n",
        "    df[\"AgeBin\"]  = pd.cut(df[\"Age\"], bins=[0,12,20,40,60,100], labels=False)\n",
        "    # Drop unused columns\n",
        "    df = df.drop(columns=[\"Name\",\"SibSp\",\"Parch\"])\n",
        "    # One-hot encode\n",
        "    df = pd.get_dummies(df, columns=[\"Sex\",\"Embarked\",\"Title\"], drop_first=True)\n",
        "    return df\n",
        "\n",
        "def validate_data(df):\n",
        "    errors = []\n",
        "    # Check for nulls\n",
        "    null_counts = df.isnull().sum()\n",
        "    if null_counts.any():\n",
        "        errors.append(f\"Null values found:\\n{null_counts[null_counts>0]}\")\n",
        "    # Check expected columns\n",
        "    expected_cols = {\"Survived\",\"Pclass\",\"Age\",\"Fare\",\"FamilySize\",\"IsAlone\",\"FareBin\",\"AgeBin\"}\n",
        "    missing = expected_cols - set(df.columns)\n",
        "    if missing:\n",
        "        errors.append(f\"Missing columns: {missing}\")\n",
        "    return errors\n"
      ],
      "metadata": {
        "id": "MrPB0SBQumWn"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Execute the Pipeline\n",
        "# Clean and feature-engineer training data\n",
        "train_clean = clean_data(train_df)\n",
        "train_feat  = engineer_features(train_clean)\n",
        "train_errors = validate_data(train_feat)\n",
        "print(\"Train validation errors:\", train_errors or \"None\")\n",
        "\n",
        "# Clean and feature-engineer validation data\n",
        "val_clean = clean_data(val_df)\n",
        "val_feat  = engineer_features(val_clean)\n",
        "val_errors = validate_data(val_feat)\n",
        "print(\"Validation validation errors:\", val_errors or \"None\")\n"
      ],
      "metadata": {
        "id": "Sl1uS4dWuo1c",
        "outputId": "f116bfd0-0b32-4eba-87a9-f7793056e60f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train validation errors: None\n",
            "Validation validation errors: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Save Prepared Data\n",
        "train_feat.to_csv(\"titanic_train_prepared.csv\", index=False)\n",
        "val_feat.to_csv(\"titanic_val_prepared.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "uA4dh1OFurvn"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reflection Questions\n",
        "- How did validation checks help catch data quality issues early?\n",
        "\n",
        "- Which engineered features contributed most to dataset richness?\n",
        "\n",
        "- How would you extend this pipeline to include scaling, imputation strategies, or integrate with a model training step?\n",
        "\n"
      ],
      "metadata": {
        "id": "E4uyKQl9uwbK"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}