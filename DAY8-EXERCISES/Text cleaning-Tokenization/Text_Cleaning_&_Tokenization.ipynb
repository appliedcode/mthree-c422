{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b183244d",
      "metadata": {
        "id": "b183244d",
        "outputId": "f9de5d24-fd24-466a-dd6d-6b36ffd25f85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ spaCy model loaded successfully!\n",
            "Sample Messy Texts:\n",
            "==================================================\n",
            " 1. 'Hey there!!! Check out this AMAZING deal at https://example.com/sale üéâ'\n",
            " 2. 'EMAIL me at john.doe@company.com for more INFO!!!'\n",
            " 3. '   So much    extra    whitespace   everywhere   '\n",
            " 4. 'HTML content: <p>This is a paragraph</p> with <b>bold</b> text'\n",
            " 5. 'Phone: +1-555-123-4567 or call (555) 987-6543 today!'\n",
            " 6. 'Social media: Follow @username #trending #viral #amazing'\n",
            " 7. 'Mixed CASE and 123 numbers and $pecial ch@r@cters!!!'\n",
            " 8. 'Line breaks\\nand\\ttabs\\teverywhere\\r\\n'\n",
            " 9. \"Contractions like don't, won't, I'm, and we're are common\"\n",
            "10. 'Numbers: 1st, 2nd, 3rd places and dates like 01/15/2024'\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Download required NLTK data (if not already downloaded)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "# Load spaCy model (install with: python -m spacy download en_core_web_sm)\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    print(\"‚úÖ spaCy model loaded successfully!\")\n",
        "except OSError:\n",
        "    print(\"‚ùå Please install spaCy English model: python -m spacy download en_core_web_sm\")\n",
        "    nlp = None\n",
        "\n",
        "# Sample messy text data for demonstration\n",
        "messy_texts = [\n",
        "    \"Hey there!!! Check out this AMAZING deal at https://example.com/sale üéâ\",\n",
        "    \"EMAIL me at john.doe@company.com for more INFO!!!\",\n",
        "    \"   So much    extra    whitespace   everywhere   \",\n",
        "    \"HTML content: <p>This is a paragraph</p> with <b>bold</b> text\",\n",
        "    \"Phone: +1-555-123-4567 or call (555) 987-6543 today!\",\n",
        "    \"Social media: Follow @username #trending #viral #amazing\",\n",
        "    \"Mixed CASE and 123 numbers and $pecial ch@r@cters!!!\",\n",
        "    \"Line breaks\\nand\\ttabs\\teverywhere\\r\\n\",\n",
        "    \"Contractions like don't, won't, I'm, and we're are common\",\n",
        "    \"Numbers: 1st, 2nd, 3rd places and dates like 01/15/2024\"\n",
        "]\n",
        "\n",
        "print(\"Sample Messy Texts:\")\n",
        "print(\"=\" * 50)\n",
        "for i, text in enumerate(messy_texts, 1):\n",
        "    print(f\"{i:2d}. {repr(text)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ddccaa4",
      "metadata": {
        "id": "6ddccaa4"
      },
      "source": [
        "Text Cleaning with Regular Expressions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "dfea5f4e",
      "metadata": {
        "id": "dfea5f4e",
        "outputId": "3d1c8a47-fed6-4f01-dff9-9a7e1f00cf66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEXT CLEANING DEMONSTRATION\n",
            "================================================================================\n",
            "\n",
            "Example 1:\n",
            "Original: 'Hey there!!! Check out this AMAZING deal at https://example.com/sale üéâ'\n",
            "Display:  Hey there!!! Check out this AMAZING deal at https://example.com/sale üéâ\n",
            "--------------------------------------------------------------------------------\n",
            "After URL removal: Hey there!!! Check out this AMAZING deal at  üéâ\n",
            "After email removal: Hey there!!! Check out this AMAZING deal at  üéâ\n",
            "After HTML removal: Hey there!!! Check out this AMAZING deal at  üéâ\n",
            "After phone removal: Hey there!!! Check out this AMAZING deal at  üéâ\n",
            "After social media removal: Hey there!!! Check out this AMAZING deal at  üéâ\n",
            "After whitespace normalization: Hey there!!! Check out this AMAZING deal at üéâ\n",
            "After lowercasing: hey there!!! check out this amazing deal at üéâ\n",
            "================================================================================\n",
            "\n",
            "Final result: 'hey there!!! check out this amazing deal at üéâ'\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Example 2:\n",
            "Original: 'EMAIL me at john.doe@company.com for more INFO!!!'\n",
            "Display:  EMAIL me at john.doe@company.com for more INFO!!!\n",
            "--------------------------------------------------------------------------------\n",
            "After URL removal: EMAIL me at john.doe@company.com for more INFO!!!\n",
            "After email removal: EMAIL me at  for more INFO!!!\n",
            "After HTML removal: EMAIL me at  for more INFO!!!\n",
            "After phone removal: EMAIL me at  for more INFO!!!\n",
            "After social media removal: EMAIL me at  for more INFO!!!\n",
            "After whitespace normalization: EMAIL me at for more INFO!!!\n",
            "After lowercasing: email me at for more info!!!\n",
            "================================================================================\n",
            "\n",
            "Final result: 'email me at for more info!!!'\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Example 3:\n",
            "Original: '   So much    extra    whitespace   everywhere   '\n",
            "Display:     So much    extra    whitespace   everywhere   \n",
            "--------------------------------------------------------------------------------\n",
            "After URL removal:    So much    extra    whitespace   everywhere   \n",
            "After email removal:    So much    extra    whitespace   everywhere   \n",
            "After HTML removal:    So much    extra    whitespace   everywhere   \n",
            "After phone removal:    So much    extra    whitespace   everywhere   \n",
            "After social media removal:    So much    extra    whitespace   everywhere   \n",
            "After whitespace normalization: So much extra whitespace everywhere\n",
            "After lowercasing: so much extra whitespace everywhere\n",
            "================================================================================\n",
            "\n",
            "Final result: 'so much extra whitespace everywhere'\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Step-by-step text cleaning demonstration\n",
        "\n",
        "def demonstrate_cleaning_steps(text):\n",
        "    \"\"\"Show each cleaning step with before/after examples\"\"\"\n",
        "    print(f\"Original: {repr(text)}\")\n",
        "    print(f\"Display:  {text}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Step 1: Remove URLs\n",
        "    import re\n",
        "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "    step1 = re.sub(url_pattern, '', text)\n",
        "    print(f\"After URL removal: {step1}\")\n",
        "\n",
        "    # Step 2: Remove email addresses\n",
        "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
        "    step2 = re.sub(email_pattern, '', step1)\n",
        "    print(f\"After email removal: {step2}\")\n",
        "\n",
        "    # Step 3: Remove HTML tags\n",
        "    html_pattern = r'<[^>]+>'\n",
        "    step3 = re.sub(html_pattern, '', step2)\n",
        "    print(f\"After HTML removal: {step3}\")\n",
        "\n",
        "    # Step 4: Remove phone numbers\n",
        "    phone_pattern = r'\\+?1?[-.\\s]?\\(?[0-9]{3}\\)?[-.\\s]?[0-9]{3}[-.\\s]?[0-9]{4}'\n",
        "    step4 = re.sub(phone_pattern, '', step3)\n",
        "    print(f\"After phone removal: {step4}\")\n",
        "\n",
        "    # Step 5: Remove social media elements\n",
        "    hashtag_pattern = r'#\\w+'\n",
        "    mention_pattern = r'@\\w+'\n",
        "    step5 = re.sub(hashtag_pattern, '', step4)\n",
        "    step5 = re.sub(mention_pattern, '', step5)\n",
        "    print(f\"After social media removal: {step5}\")\n",
        "\n",
        "    # Step 6: Normalize whitespace\n",
        "    whitespace_pattern = r'\\s+'\n",
        "    step6 = re.sub(whitespace_pattern, ' ', step5).strip()\n",
        "    print(f\"After whitespace normalization: {step6}\")\n",
        "\n",
        "    # Step 7: Convert to lowercase\n",
        "    step7 = step6.lower()\n",
        "    print(f\"After lowercasing: {step7}\")\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    return step7\n",
        "\n",
        "# Test with our messy examples\n",
        "print(\"TEXT CLEANING DEMONSTRATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, text in enumerate(messy_texts[:3], 1):  # Show first 3 examples\n",
        "    print(f\"\\nExample {i}:\")\n",
        "    cleaned = demonstrate_cleaning_steps(text)\n",
        "    print(f\"\\nFinal result: '{cleaned}'\")\n",
        "    print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "accbf079",
      "metadata": {
        "id": "accbf079"
      },
      "source": [
        "Section 2: Tokenization - Breaking Text into Meaningful Units\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b3456766",
      "metadata": {
        "id": "b3456766",
        "outputId": "800f63fc-9327-4029-8e93-f219026ceb9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOKENIZATION COMPARISON\n",
            "============================================================\n",
            "Sample text: Dr. Smith's research on AI (published in 2024) shows that machine learning can't\n",
            "solve every problem. However, it's incredibly useful! Visit https://example.com\n",
            "for more details. What do you think?\n",
            "------------------------------------------------------------\n",
            "\n",
            "1. SIMPLE SPLIT (29 tokens):\n",
            "   ['Dr.', \"Smith's\", 'research', 'on', 'AI', '(published', 'in', '2024)', 'shows', 'that', 'machine', 'learning', \"can't\", 'solve', 'every', 'problem.', 'However,', \"it's\", 'incredibly', 'useful!', 'Visit', 'https://example.com', 'for', 'more', 'details.', 'What', 'do', 'you', 'think?']\n",
            "\n",
            "2. NLTK WORD TOKENIZE (41 tokens):\n",
            "   ['Dr.', 'Smith', \"'s\", 'research', 'on', 'AI', '(', 'published', 'in', '2024', ')', 'shows', 'that', 'machine', 'learning', 'ca', \"n't\", 'solve', 'every', 'problem', '.', 'However', ',', 'it', \"'s\", 'incredibly', 'useful', '!', 'Visit', 'https', ':', '//example.com', 'for', 'more', 'details', '.', 'What', 'do', 'you', 'think', '?']\n",
            "\n",
            "3. NLTK SENTENCE TOKENIZE (4 sentences):\n",
            "   1. Dr. Smith's research on AI (published in 2024) shows that machine learning can't\n",
            "solve every problem.\n",
            "   2. However, it's incredibly useful!\n",
            "   3. Visit https://example.com\n",
            "for more details.\n",
            "   4. What do you think?\n",
            "\n",
            "4. SPACY TOKENIZE (43 tokens):\n",
            "   ['\\n', 'Dr.', 'Smith', \"'s\", 'research', 'on', 'AI', '(', 'published', 'in', '2024', ')', 'shows', 'that', 'machine', 'learning', 'ca', \"n't\", '\\n', 'solve', 'every', 'problem', '.', 'However', ',', 'it', \"'s\", 'incredibly', 'useful', '!', 'Visit', 'https://example.com', '\\n', 'for', 'more', 'details', '.', 'What', 'do', 'you', 'think', '?', '\\n']\n",
            "\n",
            "5. SPACY SENTENCES (4 sentences):\n",
            "   1. Dr. Smith's research on AI (published in 2024) shows that machine learning can't\n",
            "solve every problem.\n",
            "   2. However, it's incredibly useful!\n",
            "   3. Visit https://example.com\n",
            "for more details.\n",
            "   4. What do you think?\n",
            "\n",
            "6. REGEX TOKENIZE (\\b\\w+\\b) (34 tokens):\n",
            "   ['Dr', 'Smith', 's', 'research', 'on', 'AI', 'published', 'in', '2024', 'shows', 'that', 'machine', 'learning', 'can', 't', 'solve', 'every', 'problem', 'However', 'it', 's', 'incredibly', 'useful', 'Visit', 'https', 'example', 'com', 'for', 'more', 'details', 'What', 'do', 'you', 'think']\n",
            "\n",
            "============================================================\n",
            "Key Differences:\n",
            "‚Ä¢ Simple split: Fastest but crude, keeps punctuation\n",
            "‚Ä¢ NLTK: Good balance, handles punctuation well\n",
            "‚Ä¢ spaCy: Most sophisticated, includes linguistic analysis\n",
            "‚Ä¢ Regex: Customizable but requires pattern knowledge\n"
          ]
        }
      ],
      "source": [
        "# Tokenization Comparison: NLTK vs spaCy vs Simple Split\n",
        "\n",
        "sample_text = \"\"\"\n",
        "Dr. Smith's research on AI (published in 2024) shows that machine learning can't\n",
        "solve every problem. However, it's incredibly useful! Visit https://example.com\n",
        "for more details. What do you think?\n",
        "\"\"\"\n",
        "\n",
        "print(\"TOKENIZATION COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Sample text: {sample_text.strip()}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Method 1: Simple Python split\n",
        "simple_tokens = sample_text.split()\n",
        "print(f\"\\n1. SIMPLE SPLIT ({len(simple_tokens)} tokens):\")\n",
        "print(f\"   {simple_tokens}\")\n",
        "\n",
        "# Method 2: NLTK word tokenization\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk_word_tokens = word_tokenize(sample_text)\n",
        "print(f\"\\n2. NLTK WORD TOKENIZE ({len(nltk_word_tokens)} tokens):\")\n",
        "print(f\"   {nltk_word_tokens}\")\n",
        "\n",
        "# Method 3: NLTK sentence tokenization\n",
        "nltk_sent_tokens = sent_tokenize(sample_text)\n",
        "print(f\"\\n3. NLTK SENTENCE TOKENIZE ({len(nltk_sent_tokens)} sentences):\")\n",
        "for i, sent in enumerate(nltk_sent_tokens, 1):\n",
        "    print(f\"   {i}. {sent.strip()}\")\n",
        "\n",
        "# Method 4: spaCy tokenization (if available)\n",
        "if nlp:\n",
        "    doc = nlp(sample_text)\n",
        "    spacy_tokens = [token.text for token in doc]\n",
        "    print(f\"\\n4. SPACY TOKENIZE ({len(spacy_tokens)} tokens):\")\n",
        "    print(f\"   {spacy_tokens}\")\n",
        "\n",
        "    # spaCy sentences\n",
        "    spacy_sents = [sent.text.strip() for sent in doc.sents]\n",
        "    print(f\"\\n5. SPACY SENTENCES ({len(spacy_sents)} sentences):\")\n",
        "    for i, sent in enumerate(spacy_sents, 1):\n",
        "        print(f\"   {i}. {sent}\")\n",
        "\n",
        "# Method 5: Regular expression tokenization\n",
        "import re\n",
        "regex_tokens = re.findall(r'\\b\\w+\\b', sample_text)\n",
        "print(f\"\\n6. REGEX TOKENIZE (\\\\b\\\\w+\\\\b) ({len(regex_tokens)} tokens):\")\n",
        "print(f\"   {regex_tokens}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Key Differences:\")\n",
        "print(\"‚Ä¢ Simple split: Fastest but crude, keeps punctuation\")\n",
        "print(\"‚Ä¢ NLTK: Good balance, handles punctuation well\")\n",
        "print(\"‚Ä¢ spaCy: Most sophisticated, includes linguistic analysis\")\n",
        "print(\"‚Ä¢ Regex: Customizable but requires pattern knowledge\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}