{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring Core NLP Concepts\n",
        "Welcome to this hands-on NLP Colab lab! You will work through key tasks—tokenization, POS tagging, stemming, stop-word filtering, vocabulary matching, lemmatization, dependency parsing, NER, and intent classification—using Python libraries. Follow the instructions and complete the exercises."
      ],
      "metadata": {
        "id": "sDeW9pllWu7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install --upgrade pip setuptools wheel -q\n",
        "!pip install --quiet nltk spacy textblob sklearn\n",
        "\n",
        "# Download NLTK data and spaCy model\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "!python -m spacy download en_core_web_sm -q\n"
      ],
      "metadata": {
        "id": "yMULvewKWrzm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6285227a-b8af-423c-8142-c03e1dbb0b69"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.8 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Tokenization\n",
        "# Goal: Split text into tokens (words and punctuation).\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "text = \"Natural Language Processing enables machines to understand human language.\"\n",
        "print(\"Sentences:\", sent_tokenize(text))\n",
        "print(\"Tokens:\", word_tokenize(text))"
      ],
      "metadata": {
        "id": "vNgJ7l_9Wzfo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd85b04b-8173-4178-a3c0-903663ac0875"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences: ['Natural Language Processing enables machines to understand human language.']\n",
            "Tokens: ['Natural', 'Language', 'Processing', 'enables', 'machines', 'to', 'understand', 'human', 'language', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 1.1: Tokenize the following paragraph into words and sentences:\n",
        "\n",
        "paragraph = \"Machine learning models power many NLP tasks. They learn patterns from data!\"\n",
        "def tokenize_paragraph(paragraph):\n",
        "    sentences = sent_tokenize(paragraph)\n",
        "    tokens = [word_tokenize(sentence) for sentence in sentences]\n",
        "    return sentences, tokens\n",
        "\n",
        "# Test the function and print results\n",
        "sentences, tokens = tokenize_paragraph(paragraph)\n",
        "\n",
        "print(\"=== Sentences ===\")\n",
        "for i, sentence in enumerate(sentences, 1):\n",
        "    print(f\"{i}: {sentence}\")\n",
        "\n",
        "print(\"\\n=== Tokens ===\")\n",
        "for i, token_list in enumerate(tokens, 1):\n",
        "    print(f\"Sentence {i} tokens: {token_list}\")"
      ],
      "metadata": {
        "id": "Iwanu3kzW3KU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eed088d6-5c8a-40e4-facc-12d4ce0e7396"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Sentences ===\n",
            "1: Machine learning models power many NLP tasks.\n",
            "2: They learn patterns from data!\n",
            "\n",
            "=== Tokens ===\n",
            "Sentence 1 tokens: ['Machine', 'learning', 'models', 'power', 'many', 'NLP', 'tasks', '.']\n",
            "Sentence 2 tokens: ['They', 'learn', 'patterns', 'from', 'data', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Part-of-Speech Tagging\n",
        "# Goal: Assign grammatical tags to each token.\n",
        "import nltk\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(pos_tags)\n"
      ],
      "metadata": {
        "id": "VaJMjJLOXCV9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1b84246-ad13-4f23-90b9-ba4ba4348b3a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('enables', 'VBZ'), ('machines', 'NNS'), ('to', 'TO'), ('understand', 'VB'), ('human', 'JJ'), ('language', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2.1: Tag POS for tokens from your Exercise 1.1.\n",
        "def tag_pos(tokens):\n",
        "    return nltk.pos_tag(tokens)\n",
        "\n",
        "def tag_pos(tokens):\n",
        "    return nltk.pos_tag(tokens)\n",
        "\n",
        "pos_result = tag_pos(tokens)\n",
        "\n",
        "print(\"=== POS Tags ===\")\n",
        "for word, pos in pos_result:\n",
        "    print(f\"{word}: {pos}\")"
      ],
      "metadata": {
        "id": "rjdfixgHXLAe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b5f4464-8036-4516-8f2f-10ea50962759"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== POS Tags ===\n",
            "Natural: JJ\n",
            "Language: NNP\n",
            "Processing: NNP\n",
            "enables: VBZ\n",
            "machines: NNS\n",
            "to: TO\n",
            "understand: VB\n",
            "human: JJ\n",
            "language: NN\n",
            ".: .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Stemming\n",
        "# Goal: Reduce words to their root forms (may be non-dictionary).\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "words = [\"running\", \"runs\", \"ran\", \"easily\", \"fairly\"]\n",
        "print({w: stemmer.stem(w) for w in words})\n"
      ],
      "metadata": {
        "id": "8Q_ASJ2qXMJ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66a3c3f6-a8fd-4092-fd4c-5bb6c55155da"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'running': 'run', 'runs': 'run', 'ran': 'ran', 'easily': 'easili', 'fairly': 'fairli'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3.1: Stem the tokens from your Exercise 1.1.\n",
        "def stem_tokens(tokens):\n",
        "    return [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "stemmed_tokens = stem_tokens(tokens)\n",
        "\n",
        "print(\"=== Stemmed Tokens ===\")\n",
        "for token, stemmed_token in zip(tokens, stemmed_tokens):\n",
        "    print(f\"{token} -> {stemmed_token}\")"
      ],
      "metadata": {
        "id": "KpJNVgRhXTjF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "201fdf13-7969-40ca-df30-9274c9e8062a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Stemmed Tokens ===\n",
            "Natural -> natur\n",
            "Language -> languag\n",
            "Processing -> process\n",
            "enables -> enabl\n",
            "machines -> machin\n",
            "to -> to\n",
            "understand -> understand\n",
            "human -> human\n",
            "language -> languag\n",
            ". -> .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Stop-Word Filtering\n",
        "# Goal: Remove common, low-value words.\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = word_tokenize(text.lower())\n",
        "filtered = [w for w in tokens if w.isalpha() and w not in stop_words]\n",
        "print(filtered)"
      ],
      "metadata": {
        "id": "kBo5Lo3hXVPk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5a20777-683c-4558-e38f-d5bce1f1c8d2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natural', 'language', 'processing', 'enables', 'machines', 'understand', 'human', 'language']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 4.1: Filter stop words from your Exercise 1.1 tokens.\n",
        "def filter_stop_words(tokens):\n",
        "    return [w for w in tokens if w.isalpha() and w.lower() not in stop_words]\n",
        "\n",
        "filtered_tokens = filter_stop_words(tokens)\n",
        "print(\"=== Filtered Tokens ===\")\n",
        "print(filtered_tokens)\n"
      ],
      "metadata": {
        "id": "_CVnLYVWXakJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0869669a-b075-4a41-f3cb-e5226263e1a0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Filtered Tokens ===\n",
            "['natural', 'language', 'processing', 'enables', 'machines', 'understand', 'human', 'language']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Vocabulary Matching\n",
        "# Goal: Check tokens against a predefined vocabulary.\n",
        "\n",
        "vocab = {\"natural\", \"language\", \"machine\", \"data\", \"processing\"}\n",
        "tokens = [w.lower() for w in word_tokenize(text)]\n",
        "in_vocab = [w for w in tokens if w.isalpha() and w in vocab]\n",
        "print(\"In-vocab tokens:\", in_vocab)\n",
        "print(\"OOV tokens:\", [w for w in tokens if w.isalpha() and w not in vocab])\n"
      ],
      "metadata": {
        "id": "IYEs9fy5XcVy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73ed1f2b-9b17-4710-f9b8-cf438c4afebc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In-vocab tokens: ['natural', 'language', 'processing', 'language']\n",
            "OOV tokens: ['enables', 'machines', 'to', 'understand', 'human']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 5.1: Define your own small vocabulary and classify tokens from Exercise 1.1 into in-vocab vs. out-of-vocab.\n",
        "vocab = {\"machine\", \"artificial\" \"learning\", \"data\", \"language\"}\n",
        "\n",
        "in_vocab = [w for w in filtered_tokens if w in vocab]\n",
        "out_of_vocab = [w for w in filtered_tokens if w not in vocab]\n",
        "\n",
        "print(\"=== In-Vocab Tokens ===\")\n",
        "print(in_vocab)\n",
        "\n",
        "print(\"\\n=== Out-of-Vocab Tokens ===\")\n",
        "print(out_of_vocab)\n"
      ],
      "metadata": {
        "id": "YsKmsc4AXgXa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbdd862b-9eab-494d-a85b-faadfa1c3d74"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== In-Vocab Tokens ===\n",
            "['language', 'language']\n",
            "\n",
            "=== Out-of-Vocab Tokens ===\n",
            "['natural', 'processing', 'enables', 'machines', 'understand', 'human']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Lemmatization\n",
        "# Goal: Convert words to their dictionary form.\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [\"running\", \"better\", \"wolves\"]\n",
        "print({w: lemmatizer.lemmatize(w) for w in words})\n",
        "# For verbs:\n",
        "print(\"run (verb):\", lemmatizer.lemmatize(\"running\", pos='v'))\n"
      ],
      "metadata": {
        "id": "RDcsQ-_CXjyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de1cbff6-caa5-498f-a2df-1521708d6016"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'running': 'running', 'better': 'better', 'wolves': 'wolf'}\n",
            "run (verb): run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 6.1: Lemmatize tokens from Exercise 1.1 (both default and verb POS).\n",
        "def lemmatize_tokens(tokens):\n",
        "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "def lemmatize_tokens_verbs(tokens):\n",
        "    return [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
        "\n",
        "lemmatized_tokens = lemmatize_tokens(tokens)\n",
        "lemmatized_tokens_verbs = lemmatize_tokens_verbs(tokens)\n",
        "print(\"=== Lemmatized Tokens ===\")\n",
        "print(lemmatized_tokens)\n",
        "print(\"=== Lemmatized Verbs ===\")\n",
        "print(lemmatized_tokens_verbs)\n",
        "\n"
      ],
      "metadata": {
        "id": "g74PrNQ0XqWx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66c63dfb-0e53-4bc8-e99d-3bf5becc1182"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Lemmatized Tokens ===\n",
            "['natural', 'language', 'processing', 'enables', 'machine', 'to', 'understand', 'human', 'language', '.']\n",
            "=== Lemmatized Verbs ===\n",
            "['natural', 'language', 'process', 'enable', 'machine', 'to', 'understand', 'human', 'language', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Dependency Parsing\n",
        "# Goal: Identify syntactic relationships between tokens.\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "    print(token.text, token.dep_, token.head.text)\n"
      ],
      "metadata": {
        "id": "4drXo7zjXss9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aef48f1-f486-4307-cecb-71920a8a56a4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural compound Language\n",
            "Language compound Processing\n",
            "Processing nsubj enables\n",
            "enables ROOT enables\n",
            "machines nsubj understand\n",
            "to aux understand\n",
            "understand ccomp enables\n",
            "human amod language\n",
            "language dobj understand\n",
            ". punct enables\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 7.1: Parse the sentence “They learn patterns from data” and list each token’s dependency label and head.\n",
        "doc = nlp(\"They learn patterns from data\")\n",
        "for token in doc:\n",
        "    print(token.text, token.dep_, token.head.text)"
      ],
      "metadata": {
        "id": "LmRo7ZwEXvGH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f68110a-fefd-4d7e-bfe1-5127842b26e9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "They nsubj learn\n",
            "learn ROOT learn\n",
            "patterns dobj learn\n",
            "from prep patterns\n",
            "data pobj from\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Named-Entity Recognition (NER)\n",
        "# Goal: Extract real-world entities from text.\n",
        "doc = nlp(\"Google was founded in 1998 by Larry Page and Sergey Brin in California.\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "metadata": {
        "id": "k78PeFooXxwp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d33a7d16-d18a-4957-e5e5-d9f6ee1c6da1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google ORG\n",
            "1998 DATE\n",
            "Larry Page PERSON\n",
            "Sergey Brin PERSON\n",
            "California GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exercise 8.1: Run NER on this sentence and add at least two more sentences of your own.\n",
        "pap_sentence = \"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\"\n",
        "pap_sentence += \"However little known the feelings or views of such a man may be on his first entering a neighbourhood, this truth is so well fixed in the minds of the surrounding families, that he is considered as the rightful property of some one or other of their daughters.\"\n",
        "pap_sentence += \"My dear Mr. Bennet,” said his lady to him one day, “have you heard that Netherfield Park is let at last?\"\n",
        "pap_sentence += \"The novel was published in 1813 and remains a classic of English literature.\"\n",
        "\n",
        "doc = nlp(pap_sentence)\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n",
        "    print(f\"{ent.text} ({ent.label_})\")"
      ],
      "metadata": {
        "id": "bwBm0NMJX3WN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbaffab6-c2e6-4151-cfd8-bce48b0c1e68"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first ORDINAL\n",
            "first (ORDINAL)\n",
            "some one CARDINAL\n",
            "some one (CARDINAL)\n",
            "Bennet PERSON\n",
            "Bennet (PERSON)\n",
            "one day DATE\n",
            "one day (DATE)\n",
            "Netherfield Park FAC\n",
            "Netherfield Park (FAC)\n",
            "1813 DATE\n",
            "1813 (DATE)\n",
            "English LANGUAGE\n",
            "English (LANGUAGE)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}