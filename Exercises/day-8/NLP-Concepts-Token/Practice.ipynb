{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/appliedcode/mthree-c422/blob/mthree-c422-dipti/Exercises/day-8/NLP-Concepts-Token/Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring Core NLP Concepts\n",
        "Welcome to this hands-on NLP Colab lab! You will work through key tasks—tokenization, POS tagging, stemming, stop-word filtering, vocabulary matching, lemmatization, dependency parsing, NER, and intent classification—using Python libraries. Follow the instructions and complete the exercises."
      ],
      "metadata": {
        "id": "sDeW9pllWu7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install --upgrade pip setuptools wheel -q\n",
        "!pip install --quiet nltk spacy textblob sklearn\n",
        "\n",
        "\n",
        "\n",
        "# Download NLTK data and spaCy model\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "!python -m spacy download en_core_web_sm -q\n"
      ],
      "metadata": {
        "id": "yMULvewKWrzm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af5c4017-8c57-415c-8582-8c33e7d8eb41"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/__main__.py\", line 4, in <module>\n",
            "    setup_cli()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/cli/_util.py\", line 87, in setup_cli\n",
            "    command(prog_name=COMMAND)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/core.py\", line 1442, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/typer/core.py\", line 757, in main\n",
            "    return _main(\n",
            "           ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/typer/core.py\", line 195, in _main\n",
            "    rv = self.invoke(ctx)\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/core.py\", line 1830, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/core.py\", line 1226, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/click/core.py\", line 794, in invoke\n",
            "    return callback(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/typer/main.py\", line 699, in wrapper\n",
            "    return callback(**use_params)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/cli/download.py\", line 44, in download_cli\n",
            "    download(model, direct, sdist, *ctx.args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/cli/download.py\", line 95, in download\n",
            "    if is_in_jupyter():\n",
            "       ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/spacy/util.py\", line 1107, in is_in_jupyter\n",
            "    import google.colab\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/__init__.py\", line 21, in <module>\n",
            "    from google.colab import _shell_customizations\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/colab/_shell_customizations.py\", line 23, in <module>\n",
            "    _GREEN = coloransi.TermColors.Green\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: type object 'TermColors' has no attribute 'Green'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Tokenization\n",
        "# Goal: Split text into tokens (words and punctuation).\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "text = \"Natural Language Processing enables machines to understand human language.\"\n",
        "print(\"Sentences:\", sent_tokenize(text))\n",
        "print(\"Tokens:\", word_tokenize(text))"
      ],
      "metadata": {
        "id": "vNgJ7l_9Wzfo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "486c4019-6654-4540-d551-39e889bf9015"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences: ['Natural Language Processing enables machines to understand human language.']\n",
            "Tokens: ['Natural', 'Language', 'Processing', 'enables', 'machines', 'to', 'understand', 'human', 'language', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 1.1: Tokenize the following paragraph into words and sentences:\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "paragraph = \"Machine learning models power many NLP tasks. They learn patterns from data!\"\n",
        "print(\"Sentences:\", sent_tokenize(paragraph))\n",
        "print(\"Tokens:\", word_tokenize(paragraph))"
      ],
      "metadata": {
        "id": "Iwanu3kzW3KU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dcd40f1-c6cf-4320-ec4b-406e580d08e9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences: ['Machine learning models power many NLP tasks.', 'They learn patterns from data!']\n",
            "Tokens: ['Machine', 'learning', 'models', 'power', 'many', 'NLP', 'tasks', '.', 'They', 'learn', 'patterns', 'from', 'data', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Part-of-Speech Tagging\n",
        "# Goal: Assign grammatical tags to each token.\n",
        "import nltk\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(pos_tags)\n"
      ],
      "metadata": {
        "id": "VaJMjJLOXCV9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c10da4d-53b7-47f0-81a0-cd1673560f91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('enables', 'VBZ'), ('machines', 'NNS'), ('to', 'TO'), ('understand', 'VB'), ('human', 'JJ'), ('language', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2.1: Tag POS for tokens from your Exercise 1.1.\n",
        "import nltk\n",
        "tokens = word_tokenize(paragraph)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "id": "rjdfixgHXLAe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3b09665-2834-410d-bec4-2f28c362434d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Machine', 'NN'), ('learning', 'NN'), ('models', 'NNS'), ('power', 'NN'), ('many', 'JJ'), ('NLP', 'NNP'), ('tasks', 'NNS'), ('.', '.'), ('They', 'PRP'), ('learn', 'VBP'), ('patterns', 'NNS'), ('from', 'IN'), ('data', 'NN'), ('!', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Stemming\n",
        "# Goal: Reduce words to their root forms (may be non-dictionary).\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "words = [\"running\", \"runs\", \"ran\", \"easily\", \"fairly\"]\n",
        "print({w: stemmer.stem(w) for w in words})\n"
      ],
      "metadata": {
        "id": "8Q_ASJ2qXMJ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ff7e536-b35e-4135-ece3-acb2515bd066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'running': 'run', 'runs': 'run', 'ran': 'ran', 'easily': 'easili', 'fairly': 'fairli'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3.1: Stem the tokens from your Exercise 1.1.\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "words = [\"Machine\", \"learning\", \"models\", \"power\", \"many\", \"NLP\", \"tasks.\", \"They\", \"learn\", \"patterns\", \"from\", \"data!\"]\n",
        "print({w: stemmer.stem(w) for w in words})\n"
      ],
      "metadata": {
        "id": "KpJNVgRhXTjF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dcf6356-1895-43c6-f065-8234b30137b2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Machine': 'machin', 'learning': 'learn', 'models': 'model', 'power': 'power', 'many': 'mani', 'NLP': 'nlp', 'tasks.': 'tasks.', 'They': 'they', 'learn': 'learn', 'patterns': 'pattern', 'from': 'from', 'data!': 'data!'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Stop-Word Filtering\n",
        "# Goal: Remove common, low-value words.\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = word_tokenize(text.lower())\n",
        "filtered = [w for w in tokens if w.isalpha() and w not in stop_words]\n",
        "print(filtered)"
      ],
      "metadata": {
        "id": "kBo5Lo3hXVPk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99ef8fc8-9a54-4df4-c029-bce6b31cd99f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natural', 'language', 'processing', 'enables', 'machines', 'understand', 'human', 'language']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 4.1: Filter stop words from your Exercise 1.1 tokens.\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = word_tokenize(paragraph.lower())\n",
        "filtered = [w for w in tokens if w.isalpha() and w not in stop_words]\n",
        "print(filtered)"
      ],
      "metadata": {
        "id": "_CVnLYVWXakJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d730208-42b1-4b79-d6e7-4e1cca41416f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['machine', 'learning', 'models', 'power', 'many', 'nlp', 'tasks', 'learn', 'patterns', 'data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Vocabulary Matching\n",
        "# Goal: Check tokens against a predefined vocabulary.\n",
        "\n",
        "vocab = {\"natural\", \"language\", \"machine\", \"data\", \"processing\"}\n",
        "tokens = [w.lower() for w in word_tokenize(text)]\n",
        "in_vocab = [w for w in tokens if w.isalpha() and w in vocab]\n",
        "print(\"In-vocab tokens:\", in_vocab)\n",
        "print(\"OOV tokens:\", [w for w in tokens if w.isalpha() and w not in vocab])\n"
      ],
      "metadata": {
        "id": "IYEs9fy5XcVy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "295b70e2-2bf1-47da-cf4e-540a7896a041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In-vocab tokens: ['natural', 'language', 'processing', 'language']\n",
            "OOV tokens: ['enables', 'machines', 'to', 'understand', 'human']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 5.1: Define your own small vocabulary and classify tokens from Exercise 1.1 into in-vocab vs. out-of-vocab.\n",
        "#\"Machine learning models power many NLP tasks. They learn patterns from data!\"\n",
        "vocab = {\"machine\", \"learn\", \"NLP\", \"patterns\", \"power\"}\n",
        "tokens = [w.lower() for w in word_tokenize(paragraph)]\n",
        "in_vocab = [w for w in tokens if w.isalpha() and w in vocab]\n",
        "print(\"In-vocab tokens:\", in_vocab)\n",
        "print(\"OOV tokens:\", [w for w in tokens if w.isalpha() and w not in vocab])"
      ],
      "metadata": {
        "id": "YsKmsc4AXgXa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d830933f-1dec-4cf8-b465-d382775f8560"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In-vocab tokens: ['machine', 'power', 'learn', 'patterns']\n",
            "OOV tokens: ['learning', 'models', 'many', 'nlp', 'tasks', 'they', 'from', 'data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Lemmatization\n",
        "# Goal: Convert words to their dictionary form.\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [\"running\", \"better\", \"wolves\"]\n",
        "print({w: lemmatizer.lemmatize(w) for w in words})\n",
        "# For verbs:\n",
        "print(\"run (verb):\", lemmatizer.lemmatize(\"running\", pos='v'))\n"
      ],
      "metadata": {
        "id": "RDcsQ-_CXjyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cb73b61-bf82-497a-ab96-c256da87cf61"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'running': 'running', 'better': 'better', 'wolves': 'wolf'}\n",
            "run (verb): run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 6.1: Lemmatize tokens from Exercise 1.1 (both default and verb POS).\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [\"Machine\", \"learning\", \"models\", \"power\", \"many\", \"NLP\", \"tasks\", \"They\", \"learn\", \"patterns\", \"from\", \"data\"]\n",
        "print({w: lemmatizer.lemmatize(w) for w in words})\n",
        "# For verbs:\n",
        "print(\"run (verb):\", lemmatizer.lemmatize(\"running\", pos='v'))\n"
      ],
      "metadata": {
        "id": "g74PrNQ0XqWx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a87b248-98d3-435a-b078-a1d77a986c4a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Machine': 'Machine', 'learning': 'learning', 'models': 'model', 'power': 'power', 'many': 'many', 'NLP': 'NLP', 'tasks': 'task', 'They': 'They', 'learn': 'learn', 'patterns': 'pattern', 'from': 'from', 'data': 'data'}\n",
            "run (verb): run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Dependency Parsing\n",
        "# Goal: Identify syntactic relationships between tokens.\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "    print(token.text, token.dep_, token.head.text)\n"
      ],
      "metadata": {
        "id": "4drXo7zjXss9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55c914ca-ae67-4d3c-bde2-72a67c7e58c2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural compound Language\n",
            "Language compound Processing\n",
            "Processing nsubj enables\n",
            "enables ROOT enables\n",
            "machines nsubj understand\n",
            "to aux understand\n",
            "understand ccomp enables\n",
            "human amod language\n",
            "language dobj understand\n",
            ". punct enables\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 7.1: Parse the sentence “They learn patterns from data” and list each token’s dependency label and head.\n",
        "import spacy\n",
        "setn=\"They learn patterns from data\"\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(setn)\n",
        "for token in doc:\n",
        "    print(token.text, token.dep_, token.head.text)"
      ],
      "metadata": {
        "id": "LmRo7ZwEXvGH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8d0563e-a858-47d3-8985-651926692227"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "They nsubj learn\n",
            "learn ROOT learn\n",
            "patterns dobj learn\n",
            "from prep patterns\n",
            "data pobj from\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Named-Entity Recognition (NER)\n",
        "# Goal: Extract real-world entities from text.\n",
        "doc = nlp(\"Google was founded in 1998 by Larry Page and Sergey Brin in California.\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "metadata": {
        "id": "k78PeFooXxwp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bbb3f10-8ad7-4dc7-df17-d425347c2577"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google ORG\n",
            "1998 DATE\n",
            "Larry Page PERSON\n",
            "Sergey Brin PERSON\n",
            "California GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exercise 8.1: Run NER on this sentence and add at least two more sentences of your own.\n",
        "doc = nlp(u\"Google was founded in 1998 by Larry Page and Sergey Brin in California.\\\n",
        "Because it was his birthday, he got a new bike, and he was very happy.\\\n",
        "          The sun is shining, and the birds are singing.\\\n",
        "          The ball was chased by the dog. \")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n"
      ],
      "metadata": {
        "id": "bwBm0NMJX3WN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a393d51b-cbd6-42b1-9c88-0ef8ebc7ffbc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google ORG\n",
            "1998 DATE\n",
            "Larry Page PERSON\n",
            "Sergey Brin PERSON\n",
            "California GPE\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}