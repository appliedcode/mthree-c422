{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Conversion Techniques: Single-Lab Exercises\n"
      ],
      "metadata": {
        "id": "Q_6EsRS867H0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "su6DvNkG49DL",
        "outputId": "9a0c100b-5a1a-4778-ba85-e02c214aee20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\n",
        "\n",
        "import nltk\n",
        "# Download both old and new tokenizer data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Bag of Words (BoW)\n",
        "Exercise Brief\n",
        "Convert three short product reviews into a Bag-of-Words matrix and compare a manual implementation with CountVectorizer.\n",
        "\n",
        "Reviews\n",
        "‚ÄúThis phone has great battery life‚Äù\n",
        "\n",
        "‚ÄúBattery life on this phone is poor‚Äù\n",
        "\n",
        "‚ÄúI love the camera on this phone‚Äù\n",
        "\n",
        "Tasks\n",
        "Pre-process each review: lowercase, tokenize, remove stop-words.\n",
        "\n",
        "Construct a vocabulary of unique words.\n",
        "\n",
        "Create a frequency vector for every review (manual).\n",
        "\n",
        "Repeat using sklearn.feature_extraction.text.CountVectorizer.\n",
        "\n",
        "Compare the two matrices."
      ],
      "metadata": {
        "id": "vboHmzcU7Awv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import nltk, string, pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"This phone has great battery life\",\n",
        "    \"Battery life on this phone is poor\",\n",
        "    \"I love the camera on this phone\"\n",
        "]\n",
        "\n",
        "# download once per session\n",
        "nltk.download('punkt'); nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
        "    return [w for w in word_tokenize(text) if w not in stop_words]\n",
        "\n",
        "tokens_list = [preprocess(doc) for doc in corpus]\n",
        "vocab = sorted({w for sent in tokens_list for w in sent})\n",
        "\n",
        "def bow_vector(tokens):\n",
        "    return [tokens.count(term) for term in vocab]\n",
        "\n",
        "manual_bow = [bow_vector(t) for t in tokens_list]\n",
        "manual_df  = pd.DataFrame(manual_bow, columns=vocab)\n",
        "\n",
        "cv = CountVectorizer(lowercase=True, stop_words='english')\n",
        "cv_bow = cv.fit_transform(corpus).toarray()\n",
        "cv_df  = pd.DataFrame(cv_bow, columns=cv.get_feature_names_out())\n",
        "\n",
        "print(manual_df, '\\n'); print(cv_df)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbO7DFO87Ev9",
        "outputId": "ed7baa96-3dea-4d6e-e3bc-3c9839bc3a93"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   battery  camera  great  life  love  phone  poor\n",
            "0        1       0      1     1     0      1     0\n",
            "1        1       0      0     1     0      1     1\n",
            "2        0       1      0     0     1      1     0 \n",
            "\n",
            "   battery  camera  great  life  love  phone  poor\n",
            "0        1       0      1     1     0      1     0\n",
            "1        1       0      0     1     0      1     1\n",
            "2        0       1      0     0     1      1     0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß™ Exercise Brief: N-grams Generation and Analysis\n",
        "üìù Sentence\n",
        "‚ÄúNatural language processing is fascinating and powerful‚Äù\n",
        "\n",
        "üß© Tasks\n",
        "1. Tokenize the Sentence\n",
        "Use nltk.word_tokenize to split the sentence into tokens.\n",
        "2. Generate N-grams (n = 1 to 4)\n",
        "Use nltk.util.ngrams to create:\n",
        "Unigrams (n=1)\n",
        "Bigrams (n=2)\n",
        "Trigrams (n=3)\n",
        "Four-grams (n=4)\n",
        "Store:\n",
        "As lists of tuples (e.g., ('natural', 'language'))\n",
        "As joined strings (e.g., \"natural language\")\n",
        "3. Frequency Analysis\n",
        "Use collections.Counter to compute:\n",
        "Bigram frequency count\n",
        "Trigram frequency count\n",
        "Report most frequent combinations (if ties, show all with max count)"
      ],
      "metadata": {
        "id": "luWfYrKi7HdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "sentence = \"Natural language processing is fascinating and powerful\"\n",
        "tokens   = word_tokenize(sentence.lower())\n",
        "\n",
        "for n in range(1, 5):\n",
        "    grams = list(ngrams(tokens, n))\n",
        "    print(f\"{n}-grams:\", grams)\n",
        "\n",
        "bigram_freq  = Counter(ngrams(tokens, 2))\n",
        "trigram_freq = Counter(ngrams(tokens, 3))\n",
        "print(\"Top bigrams:\", bigram_freq.most_common())\n",
        "print(\"Top trigrams:\", trigram_freq.most_common())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zQ53v3n7K_n",
        "outputId": "db9909af-31f3-4936-c73e-93b7d072daeb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1-grams: [('natural',), ('language',), ('processing',), ('is',), ('fascinating',), ('and',), ('powerful',)]\n",
            "2-grams: [('natural', 'language'), ('language', 'processing'), ('processing', 'is'), ('is', 'fascinating'), ('fascinating', 'and'), ('and', 'powerful')]\n",
            "3-grams: [('natural', 'language', 'processing'), ('language', 'processing', 'is'), ('processing', 'is', 'fascinating'), ('is', 'fascinating', 'and'), ('fascinating', 'and', 'powerful')]\n",
            "4-grams: [('natural', 'language', 'processing', 'is'), ('language', 'processing', 'is', 'fascinating'), ('processing', 'is', 'fascinating', 'and'), ('is', 'fascinating', 'and', 'powerful')]\n",
            "Top bigrams: [(('natural', 'language'), 1), (('language', 'processing'), 1), (('processing', 'is'), 1), (('is', 'fascinating'), 1), (('fascinating', 'and'), 1), (('and', 'powerful'), 1)]\n",
            "Top trigrams: [(('natural', 'language', 'processing'), 1), (('language', 'processing', 'is'), 1), (('processing', 'is', 'fascinating'), 1), (('is', 'fascinating', 'and'), 1), (('fascinating', 'and', 'powerful'), 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìò Exercise Brief: TF-IDF Vectorization of News Headlines\n",
        "üì∞ Headlines\n",
        "‚ÄúStock market crashes amid global uncertainty‚Äù\n",
        "‚ÄúGlobal leaders discuss climate change solutions‚Äù\n",
        "üß© Tasks\n",
        "1. Pre-processing\n",
        "Convert all text to lowercase\n",
        "Tokenize the headlines (split into words)\n",
        "Remove stop words (e.g., \"the\", \"amid\", etc.)\n",
        "2. Compute Term Frequency (TF)\n",
        "Count term occurrences in each headline\n",
        "Normalize by total terms in the headline\n",
        "3. Compute Inverse Document Frequency (IDF)\n",
        "Use the formula:\n",
        "[ \\text{IDF}(t) = \\log\\left(\\frac{N}{1 + \\text{df}(t)}\\right) ]\n",
        "Where:\n",
        "N = total number of documents (2 in this case)\n",
        "df(t) = number of documents containing term t\n",
        "4. Build TF-IDF Matrix (Manual)\n",
        "Multiply each term‚Äôs TF by its IDF\n",
        "Result: 2 √ó V matrix, where V is the vocabulary size\n",
        "5. Recreate Matrix with TfidfVectorizer (Sklearn)\n",
        "Use sklearn.feature_extraction.text.TfidfVectorizer with:\n",
        "stop_words='english'\n",
        "lowercase=True\n",
        "6. Top-Weighted Terms\n",
        "For each headline:\n",
        "Extract top three terms with the highest TF-IDF weights\n",
        "Report terms and their corresponding scores\n",
        "‚úÖ Expected Output\n",
        "Manual TF-IDF Matrix\n",
        "Sklearn TF-IDF Matrix\n",
        "Top 3 TF-IDF terms per headline"
      ],
      "metadata": {
        "id": "m1YTaakT8As8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from math import log\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "docs = [\n",
        "    \"Stock market crashes amid global uncertainty\",\n",
        "    \"Global leaders discuss climate change solutions\"\n",
        "]\n",
        "\n",
        "# scikit-learn path\n",
        "vec = TfidfVectorizer(lowercase=True, stop_words='english')\n",
        "tfidf = vec.fit_transform(docs).toarray()\n",
        "print(pd.DataFrame(tfidf, columns=vec.get_feature_names_out()))\n",
        "\n",
        "# manual IDF (for pedagogy)\n",
        "tokens = [word_tokenize(d.lower()) for d in docs]\n",
        "stop = set(stopwords.words('english'))\n",
        "proc   = [[w for w in t if w.isalpha() and w not in stop] for t in tokens]\n",
        "vocab  = sorted({w for doc in proc for w in doc})\n",
        "idf    = {t: log(len(docs) / sum(t in p for p in proc)) for t in vocab}\n",
        "tfidf_manual = []\n",
        "for doc in proc:\n",
        "    length = len(doc)\n",
        "    tfidf_manual.append([doc.count(t)/length*idf[t] for t in vocab])\n",
        "print(pd.DataFrame(tfidf_manual, columns=vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmhYj4rx8FfJ",
        "outputId": "bddf5c57-e426-494f-cdb3-361d69d3ff86"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      amid   change  climate  crashes  discuss    global  leaders   market  \\\n",
            "0  0.42616  0.00000  0.00000  0.42616  0.00000  0.303216  0.00000  0.42616   \n",
            "1  0.00000  0.42616  0.42616  0.00000  0.42616  0.303216  0.42616  0.00000   \n",
            "\n",
            "   solutions    stock  uncertainty  \n",
            "0    0.00000  0.42616      0.42616  \n",
            "1    0.42616  0.00000      0.00000  \n",
            "       amid    change   climate   crashes   discuss  global   leaders  \\\n",
            "0  0.115525  0.000000  0.000000  0.115525  0.000000     0.0  0.000000   \n",
            "1  0.000000  0.115525  0.115525  0.000000  0.115525     0.0  0.115525   \n",
            "\n",
            "     market  solutions     stock  uncertainty  \n",
            "0  0.115525   0.000000  0.115525     0.115525  \n",
            "1  0.000000   0.115525  0.000000     0.000000  \n"
          ]
        }
      ]
    }
  ]
}