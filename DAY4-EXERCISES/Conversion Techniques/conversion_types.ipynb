{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrSAr_-80y40",
        "outputId": "b7b8699e-bbe7-40fe-bf3a-62b96e916063"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "# Download both old and new tokenizer data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bag of Words"
      ],
      "metadata": {
        "id": "WUmtQo4Y0_fZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk, string, pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"This phone has great battery life\",\n",
        "    \"Battery life on this phone is poor\",\n",
        "    \"I love the camera on this phone\"\n",
        "]\n",
        "\n",
        "# download once per session\n",
        "nltk.download('punkt'); nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
        "    return [w for w in word_tokenize(text) if w not in stop_words]\n",
        "\n",
        "tokens_list = [preprocess(doc) for doc in corpus]\n",
        "vocab = sorted({w for sent in tokens_list for w in sent})\n",
        "\n",
        "def bow_vector(tokens):\n",
        "    return [tokens.count(term) for term in vocab]\n",
        "\n",
        "manual_bow = [bow_vector(t) for t in tokens_list]\n",
        "manual_df  = pd.DataFrame(manual_bow, columns=vocab)\n",
        "\n",
        "cv = CountVectorizer(lowercase=True, stop_words='english')\n",
        "cv_bow = cv.fit_transform(corpus).toarray()\n",
        "cv_df  = pd.DataFrame(cv_bow, columns=cv.get_feature_names_out())\n",
        "\n",
        "print(manual_df, '\\n'); print(cv_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKb6L4Z31BRX",
        "outputId": "9adda1fc-68c9-40f6-9647-4292e7c01163"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   battery  camera  great  life  love  phone  poor\n",
            "0        1       0      1     1     0      1     0\n",
            "1        1       0      0     1     0      1     1\n",
            "2        0       1      0     0     1      1     0 \n",
            "\n",
            "   battery  camera  great  life  love  phone  poor\n",
            "0        1       0      1     1     0      1     0\n",
            "1        1       0      0     1     0      1     1\n",
            "2        0       1      0     0     1      1     0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N-grams Generation and Analysis"
      ],
      "metadata": {
        "id": "Mg418wGJ1NAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "sentence = \"Natural language processing is fascinating and powerful\"\n",
        "tokens   = word_tokenize(sentence.lower())\n",
        "\n",
        "for n in range(1, 5):\n",
        "    grams = list(ngrams(tokens, n))\n",
        "    print(f\"{n}-grams:\", grams)\n",
        "\n",
        "bigram_freq  = Counter(ngrams(tokens, 2))\n",
        "trigram_freq = Counter(ngrams(tokens, 3))\n",
        "print(\"Top bigrams:\", bigram_freq.most_common())\n",
        "print(\"Top trigrams:\", trigram_freq.most_common())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKrv_tK41nLl",
        "outputId": "1a846999-2ed2-4e58-f468-a9117350d52c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1-grams: [('natural',), ('language',), ('processing',), ('is',), ('fascinating',), ('and',), ('powerful',)]\n",
            "2-grams: [('natural', 'language'), ('language', 'processing'), ('processing', 'is'), ('is', 'fascinating'), ('fascinating', 'and'), ('and', 'powerful')]\n",
            "3-grams: [('natural', 'language', 'processing'), ('language', 'processing', 'is'), ('processing', 'is', 'fascinating'), ('is', 'fascinating', 'and'), ('fascinating', 'and', 'powerful')]\n",
            "4-grams: [('natural', 'language', 'processing', 'is'), ('language', 'processing', 'is', 'fascinating'), ('processing', 'is', 'fascinating', 'and'), ('is', 'fascinating', 'and', 'powerful')]\n",
            "Top bigrams: [(('natural', 'language'), 1), (('language', 'processing'), 1), (('processing', 'is'), 1), (('is', 'fascinating'), 1), (('fascinating', 'and'), 1), (('and', 'powerful'), 1)]\n",
            "Top trigrams: [(('natural', 'language', 'processing'), 1), (('language', 'processing', 'is'), 1), (('processing', 'is', 'fascinating'), 1), (('is', 'fascinating', 'and'), 1), (('fascinating', 'and', 'powerful'), 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF Vectorization of News Headlines"
      ],
      "metadata": {
        "id": "2WJOPyF01136"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import log\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "docs = [\n",
        "    \"Stock market crashes amid global uncertainty\",\n",
        "    \"Global leaders discuss climate change solutions\"\n",
        "]\n",
        "\n",
        "# scikit-learn path\n",
        "vec = TfidfVectorizer(lowercase=True, stop_words='english')\n",
        "tfidf = vec.fit_transform(docs).toarray()\n",
        "print(pd.DataFrame(tfidf, columns=vec.get_feature_names_out()))\n",
        "\n",
        "# manual IDF (for pedagogy)\n",
        "tokens = [word_tokenize(d.lower()) for d in docs]\n",
        "stop = set(stopwords.words('english'))\n",
        "proc   = [[w for w in t if w.isalpha() and w not in stop] for t in tokens]\n",
        "vocab  = sorted({w for doc in proc for w in doc})\n",
        "idf    = {t: log(len(docs) / sum(t in p for p in proc)) for t in vocab}\n",
        "tfidf_manual = []\n",
        "for doc in proc:\n",
        "    length = len(doc)\n",
        "    tfidf_manual.append([doc.count(t)/length*idf[t] for t in vocab])\n",
        "print(pd.DataFrame(tfidf_manual, columns=vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXibQu4r137v",
        "outputId": "aa437043-624b-4f1d-b466-f201d24a6f96"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      amid   change  climate  crashes  discuss    global  leaders   market  \\\n",
            "0  0.42616  0.00000  0.00000  0.42616  0.00000  0.303216  0.00000  0.42616   \n",
            "1  0.00000  0.42616  0.42616  0.00000  0.42616  0.303216  0.42616  0.00000   \n",
            "\n",
            "   solutions    stock  uncertainty  \n",
            "0    0.00000  0.42616      0.42616  \n",
            "1    0.42616  0.00000      0.00000  \n",
            "       amid    change   climate   crashes   discuss  global   leaders  \\\n",
            "0  0.115525  0.000000  0.000000  0.115525  0.000000     0.0  0.000000   \n",
            "1  0.000000  0.115525  0.115525  0.000000  0.115525     0.0  0.115525   \n",
            "\n",
            "     market  solutions     stock  uncertainty  \n",
            "0  0.115525   0.000000  0.115525     0.115525  \n",
            "1  0.000000   0.115525  0.000000     0.000000  \n"
          ]
        }
      ]
    }
  ]
}