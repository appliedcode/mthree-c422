{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/appliedcode/mthree-c422/blob/main/Exercises/day-10/First_exercise_chatgpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lab Exercise: Introduction to Prompt Engineering and Large Language Models (LLMs)\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Understand *what prompt engineering is* and its importance in AI\n",
    "- Learn about *Large Language Models (LLMs)* and their key capabilities\n",
    "- Explore prompt engineering as a *new programming paradigm*\n",
    "- Survey *major LLM types* from OpenAI, Anthropic, and Google\n",
    "- Discuss *real-world applications* of prompt engineering\n",
    "- Gain practical experience crafting and testing prompts with OpenAI's GPT models in Colab\n",
    "\n",
    "***\n",
    "\n",
    "## 1. What is Prompt Engineering and Why It Matters\n",
    "\n",
    "Read this definition:\n",
    "*Prompt Engineering* is the practice of designing and refining the instructions or inputs (\"prompts\") given to Large Language Models (LLMs) to elicit desired and accurate outputs. Unlike traditional programming, here you \"program\" by language interaction.\n",
    "\n",
    "**Why it matters:**\n",
    "\n",
    "- Maximizes LLM output quality without retraining\n",
    "- Enables custom task control via instructions, examples, personas\n",
    "- Reduces hallucinations and irrelevant output\n",
    "- Vital for practical AI deployments\n",
    "\n",
    "***\n",
    "\n",
    "## 2. Overview of Large Language Models (LLMs)\n",
    "\n",
    "- LLMs are pretrained AI models with billions+ parameters trained on massive text corpora.\n",
    "- They understand and can generate human-like text in multiple formats and languages.\n",
    "- Examples:\n",
    "    - **OpenAI:** GPT-3, GPT-4, ChatGPT\n",
    "    - **Anthropic:** Claude family\n",
    "    - **Google:** PaLM, Gemini\n",
    "\n",
    "*Capabilities:* text generation, summarization, translation, question answering, code writing, conversational AI.\n",
    "\n",
    "***\n",
    "\n",
    "## 3. Prompt Engineering as a New Programming Paradigm\n",
    "\n",
    "- Move from traditional code and logic to instructing LLMs with natural language prompts\n",
    "- Incorporates clear instructions, few-shot examples, role assignments (personas)\n",
    "- Iteratively refine prompt design based on responses\n",
    "- Allows rapid prototyping across diverse tasks\n",
    "\n",
    "***\n",
    "\n",
    "## 4. Types of Popular LLMs\n",
    "\n",
    "| Provider | Model Examples | Special Features |\n",
    "| :-- | :-- | :-- |\n",
    "| OpenAI | GPT-3, GPT-4, ChatGPT | Large-scale APIs, conversational AI |\n",
    "| Anthropic | Claude | Safety-focused, steerable chatbots |\n",
    "| Google (DeepMind) | PaLM, Gemini | Multi-modal, grounded reasoning |\n",
    "\n",
    "Good to know: Different models have different token limits, pricing, and availability.\n",
    "\n",
    "***\n",
    "\n",
    "## 5. Real-World Applications of Prompt Engineering\n",
    "\n",
    "- Customer support chatbots\n",
    "- Content creation (blogs, ads, scripts)\n",
    "- Code generation and debugging\n",
    "- Legal and medical document analysis\n",
    "- Personalized tutoring and coaching\n",
    "- Data extraction and summarization\n",
    "\n",
    "***\n"
   ],
   "metadata": {
    "id": "hFnkDgKDTUgk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import userdata\n",
    "import os\n",
    "\n",
    "# Set your OpenAI API key securely in Colab Secrets (once)\n",
    "# userdata.set(\"OPENAI_API_KEY\", \"your-api-key-here\")\n",
    "# Retrieve key in your notebook\n",
    "openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n",
    "if openai_api_key:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "    print(\"âœ… OpenAI API key loaded safely\")\n",
    "else:\n",
    "    print(\"âŒ OpenAI API key not found. Please set it using Colab Secrets.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b-MrRDkKZxI6",
    "outputId": "9b774612-44e6-4e4d-b19d-41a4f08c7c33",
    "ExecuteTime": {
     "end_time": "2025-08-11T08:10:04.546573Z",
     "start_time": "2025-08-11T08:10:04.504660Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcolab\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m userdata\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mos\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Set your OpenAI API key securely in Colab Secrets (once)\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# userdata.set(\"OPENAI_API_KEY\", \"your-api-key-here\")\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# Retrieve key in your notebook\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'google.colab'"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install --quiet openai -q"
   ],
   "metadata": {
    "id": "eyeKWLK4bM2Q",
    "ExecuteTime": {
     "end_time": "2025-08-11T08:09:08.600058Z",
     "start_time": "2025-08-11T08:09:08.543350Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Access is denied.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "# Create client\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])"
   ],
   "metadata": {
    "id": "lNqOYjPrbNo7",
    "ExecuteTime": {
     "end_time": "2025-08-11T08:20:15.867989Z",
     "start_time": "2025-08-11T08:20:14.804814Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Create client\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mopenai\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m OpenAI\n\u001B[1;32m----> 3\u001B[0m client \u001B[38;5;241m=\u001B[39m OpenAI(api_key\u001B[38;5;241m=\u001B[39mos\u001B[38;5;241m.\u001B[39menviron[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOPENAI_API_KEY\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "\u001B[1;31mNameError\u001B[0m: name 'os' is not defined"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "prompt = \"Explain what prompt engineering is in one paragraph.\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",  # Free-tier friendly\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful teacher.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ’¬ Model Output:\\n\")\n",
    "print(response.choices[0].message.content)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8U4WXgvtbWV-",
    "outputId": "75b2d7c6-566b-490c-81c7-5a235268137f"
   },
   "execution_count": 35,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "ðŸ’¬ Model Output:\n",
      "\n",
      "Prompt engineering is the process of crafting and refining the input prompts given to artificial intelligence models, particularly large language models, to elicit the most accurate, relevant, and contextually appropriate responses. It involves understanding the model's capabilities and limitations, experimenting with different phrasings, lengths, and structures of prompts, and strategically framing questions or tasks to guide the AI toward desired outputs. Effective prompt engineering can significantly enhance the quality of the interaction and the utility of the generated content, making it a key skill\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to Colab",
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "name": "conda-base-py",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
