{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/appliedcode/mthree-c422/blob/mthree-c422-Avantika/Exercises/day-13/Bias/Bias.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "\n",
        "# Practice Exercise Set: Understanding Bias in Algorithms\n",
        "\n",
        "This set focuses on exploring data, algorithmic, and societal bias, methods to detect and mitigate bias, and applying fairness metrics to evaluate models.\n",
        "\n",
        "***\n",
        "\n",
        "## 1. Types of Bias in Algorithms\n",
        "\n",
        "### Exercise 1.1 — Explore Types of Bias (Conceptual)\n",
        "\n",
        "- **Data Bias:** Bias arising from unrepresentative or skewed datasets (e.g., selection bias, sampling bias).\n",
        "- **Algorithmic Bias:** Bias introduced by model choices or design (e.g., feature selection bias, inductive bias).\n",
        "- **Societal Bias:** Pre-existing societal inequalities reflected in data and models (e.g., historical discrimination, stereotype bias).\n",
        "\n",
        "\n",
        "### Exercise 1.2 — Identify Bias Sources\n",
        "\n",
        "- Given a dataset of loan applications with demographic attributes, reflect on possible sources of each bias type. For example, data bias may come from under-sampling minorities, algorithm bias from model ignoring fairness, and societal bias from systemic economic disparities.\n",
        "\n",
        "*(This task is primarily conceptual and discussion-based.)*\n",
        "\n",
        "***\n",
        "\n",
        "## 2. Techniques for Detecting Bias\n",
        "\n",
        "We’ll use a synthetic loan dataset for hands-on detection.\n",
        "\n",
        "### Exercise 2.1 — Data Bias Detection\n",
        "\n",
        "- Analyze demographic group distribution and label imbalance.\n",
        "- Perform chi-square test for independence.\n",
        "- Visualize approval rates by groups."
      ],
      "metadata": {
        "id": "YGaiV8GXXwbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(0)\n",
        "n = 1000\n",
        "genders = ['Male', 'Female']\n",
        "ethnicities = ['Group_A', 'Group_B', 'Group_C']\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    'Gender': np.random.choice(genders, n, p=[0.6, 0.4]),\n",
        "    'Ethnicity': np.random.choice(ethnicities, n, p=[0.5, 0.3, 0.2])\n",
        "})\n",
        "\n",
        "# Simulated loan approvals with bias: Group_A has higher approval rate\n",
        "data['Loan_Approved'] = np.where(\n",
        "    (data['Ethnicity'] == 'Group_A'),\n",
        "    np.random.choice([1, 0], n, p=[0.7, 0.3]),\n",
        "    np.random.choice([1, 0], n, p=[0.4, 0.6])\n",
        ")\n",
        "\n",
        "# Group distribution\n",
        "print(data['Ethnicity'].value_counts(normalize=True))\n",
        "\n",
        "# Approval rates by Ethnicity\n",
        "approval_rates = data.groupby('Ethnicity')['Loan_Approved'].mean()\n",
        "print(approval_rates)\n",
        "\n",
        "# Chi-square test\n",
        "contingency_table = pd.crosstab(data['Ethnicity'], data['Loan_Approved'])\n",
        "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
        "print(f\"Chi-square test p-value: {p}\")\n",
        "\n",
        "# Bar plot\n",
        "approval_rates.plot(kind='bar', title='Loan Approval Rate by Ethnicity')\n",
        "plt.ylabel('Approval Rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "95OUF231pjcx",
        "outputId": "c43dd01d-6239-45a9-cb83-1cf630fe06ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ethnicity\n",
            "Group_A    0.481\n",
            "Group_B    0.294\n",
            "Group_C    0.225\n",
            "Name: proportion, dtype: float64\n",
            "Ethnicity\n",
            "Group_A    0.704782\n",
            "Group_B    0.421769\n",
            "Group_C    0.457778\n",
            "Name: Loan_Approved, dtype: float64\n",
            "Chi-square test p-value: 1.1871666628169174e-16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation:\n",
        "\n",
        "- Imbalanced group sizes and differing approval rates indicate potential data bias.\n",
        "- A small p-value from chi-square suggests approval depends on ethnicity.\n",
        "\n",
        "***\n",
        "\n",
        "### Exercise 2.2 — Algorithmic Bias Detection\n",
        "\n",
        "- Train a baseline classifier (e.g. logistic regression).\n",
        "- Compute accuracy, True Positive Rate (TPR), False Positive Rate (FPR) per group.\n",
        "- Check feature importance for sensitive attributes.\n",
        "\n"
      ],
      "metadata": {
        "id": "XlR-wN-sprCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Prepare data\n",
        "X = pd.get_dummies(data[['Gender', 'Ethnicity']], drop_first=True)\n",
        "y = data['Loan_Approved']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Group-wise metrics function\n",
        "def group_metrics(df, labels, preds, group_col):\n",
        "    results = {}\n",
        "    for g in df[group_col].unique():\n",
        "        idx = df[group_col]==g\n",
        "        tn, fp, fn, tp = confusion_matrix(labels[idx], preds[idx]).ravel()\n",
        "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "        acc = (tp + tn) / (tp + tn + fp + fn)\n",
        "        results[g] = {'Accuracy': acc, 'TPR': tpr, 'FPR': fpr}\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "df_test = X_test.copy()\n",
        "df_test['Gender'] = data.loc[y_test.index, 'Gender'].values\n",
        "df_test['Ethnicity'] = data.loc[y_test.index, 'Ethnicity'].values\n",
        "\n",
        "metrics_gender = group_metrics(df_test, y_test, y_pred, 'Gender')\n",
        "metrics_ethnicity = group_metrics(df_test, y_test, y_pred, 'Ethnicity')\n",
        "\n",
        "print(\"Metrics by Gender:\")\n",
        "print(metrics_gender)\n",
        "print(\"\\nMetrics by Ethnicity:\")\n",
        "print(metrics_ethnicity)\n",
        "\n",
        "# Feature importance (coefficients)\n",
        "coef = pd.Series(model.coef_[0], index=X.columns)\n",
        "print(\"\\nFeature importance:\")\n",
        "print(coef.sort_values(ascending=False))"
      ],
      "metadata": {
        "id": "pPc5BIweplhQ",
        "outputId": "6088c373-cd73-49fb-be65-400e139573a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics by Gender:\n",
            "              Male    Female\n",
            "Accuracy  0.652850  0.682243\n",
            "TPR       0.605263  0.566667\n",
            "FPR       0.278481  0.170213\n",
            "\n",
            "Metrics by Ethnicity:\n",
            "           Group_C   Group_B   Group_A\n",
            "Accuracy  0.513514  0.623656  0.774436\n",
            "TPR       0.000000  0.000000  1.000000\n",
            "FPR       0.000000  0.000000  1.000000\n",
            "\n",
            "Feature importance:\n",
            "Gender_Male         -0.202833\n",
            "Ethnicity_Group_B   -0.917951\n",
            "Ethnicity_Group_C   -0.923650\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation:\n",
        "\n",
        "- Examine if performance metrics differ substantially between groups.\n",
        "- Large differences in TPR or FPR indicate algorithmic bias.\n",
        "- Sensitive features with large coefficients may drive bias.\n",
        "\n",
        "***\n",
        "\n",
        "## 3. Techniques for Mitigating Bias\n",
        "\n",
        "### Exercise 3.1 — Pre-processing: Re-sampling to Balance Dataset"
      ],
      "metadata": {
        "id": "fYj7u_fwpuzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "# Resample minority group to balance dataset\n",
        "df_minority = data[data['Ethnicity']=='Group_C']\n",
        "df_majority = data[data['Ethnicity']!='Group_C']\n",
        "\n",
        "df_minority_upsampled = resample(df_minority, replace=True, n_samples=len(df_majority), random_state=42)\n",
        "\n",
        "df_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
        "\n",
        "# Recheck approval rates\n",
        "print(df_balanced['Ethnicity'].value_counts(normalize=True))\n",
        "print(df_balanced.groupby('Ethnicity')['Loan_Approved'].mean())"
      ],
      "metadata": {
        "id": "n1OEI3GLpwZz",
        "outputId": "36862d6a-4e33-4368-c241-90f58317d3fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ethnicity\n",
            "Group_C    0.500000\n",
            "Group_A    0.310323\n",
            "Group_B    0.189677\n",
            "Name: proportion, dtype: float64\n",
            "Ethnicity\n",
            "Group_A    0.704782\n",
            "Group_B    0.421769\n",
            "Group_C    0.441290\n",
            "Name: Loan_Approved, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3.2 — In-processing: Train with Fairness Constraint (Sample for illustration)\n",
        "\n",
        "- Implement fairness constraints if toolkits like `aif360` or other libraries are available (not shown here for simplicity).\n",
        "\n",
        "\n",
        "### Exercise 3.3 — Post-processing: Adjust Decision Thresholds per Group"
      ],
      "metadata": {
        "id": "pfyBpBaZpyf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Compute thresholds to equalize TPR or FPR (high level exercise)\n",
        "# Requires predicted probabilities and per-group ROC curves (not shown due to complexity)"
      ],
      "metadata": {
        "id": "KF9vpKazp08B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "\n",
        "## 4. Fairness Metrics and Evaluation\n",
        "\n",
        "### Exercise 4.1 — Compute Common Fairness Metrics"
      ],
      "metadata": {
        "id": "m63Y5eQdp20h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def demographic_parity(y_true, y_pred, group):\n",
        "    rates = pd.DataFrame({'y_pred': y_pred, 'group': group}).groupby('group')['y_pred'].mean()\n",
        "    return rates.max() - rates.min()\n",
        "\n",
        "def equal_opportunity(y_true, y_pred, group):\n",
        "    results = {}\n",
        "    df = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred, 'group': group})\n",
        "    for g in df['group'].unique():\n",
        "        subset = df[df['group'] == g]\n",
        "        tp = ((subset['y_pred'] == 1) & (subset['y_true'] == 1)).sum()\n",
        "        fn = ((subset['y_pred'] == 0) & (subset['y_true'] == 1)).sum()\n",
        "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        results[g] = tpr\n",
        "    tpr_vals = list(results.values())\n",
        "    return max(tpr_vals) - min(tpr_vals)\n",
        "\n",
        "dp_ethnicity = demographic_parity(y_test, y_pred, df_test['Ethnicity'])\n",
        "eo_ethnicity = equal_opportunity(y_test, y_pred, df_test['Ethnicity'])\n",
        "\n",
        "print(f\"Demographic Parity Difference (Ethnicity): {dp_ethnicity:.3f}\")\n",
        "print(f\"Equal Opportunity Difference (Ethnicity): {eo_ethnicity:.3f}\")"
      ],
      "metadata": {
        "id": "JGThWlPsp2in",
        "outputId": "b87d2125-75e3-43f7-fb1c-6bd3314b75d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Demographic Parity Difference (Ethnicity): 1.000\n",
            "Equal Opportunity Difference (Ethnicity): 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 4.2 — Compare Before/After Mitigation\n",
        "\n",
        "- Repeat model training and metric calculation on balanced data and compare fairness results.\n",
        "\n",
        "\n",
        "### Exercise 4.3 — Discussion\n",
        "\n",
        "- Reflect on how accuracy may trade off with fairness.\n",
        "- Consider how mitigations affect model complexity or interpretability.\n",
        "\n",
        "***"
      ],
      "metadata": {
        "id": "cZe4Gf2ep7LF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ff615c2"
      },
      "source": [
        "# Task\n",
        "Explain the concepts of in-processing and post-processing fairness mitigation techniques, and discuss how to compare the fairness of a model before and after applying mitigation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c16f3966"
      },
      "source": [
        "## In-processing mitigation (task 3.2)\n",
        "\n",
        "### Subtask:\n",
        "Discuss the concept of in-processing mitigation and mention the need for external libraries for practical implementation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dc1176d"
      },
      "source": [
        "**Reasoning**:\n",
        "Write a markdown cell explaining in-processing bias mitigation, including modifying the training process and the need for specialized libraries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "fb3d9bc0",
        "outputId": "0ce8022f-c977-471e-9b67-202678aaf3a7"
      },
      "source": [
        "### In-processing Mitigation\n",
        "\n",
        "In-processing mitigation techniques address bias during the model training phase. These methods modify the training process or the model's objective function to incorporate fairness constraints.\n",
        "\n",
        "Examples include:\n",
        "\n",
        "- **Adding a fairness regularization term** to the loss function to penalize unfair outcomes.\n",
        "- **Modifying the optimization algorithm** to ensure fairness across different groups.\n",
        "\n",
        "Practical implementation of sophisticated in-processing techniques often requires specialized toolkits or libraries designed for algorithmic fairness. Libraries like `aif360` provide pre-built algorithms and frameworks to integrate fairness considerations directly into the model training pipeline."
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 3) (ipython-input-2668350054.py, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2668350054.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    In-processing mitigation techniques address bias during the model training phase. These methods modify the training process or the model's objective function to incorporate fairness constraints.\u001b[0m\n\u001b[0m                                                                                                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a16f4761"
      },
      "source": [
        "**Reasoning**:\n",
        "Correct the syntax error and regenerate the markdown cell explaining in-processing bias mitigation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5xUsIQDwEpW"
      },
      "source": [
        "# ### In-processing Mitigation\n",
        "#\n",
        "# In-processing mitigation techniques address bias during the model training phase. These methods modify the training process or the model's objective function to incorporate fairness constraints.\n",
        "#\n",
        "# Examples include:\n",
        "#\n",
        "# - **Adding a fairness regularization term** to the loss function to penalize unfair outcomes.\n",
        "# - **Modifying the optimization algorithm** to ensure fairness across different groups.\n",
        "#\n",
        "# Practical implementation of sophisticated in-processing techniques often requires specialized toolkits or libraries designed for algorithmic fairness. Libraries like `aif360` provide pre-built algorithms and frameworks to integrate fairness considerations directly into the model training pipeline."
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8daeef81"
      },
      "source": [
        "## Post-processing mitigation (task 3.3)\n",
        "\n",
        "### Subtask:\n",
        "Discuss the concept of post-processing mitigation and mention the need for predicted probabilities and per-group ROC curves for practical implementation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "146dc671"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a markdown cell to explain post-processing bias mitigation, including adjusting decision thresholds and the requirements for implementation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c08f1708"
      },
      "source": [
        "# ### Post-processing Mitigation\n",
        "#\n",
        "# Post-processing mitigation techniques address bias *after* a model has been trained and its predictions have been made. These methods adjust the model's outputs or decisions to improve fairness without retraining the model.\n",
        "#\n",
        "# A common example is **adjusting decision thresholds per group**. This involves setting different thresholds for different demographic groups to equalize fairness metrics like True Positive Rate (TPR) or False Positive Rate (FPR) across these groups.\n",
        "#\n",
        "# Implementing techniques like equalizing TPR or FPR through threshold adjustment practically requires access to the model's predicted probabilities for each instance and the ability to analyze and compare Receiver Operating Characteristic (ROC) curves for different groups to determine the appropriate thresholds."
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "471e4b9b"
      },
      "source": [
        "## Compare before/after mitigation (task 4.2)\n",
        "\n",
        "### Subtask:\n",
        "Train the model on the balanced dataset created in Task 3.1, calculate fairness metrics, and compare them to the metrics from the original model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7299c8fe"
      },
      "source": [
        "**Reasoning**:\n",
        "Prepare the balanced data for model training, split it into training and testing sets, train a logistic regression model on the balanced training data, make predictions on the balanced test data, prepare the test data for fairness metric calculation, and then calculate and print the fairness metrics for the balanced model, comparing them to the original model's metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "a48e6eff",
        "outputId": "90b8284a-c8be-404e-fd68-4e6d16753c3c"
      },
      "source": [
        "# Prepare the balanced dataset for model training\n",
        "X_balanced = pd.get_dummies(df_balanced[['Gender', 'Ethnicity']], drop_first=True)\n",
        "y_balanced = df_balanced['Loan_Approved']\n",
        "\n",
        "# Split the balanced data\n",
        "X_train_balanced, X_test_balanced, y_train_balanced, y_test_balanced = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Logistic Regression model on the balanced data\n",
        "model_balanced = LogisticRegression(solver='liblinear')\n",
        "model_balanced.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "# Make predictions on the balanced test set\n",
        "y_pred_balanced = model_balanced.predict(X_test_balanced)\n",
        "\n",
        "# Create a DataFrame for fairness metric calculation\n",
        "df_test_balanced = X_test_balanced.copy()\n",
        "# Ensure indices align correctly when adding back original columns\n",
        "df_test_balanced['Gender'] = df_balanced.loc[y_test_balanced.index, 'Gender'].values\n",
        "df_test_balanced['Ethnicity'] = df_balanced.loc[y_test_balanced.index, 'Ethnicity'].values\n",
        "\n",
        "\n",
        "# Calculate fairness metrics for the balanced model\n",
        "dp_ethnicity_balanced = demographic_parity(y_test_balanced, y_pred_balanced, df_test_balanced['Ethnicity'])\n",
        "eo_ethnicity_balanced = equal_opportunity(y_test_balanced, y_pred_balanced, df_test_balanced['Ethnicity'])\n",
        "\n",
        "# Print and compare fairness metrics\n",
        "print(\"Fairness Metrics (Balanced Model):\")\n",
        "print(f\"Demographic Parity Difference (Ethnicity): {dp_ethnicity_balanced:.3f}\")\n",
        "print(f\"Equal Opportunity Difference (Ethnicity): {eo_ethnicity_balanced:.3f}\")\n",
        "\n",
        "print(\"\\nFairness Metrics (Original Model):\")\n",
        "print(f\"Demographic Parity Difference (Ethnicity): {dp_ethnicity:.3f}\")\n",
        "print(f\"Equal Opportunity Difference (Ethnicity): {eo_ethnicity:.3f}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_balanced' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2860553124.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Prepare the balanced dataset for model training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_balanced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_balanced\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Gender'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Ethnicity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_balanced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_balanced\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Loan_Approved'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Split the balanced data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_balanced' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70f79361"
      },
      "source": [
        "**Reasoning**:\n",
        "The error indicates that `df_balanced` is not defined. This is likely because the previous cell that created `df_balanced` was not executed or the variable was not persisted correctly. I need to recreate `df_balanced` from the original `data` DataFrame using the resampling logic before proceeding with model training and evaluation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "o_oCq6-rwQLJ",
        "outputId": "0775807e-6e6f-421b-ad3f-4704cb1a0064"
      },
      "source": [
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "\n",
        "# Recreate the balanced dataset\n",
        "df_minority = data[data['Ethnicity']=='Group_C']\n",
        "df_majority = data[data['Ethnicity']!='Group_C']\n",
        "\n",
        "df_minority_upsampled = resample(df_minority, replace=True, n_samples=len(df_majority), random_state=42)\n",
        "\n",
        "df_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
        "\n",
        "# Prepare the balanced dataset for model training\n",
        "X_balanced = pd.get_dummies(df_balanced[['Gender', 'Ethnicity']], drop_first=True)\n",
        "y_balanced = df_balanced['Loan_Approved']\n",
        "\n",
        "# Split the balanced data\n",
        "X_train_balanced, X_test_balanced, y_train_balanced, y_test_balanced = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Logistic Regression model on the balanced data\n",
        "model_balanced = LogisticRegression(solver='liblinear')\n",
        "model_balanced.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "# Make predictions on the balanced test set\n",
        "y_pred_balanced = model_balanced.predict(X_test_balanced)\n",
        "\n",
        "# Create a DataFrame for fairness metric calculation\n",
        "df_test_balanced = X_test_balanced.copy()\n",
        "# Ensure indices align correctly when adding back original columns\n",
        "df_test_balanced['Gender'] = df_balanced.loc[y_test_balanced.index, 'Gender'].values\n",
        "df_test_balanced['Ethnicity'] = df_balanced.loc[y_test_balanced.index, 'Ethnicity'].values\n",
        "\n",
        "\n",
        "# Calculate fairness metrics for the balanced model\n",
        "dp_ethnicity_balanced = demographic_parity(y_test_balanced, y_pred_balanced, df_test_balanced['Ethnicity'])\n",
        "eo_ethnicity_balanced = equal_opportunity(y_test_balanced, y_pred_balanced, df_test_balanced['Ethnicity'])\n",
        "\n",
        "# Print and compare fairness metrics\n",
        "print(\"Fairness Metrics (Balanced Model):\")\n",
        "print(f\"Demographic Parity Difference (Ethnicity): {dp_ethnicity_balanced:.3f}\")\n",
        "print(f\"Equal Opportunity Difference (Ethnicity): {eo_ethnicity_balanced:.3f}\")\n",
        "\n",
        "print(\"\\nFairness Metrics (Original Model):\")\n",
        "print(f\"Demographic Parity Difference (Ethnicity): {dp_ethnicity:.3f}\")\n",
        "print(f\"Equal Opportunity Difference (Ethnicity): {eo_ethnicity:.3f}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Length of values (1340) does not match length of index (465)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3693499176.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mdf_test_balanced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test_balanced\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Ensure indices align correctly when adding back original columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mdf_test_balanced\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Gender'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_balanced\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_test_balanced\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Gender'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mdf_test_balanced\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Ethnicity'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_balanced\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_test_balanced\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Ethnicity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4309\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4310\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4311\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4522\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4523\u001b[0m         \"\"\"\n\u001b[0;32m-> 4524\u001b[0;31m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4526\u001b[0m         if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5266\u001b[0;31m             \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5267\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5268\u001b[0m         if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    571\u001b[0m     \"\"\"\n\u001b[1;32m    572\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    574\u001b[0m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Length of values (1340) does not match length of index (465)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f20c4dc0"
      },
      "source": [
        "**Reasoning**:\n",
        "The error \"ValueError: Length of values (1340) does not match length of index (465)\" occurs when trying to assign columns to `df_test_balanced`. This is because `df_balanced.loc[y_test_balanced.index, 'Gender'].values` is selecting rows from the entire `df_balanced` DataFrame based on the indices of the test set, which results in more rows than in the test set due to upsampling. To fix this, I need to create `df_test_balanced` by merging the original `data` DataFrame with `X_test_balanced` on their index before adding the 'Gender' and 'Ethnicity' columns. This ensures that the sensitive attribute columns are correctly aligned with the test set features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "2_Uago0AwTSv",
        "outputId": "033b35c0-efb2-4e30-b753-a0aa2b26a550"
      },
      "source": [
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "\n",
        "# Recreate the balanced dataset\n",
        "df_minority = data[data['Ethnicity']=='Group_C']\n",
        "df_majority = data[data['Ethnicity']!='Group_C']\n",
        "\n",
        "df_minority_upsampled = resample(df_minority, replace=True, n_samples=len(df_majority), random_state=42)\n",
        "\n",
        "df_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
        "\n",
        "# Prepare the balanced dataset for model training\n",
        "X_balanced = pd.get_dummies(df_balanced[['Gender', 'Ethnicity']], drop_first=True)\n",
        "y_balanced = df_balanced['Loan_Approved']\n",
        "\n",
        "# Split the balanced data\n",
        "X_train_balanced, X_test_balanced, y_train_balanced, y_test_balanced = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Logistic Regression model on the balanced data\n",
        "model_balanced = LogisticRegression(solver='liblinear')\n",
        "model_balanced.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "# Make predictions on the balanced test set\n",
        "y_pred_balanced = model_balanced.predict(X_test_balanced)\n",
        "\n",
        "# Create a DataFrame for fairness metric calculation by merging with original data on index\n",
        "df_test_balanced = pd.merge(X_test_balanced, data[['Gender', 'Ethnicity']], left_index=True, right_index=True, how='left')\n",
        "\n",
        "# Calculate fairness metrics for the balanced model\n",
        "dp_ethnicity_balanced = demographic_parity(y_test_balanced, y_pred_balanced, df_test_balanced['Ethnicity'])\n",
        "eo_ethnicity_balanced = equal_opportunity(y_test_balanced, y_pred_balanced, df_test_balanced['Ethnicity'])\n",
        "\n",
        "# Print and compare fairness metrics\n",
        "print(\"Fairness Metrics (Balanced Model):\")\n",
        "print(f\"Demographic Parity Difference (Ethnicity): {dp_ethnicity_balanced:.3f}\")\n",
        "print(f\"Equal Opportunity Difference (Ethnicity): {eo_ethnicity_balanced:.3f}\")\n",
        "\n",
        "print(\"\\nFairness Metrics (Original Model):\")\n",
        "print(f\"Demographic Parity Difference (Ethnicity): {dp_ethnicity:.3f}\")\n",
        "print(f\"Equal Opportunity Difference (Ethnicity): {eo_ethnicity:.3f}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'demographic_parity' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1449941389.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Calculate fairness metrics for the balanced model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mdp_ethnicity_balanced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdemographic_parity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_balanced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_balanced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test_balanced\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Ethnicity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0meo_ethnicity_balanced\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mequal_opportunity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_balanced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_balanced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test_balanced\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Ethnicity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'demographic_parity' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b0ba9b6"
      },
      "source": [
        "**Reasoning**:\n",
        "The error \"NameError: name 'demographic_parity' is not defined\" indicates that the functions `demographic_parity` and `equal_opportunity` are not available in the current execution environment. I need to include the definitions of these functions in the current code block before calling them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "v981yq1BwWvS",
        "outputId": "2ac292c4-0eb2-4b60-f872-0ff404f784ee"
      },
      "source": [
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "\n",
        "# Define fairness metric functions (re-included as they were not available)\n",
        "def demographic_parity(y_true, y_pred, group):\n",
        "    rates = pd.DataFrame({'y_pred': y_pred, 'group': group}).groupby('group')['y_pred'].mean()\n",
        "    return rates.max() - rates.min()\n",
        "\n",
        "def equal_opportunity(y_true, y_pred, group):\n",
        "    results = {}\n",
        "    df = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred, 'group': group})\n",
        "    for g in df['group'].unique():\n",
        "        subset = df[df['group'] == g]\n",
        "        tp = ((subset['y_pred'] == 1) & (subset['y_true'] == 1)).sum()\n",
        "        fn = ((subset['y_pred'] == 0) & (subset['y_true'] == 1)).sum()\n",
        "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        results[g] = tpr\n",
        "    tpr_vals = list(results.values())\n",
        "    return max(tpr_vals) - min(tpr_vals)\n",
        "\n",
        "# Recreate the balanced dataset\n",
        "df_minority = data[data['Ethnicity']=='Group_C']\n",
        "df_majority = data[data['Ethnicity']!='Group_C']\n",
        "\n",
        "df_minority_upsampled = resample(df_minority, replace=True, n_samples=len(df_majority), random_state=42)\n",
        "\n",
        "df_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
        "\n",
        "# Prepare the balanced dataset for model training\n",
        "X_balanced = pd.get_dummies(df_balanced[['Gender', 'Ethnicity']], drop_first=True)\n",
        "y_balanced = df_balanced['Loan_Approved']\n",
        "\n",
        "# Split the balanced data\n",
        "X_train_balanced, X_test_balanced, y_train_balanced, y_test_balanced = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Logistic Regression model on the balanced data\n",
        "model_balanced = LogisticRegression(solver='liblinear')\n",
        "model_balanced.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "# Make predictions on the balanced test set\n",
        "y_pred_balanced = model_balanced.predict(X_test_balanced)\n",
        "\n",
        "# Create a DataFrame for fairness metric calculation by merging with original data on index\n",
        "df_test_balanced = pd.merge(X_test_balanced, data[['Gender', 'Ethnicity']], left_index=True, right_index=True, how='left')\n",
        "\n",
        "# Calculate fairness metrics for the balanced model\n",
        "dp_ethnicity_balanced = demographic_parity(y_test_balanced, y_pred_balanced, df_test_balanced['Ethnicity'])\n",
        "eo_ethnicity_balanced = equal_opportunity(y_test_balanced, y_pred_balanced, df_test_balanced['Ethnicity'])\n",
        "\n",
        "# Print and compare fairness metrics\n",
        "print(\"Fairness Metrics (Balanced Model):\")\n",
        "print(f\"Demographic Parity Difference (Ethnicity): {dp_ethnicity_balanced:.3f}\")\n",
        "print(f\"Equal Opportunity Difference (Ethnicity): {eo_ethnicity_balanced:.3f}\")\n",
        "\n",
        "print(\"\\nFairness Metrics (Original Model):\")\n",
        "print(f\"Demographic Parity Difference (Ethnicity): {dp_ethnicity:.3f}\")\n",
        "print(f\"Equal Opportunity Difference (Ethnicity): {eo_ethnicity:.3f}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fairness Metrics (Balanced Model):\n",
            "Demographic Parity Difference (Ethnicity): 1.000\n",
            "Equal Opportunity Difference (Ethnicity): 1.000\n",
            "\n",
            "Fairness Metrics (Original Model):\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dp_ethnicity' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3605816754.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nFairness Metrics (Original Model):\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Demographic Parity Difference (Ethnicity): {dp_ethnicity:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Equal Opportunity Difference (Ethnicity): {eo_ethnicity:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dp_ethnicity' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a990adf"
      },
      "source": [
        "**Reasoning**:\n",
        "The error \"NameError: name 'dp_ethnicity' is not defined\" indicates that the variables `dp_ethnicity` and `eo_ethnicity` from the original model evaluation are not available in the current execution environment. I need to re-calculate the fairness metrics for the original model within this code block so they are available for comparison.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4vz_EevwkYO",
        "outputId": "166243b6-68c5-44f5-ebab-22061dce43d1"
      },
      "source": [
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "\n",
        "# Define fairness metric functions (re-included as they were not available)\n",
        "def demographic_parity(y_true, y_pred, group):\n",
        "    rates = pd.DataFrame({'y_pred': y_pred, 'group': group}).groupby('group')['y_pred'].mean()\n",
        "    return rates.max() - rates.min()\n",
        "\n",
        "def equal_opportunity(y_true, y_pred, group):\n",
        "    results = {}\n",
        "    df = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred, 'group': group})\n",
        "    for g in df['group'].unique():\n",
        "        subset = df[df['group'] == g]\n",
        "        tp = ((subset['y_pred'] == 1) & (subset['y_true'] == 1)).sum()\n",
        "        fn = ((subset['y_pred'] == 0) & (subset['y_true'] == 1)).sum()\n",
        "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        results[g] = tpr\n",
        "    tpr_vals = list(results.values())\n",
        "    return max(tpr_vals) - min(tpr_vals)\n",
        "\n",
        "# Recreate the balanced dataset\n",
        "df_minority = data[data['Ethnicity']=='Group_C']\n",
        "df_majority = data[data['Ethnicity']!='Group_C']\n",
        "\n",
        "df_minority_upsampled = resample(df_minority, replace=True, n_samples=len(df_majority), random_state=42)\n",
        "\n",
        "df_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
        "\n",
        "# Prepare the balanced dataset for model training\n",
        "X_balanced = pd.get_dummies(df_balanced[['Gender', 'Ethnicity']], drop_first=True)\n",
        "y_balanced = df_balanced['Loan_Approved']\n",
        "\n",
        "# Split the balanced data\n",
        "X_train_balanced, X_test_balanced, y_train_balanced, y_test_balanced = train_test_split(X_balanced, y_balanced, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Logistic Regression model on the balanced data\n",
        "model_balanced = LogisticRegression(solver='liblinear')\n",
        "model_balanced.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "# Make predictions on the balanced test set\n",
        "y_pred_balanced = model_balanced.predict(X_test_balanced)\n",
        "\n",
        "# Create a DataFrame for fairness metric calculation by merging with original data on index\n",
        "df_test_balanced = pd.merge(X_test_balanced, data[['Gender', 'Ethnicity']], left_index=True, right_index=True, how='left')\n",
        "\n",
        "# Calculate fairness metrics for the balanced model\n",
        "dp_ethnicity_balanced = demographic_parity(y_test_balanced, y_pred_balanced, df_test_balanced['Ethnicity'])\n",
        "eo_ethnicity_balanced = equal_opportunity(y_test_balanced, y_pred_balanced, df_test_balanced['Ethnicity'])\n",
        "\n",
        "# Re-calculate fairness metrics for the original model (as they were not available)\n",
        "# Prepare data for original model metrics calculation\n",
        "X = pd.get_dummies(data[['Gender', 'Ethnicity']], drop_first=True)\n",
        "y = data['Loan_Approved']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "df_test = X_test.copy()\n",
        "df_test['Gender'] = data.loc[y_test.index, 'Gender'].values\n",
        "df_test['Ethnicity'] = data.loc[y_test.index, 'Ethnicity'].values\n",
        "\n",
        "dp_ethnicity = demographic_parity(y_test, y_pred, df_test['Ethnicity'])\n",
        "eo_ethnicity = equal_opportunity(y_test, y_pred, df_test['Ethnicity'])\n",
        "\n",
        "\n",
        "# Print and compare fairness metrics\n",
        "print(\"Fairness Metrics (Balanced Model):\")\n",
        "print(f\"Demographic Parity Difference (Ethnicity): {dp_ethnicity_balanced:.3f}\")\n",
        "print(f\"Equal Opportunity Difference (Ethnicity): {eo_ethnicity_balanced:.3f}\")\n",
        "\n",
        "print(\"\\nFairness Metrics (Original Model):\")\n",
        "print(f\"Demographic Parity Difference (Ethnicity): {dp_ethnicity:.3f}\")\n",
        "print(f\"Equal Opportunity Difference (Ethnicity): {eo_ethnicity:.3f}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fairness Metrics (Balanced Model):\n",
            "Demographic Parity Difference (Ethnicity): 1.000\n",
            "Equal Opportunity Difference (Ethnicity): 1.000\n",
            "\n",
            "Fairness Metrics (Original Model):\n",
            "Demographic Parity Difference (Ethnicity): 1.000\n",
            "Equal Opportunity Difference (Ethnicity): 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da913d3c"
      },
      "source": [
        "## Discussion (task 4.3)\n",
        "\n",
        "### Subtask:\n",
        "Reflect on the trade-offs between accuracy and fairness, and the impact of mitigation techniques on model complexity and interpretability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4063b06"
      },
      "source": [
        "**Reasoning**:\n",
        "Write a markdown cell to discuss the trade-offs between accuracy and fairness and the impact of the mitigation technique on model complexity and interpretability, based on the previous steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50c7e040"
      },
      "source": [
        "# ### Discussion: Accuracy, Fairness, and Mitigation Impact\n",
        "#\n",
        "# In this exercise, we observed how a bias mitigation technique (upsampling the minority group 'Group_C' in the training data) was applied to address data imbalance and improve fairness with respect to 'Ethnicity'.\n",
        "#\n",
        "# While the explicit accuracy metrics (like overall accuracy) for the original and balanced models were not directly compared in the previous steps, it's important to understand the general trade-off that can exist between model accuracy and fairness. Often, optimizing solely for overall accuracy might lead to disparate performance across different sensitive groups, thus reducing fairness. Conversely, applying fairness constraints or mitigation techniques can sometimes lead to a slight decrease in overall accuracy in exchange for a more equitable distribution of errors or outcomes across groups. In our case, the upsampling aimed to give the minority group more representation during training, which *could* influence the model's ability to generalize equally well across all groups.\n",
        "#\n",
        "# The fairness metrics (Demographic Parity and Equal Opportunity differences for 'Ethnicity') for both the original and balanced models were both 1.000. This indicates that, despite the resampling, the significant difference in approval rates between the groups persisted in the model's predictions on the test set as measured by these metrics. This highlights that simple data balancing techniques may not always be sufficient to fully mitigate algorithmic bias, especially when the underlying patterns of bias are strong.\n",
        "#\n",
        "# Regarding the impact of the mitigation technique on model complexity and interpretability:\n",
        "#\n",
        "# - **Complexity:** The upsampling technique itself did not fundamentally change the complexity of the Logistic Regression model. Logistic Regression remains a relatively simple and interpretable model. The complexity was increased in the data preparation step (resampling), leading to a larger training dataset.\n",
        "# - **Interpretability:** The interpretability of the Logistic Regression model trained on the balanced data is largely the same as the original model. We can still examine the coefficients to understand the influence of different features on the predictions. The upsampling process doesn't obscure the relationship between features and the outcome in the trained model itself, although interpreting the coefficients now reflects the model's learning from the artificially balanced data distribution.\n",
        "#\n",
        "# This exercise demonstrates that mitigating bias is a complex task, and the effectiveness and trade-offs are highly dependent on the specific dataset, the type of bias present, the chosen mitigation technique, and the fairness metrics used for evaluation."
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3721fed"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **In-processing mitigation** involves modifying the model training process or objective function to incorporate fairness constraints. Practical implementation often requires specialized libraries like `aif360`.\n",
        "*   **Post-processing mitigation** involves adjusting model outputs or decisions *after* training to improve fairness without retraining. A common technique is adjusting decision thresholds per group, which requires predicted probabilities and analysis of per-group ROC curves.\n",
        "*   Training a Logistic Regression model on a dataset balanced by upsampling the minority group did not reduce the fairness disparities for 'Ethnicity' as measured by Demographic Parity and Equal Opportunity metrics, which remained 1.000 for both the original and balanced models on the test set.\n",
        "*   The upsampling mitigation technique increased the complexity of the data preparation step (larger training dataset) but did not fundamentally change the complexity or interpretability of the Logistic Regression model itself.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Simple data balancing techniques like upsampling may not be sufficient to mitigate strong underlying biases in the data or model. More advanced in-processing or post-processing techniques should be explored.\n",
        "*   Evaluating the trade-off between accuracy and fairness requires explicitly calculating and comparing accuracy metrics alongside fairness metrics before and after applying mitigation techniques.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}