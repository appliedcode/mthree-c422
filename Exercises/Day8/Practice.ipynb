{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/appliedcode/mthree-c422/blob/main/Exercises/day-8/NLP-Concepts-Token/Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","source":["## Exploring Core NLP Concepts\n","Welcome to this hands-on NLP Colab lab! You will work through key tasks—tokenization, POS tagging, stemming, stop-word filtering, vocabulary matching, lemmatization, dependency parsing, NER, and intent classification—using Python libraries. Follow the instructions and complete the exercises."],"metadata":{"id":"sDeW9pllWu7A"}},{"cell_type":"code","source":["# Install required packages\n","!pip install --upgrade pip setuptools wheel -q\n","!pip install --quiet nltk spacy textblob sklearn\n","\n","# Download NLTK data and spaCy model\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","nltk.download('punkt_tab')\n","nltk.download('omw-1.4')\n","nltk.download('averaged_perceptron_tagger_eng')\n","\n","!python -m spacy download en_core_web_sm -q\n"],"metadata":{"id":"yMULvewKWrzm","colab":{"base_uri":"https://localhost:8080/"},"outputId":"94c1722f-9c0a-487e-87c0-ba315766c4aa","executionInfo":{"status":"ok","timestamp":1754470314075,"user_tz":-330,"elapsed":16846,"user":{"displayName":"Rania","userId":"13750069140470329881"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n","\n","\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n","\u001b[1;36mhint\u001b[0m: See above for details.\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n","[nltk_data]       date!\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}]},{"cell_type":"code","source":["!pip install jedi\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xhcb0nFoiiCN","executionInfo":{"status":"ok","timestamp":1754469650623,"user_tz":-330,"elapsed":6450,"user":{"displayName":"Rania","userId":"13750069140470329881"}},"outputId":"f631de91-1e3b-43e0-9dba-4d57d21ed0e4"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting jedi\n","  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi) (0.8.4)\n","Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: jedi\n","Successfully installed jedi-0.19.2\n"]}]},{"cell_type":"code","source":["!pip install textblob\n","!pip install scikit-learn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Tgn8RH0ixnK","executionInfo":{"status":"ok","timestamp":1754469714919,"user_tz":-330,"elapsed":8430,"user":{"displayName":"Rania","userId":"13750069140470329881"}},"outputId":"72b9d039-c674-436c-a0d4-501cb7f4bc61"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n","Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (8.2.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.5.1)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (4.67.1)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"]}]},{"cell_type":"code","source":["# 1. Tokenization\n","# Goal: Split text into tokens (words and punctuation).\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","\n","text = \"Natural Language Processing enables machines to understand human language.\"\n","print(\"Sentences:\", sent_tokenize(text))\n","print(\"Tokens:\", word_tokenize(text))"],"metadata":{"id":"vNgJ7l_9Wzfo","colab":{"base_uri":"https://localhost:8080/"},"outputId":"893c3082-6035-49a8-bc01-165ce6bcab41","executionInfo":{"status":"ok","timestamp":1754470444222,"user_tz":-330,"elapsed":33,"user":{"displayName":"Rania","userId":"13750069140470329881"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentences: ['Natural Language Processing enables machines to understand human language.']\n","Tokens: ['Natural', 'Language', 'Processing', 'enables', 'machines', 'to', 'understand', 'human', 'language', '.']\n"]}]},{"cell_type":"code","source":["# Exercise 1.1: Tokenize the following paragraph into words and sentences:\n","\n","paragraph = \"Machine learning models power many NLP tasks. They learn patterns from data!\"\n","print(\"Sentences:\", sent_tokenize(paragraph))\n","print(\"Tokens:\", word_tokenize(paragraph))\n"],"metadata":{"id":"Iwanu3kzW3KU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754470445756,"user_tz":-330,"elapsed":19,"user":{"displayName":"Rania","userId":"13750069140470329881"}},"outputId":"79a44832-5bb6-4e1c-9293-878a5febbe58"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentences: ['Machine learning models power many NLP tasks.', 'They learn patterns from data!']\n","Tokens: ['Machine', 'learning', 'models', 'power', 'many', 'NLP', 'tasks', '.', 'They', 'learn', 'patterns', 'from', 'data', '!']\n"]}]},{"cell_type":"code","source":["# 2. Part-of-Speech Tagging\n","# Goal: Assign grammatical tags to each token.\n","import nltk\n","tokens = word_tokenize(text)\n","pos_tags = nltk.pos_tag(tokens)\n","print(pos_tags)\n"],"metadata":{"id":"VaJMjJLOXCV9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f1d4ca6b-decf-43e1-b77a-31cf6968f879","executionInfo":{"status":"ok","timestamp":1754470450422,"user_tz":-330,"elapsed":166,"user":{"displayName":"Rania","userId":"13750069140470329881"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('enables', 'VBZ'), ('machines', 'NNS'), ('to', 'TO'), ('understand', 'VB'), ('human', 'JJ'), ('language', 'NN'), ('.', '.')]\n"]}]},{"cell_type":"code","source":["# Exercise 2.1: Tag POS for tokens from your Exercise 1.1.\n","tokens_1_1 = word_tokenize(paragraph)\n","pos_tags_1_1 = nltk.pos_tag(tokens_1_1)\n","print(pos_tags_1_1)"],"metadata":{"id":"rjdfixgHXLAe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754470467897,"user_tz":-330,"elapsed":41,"user":{"displayName":"Rania","userId":"13750069140470329881"}},"outputId":"0282635f-a6a5-40c6-d007-6cab4b15b8cd"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[('Machine', 'NN'), ('learning', 'NN'), ('models', 'NNS'), ('power', 'NN'), ('many', 'JJ'), ('NLP', 'NNP'), ('tasks', 'NNS'), ('.', '.'), ('They', 'PRP'), ('learn', 'VBP'), ('patterns', 'NNS'), ('from', 'IN'), ('data', 'NN'), ('!', '.')]\n"]}]},{"cell_type":"code","source":["# 3. Stemming\n","# Goal: Reduce words to their root forms (may be non-dictionary).\n","from nltk.stem import PorterStemmer\n","stemmer = PorterStemmer()\n","words = [\"running\", \"runs\", \"ran\", \"easily\", \"fairly\"]\n","print({w: stemmer.stem(w) for w in words})\n"],"metadata":{"id":"8Q_ASJ2qXMJ6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"aec7c026-db9c-4418-9f45-fc9f66e4be76","executionInfo":{"status":"ok","timestamp":1754470469738,"user_tz":-330,"elapsed":15,"user":{"displayName":"Rania","userId":"13750069140470329881"}}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["{'running': 'run', 'runs': 'run', 'ran': 'ran', 'easily': 'easili', 'fairly': 'fairli'}\n"]}]},{"cell_type":"code","source":["# Exercise 3.1: Stem the tokens from your Exercise 1.1.\n","stemmed = {word: stemmer.stem(word) for word in tokens_1_1 if word.isalpha()}\n","print(stemmed)"],"metadata":{"id":"KpJNVgRhXTjF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754470482220,"user_tz":-330,"elapsed":43,"user":{"displayName":"Rania","userId":"13750069140470329881"}},"outputId":"767a56ff-4a45-478c-d5f6-25a7e640a638"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["{'Machine': 'machin', 'learning': 'learn', 'models': 'model', 'power': 'power', 'many': 'mani', 'NLP': 'nlp', 'tasks': 'task', 'They': 'they', 'learn': 'learn', 'patterns': 'pattern', 'from': 'from', 'data': 'data'}\n"]}]},{"cell_type":"code","source":["# 4. Stop-Word Filtering\n","# Goal: Remove common, low-value words.\n","from nltk.corpus import stopwords\n","stop_words = set(stopwords.words('english'))\n","tokens = word_tokenize(text.lower())\n","filtered = [w for w in tokens if w.isalpha() and w not in stop_words]\n","print(filtered)"],"metadata":{"id":"kBo5Lo3hXVPk","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6bf8f15b-05c4-496b-cd0b-75726960f9c7","executionInfo":{"status":"ok","timestamp":1754470490110,"user_tz":-330,"elapsed":17,"user":{"displayName":"Rania","userId":"13750069140470329881"}}},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["['natural', 'language', 'processing', 'enables', 'machines', 'understand', 'human', 'language']\n"]}]},{"cell_type":"code","source":["# Exercise 4.1: Filter stop words from your Exercise 1.1 tokens.\n","tokens_lower = [w.lower() for w in tokens_1_1]\n","filtered = [w for w in tokens_lower if w.isalpha() and w not in stop_words]\n","print(filtered)"],"metadata":{"id":"_CVnLYVWXakJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754470495700,"user_tz":-330,"elapsed":12,"user":{"displayName":"Rania","userId":"13750069140470329881"}},"outputId":"7f5f37f8-718d-432e-9761-5a12b9d49be6"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["['machine', 'learning', 'models', 'power', 'many', 'nlp', 'tasks', 'learn', 'patterns', 'data']\n"]}]},{"cell_type":"code","source":["# 5. Vocabulary Matching\n","# Goal: Check tokens against a predefined vocabulary.\n","\n","vocab = {\"natural\", \"language\", \"machine\", \"data\", \"processing\"}\n","tokens = [w.lower() for w in word_tokenize(text)]\n","in_vocab = [w for w in tokens if w.isalpha() and w in vocab]\n","print(\"In-vocab tokens:\", in_vocab)\n","print(\"OOV tokens:\", [w for w in tokens if w.isalpha() and w not in vocab])\n"],"metadata":{"id":"IYEs9fy5XcVy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"941d7cab-9013-4c82-a320-7113d8b01d70","executionInfo":{"status":"ok","timestamp":1754470508299,"user_tz":-330,"elapsed":19,"user":{"displayName":"Rania","userId":"13750069140470329881"}}},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["In-vocab tokens: ['natural', 'language', 'processing', 'language']\n","OOV tokens: ['enables', 'machines', 'to', 'understand', 'human']\n"]}]},{"cell_type":"code","source":["# Exercise 5.1: Define your own small vocabulary and classify tokens from Exercise 1.1 into in-vocab vs. out-of-vocab.\n","vocab_custom = {'machine', 'learning', 'data', 'nlp', 'patterns'}\n","in_vocab = [w for w in tokens_lower if w.isalpha() and w in vocab_custom]\n","oov = [w for w in tokens_lower if w.isalpha() and w not in vocab_custom]\n","print(\"In-vocab:\", in_vocab)\n","print(\"OOV:\", oov)"],"metadata":{"id":"YsKmsc4AXgXa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754470519219,"user_tz":-330,"elapsed":9,"user":{"displayName":"Rania","userId":"13750069140470329881"}},"outputId":"c3064738-49fb-4929-ff2c-c4254f4f2d74"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["In-vocab: ['machine', 'learning', 'nlp', 'patterns', 'data']\n","OOV: ['models', 'power', 'many', 'tasks', 'they', 'learn', 'from']\n"]}]},{"cell_type":"code","source":["# 6. Lemmatization\n","# Goal: Convert words to their dictionary form.\n","from nltk.stem import WordNetLemmatizer\n","lemmatizer = WordNetLemmatizer()\n","words = [\"running\", \"better\", \"wolves\"]\n","print({w: lemmatizer.lemmatize(w) for w in words})\n","# For verbs:\n","print(\"run (verb):\", lemmatizer.lemmatize(\"running\", pos='v'))"],"metadata":{"id":"RDcsQ-_CXjyE","colab":{"base_uri":"https://localhost:8080/"},"outputId":"413d84c5-736d-4081-861c-0c65d481f700","executionInfo":{"status":"ok","timestamp":1754470533469,"user_tz":-330,"elapsed":3394,"user":{"displayName":"Rania","userId":"13750069140470329881"}}},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["{'running': 'running', 'better': 'better', 'wolves': 'wolf'}\n","run (verb): run\n"]}]},{"cell_type":"code","source":["# Exercise 6.1: Lemmatize tokens from Exercise 1.1 (both default and verb POS).\n","lemmatized = {w: lemmatizer.lemmatize(w) for w in tokens_lower if w.isalpha()}\n","lemmatized_verbs = {w: lemmatizer.lemmatize(w, pos='v') for w in tokens_lower if w.isalpha()}\n","print(\"Default Lemmas:\", lemmatized)\n","print(\"Verb Lemmas:\", lemmatized_verbs)"],"metadata":{"id":"g74PrNQ0XqWx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754470537523,"user_tz":-330,"elapsed":45,"user":{"displayName":"Rania","userId":"13750069140470329881"}},"outputId":"47a81c8e-73ac-4ab8-9a7e-a72d116d0f84"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Default Lemmas: {'machine': 'machine', 'learning': 'learning', 'models': 'model', 'power': 'power', 'many': 'many', 'nlp': 'nlp', 'tasks': 'task', 'they': 'they', 'learn': 'learn', 'patterns': 'pattern', 'from': 'from', 'data': 'data'}\n","Verb Lemmas: {'machine': 'machine', 'learning': 'learn', 'models': 'model', 'power': 'power', 'many': 'many', 'nlp': 'nlp', 'tasks': 'task', 'they': 'they', 'learn': 'learn', 'patterns': 'pattern', 'from': 'from', 'data': 'data'}\n"]}]},{"cell_type":"code","source":["# 7. Dependency Parsing\n","# Goal: Identify syntactic relationships between tokens.\n","import spacy\n","nlp = spacy.load(\"en_core_web_sm\")\n","doc = nlp(text)\n","for token in doc:\n","    print(token.text, token.dep_, token.head.text)\n"],"metadata":{"id":"4drXo7zjXss9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2eb2cabb-3f8f-485f-d3dd-80595b2700a6","executionInfo":{"status":"ok","timestamp":1754470546605,"user_tz":-330,"elapsed":4958,"user":{"displayName":"Rania","userId":"13750069140470329881"}}},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Natural compound Language\n","Language compound Processing\n","Processing nsubj enables\n","enables ROOT enables\n","machines nsubj understand\n","to aux understand\n","understand ccomp enables\n","human amod language\n","language dobj understand\n",". punct enables\n"]}]},{"cell_type":"code","source":["# Exercise 7.1: Parse the sentence “They learn patterns from data” and list each token’s dependency label and head.\n","doc2 = nlp(\"They learn patterns from data\")\n","for token in doc2:\n","    print(token.text, token.dep_, token.head.text)"],"metadata":{"id":"LmRo7ZwEXvGH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754470559496,"user_tz":-330,"elapsed":25,"user":{"displayName":"Rania","userId":"13750069140470329881"}},"outputId":"0b30c53e-1ace-4197-aa77-cbd3dae2d580"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["They nsubj learn\n","learn ROOT learn\n","patterns dobj learn\n","from prep patterns\n","data pobj from\n"]}]},{"cell_type":"code","source":["# 8. Named-Entity Recognition (NER)\n","# Goal: Extract real-world entities from text.\n","doc = nlp(\"Google was founded in 1998 by Larry Page and Sergey Brin in California.\")\n","for ent in doc.ents:\n","    print(ent.text, ent.label_)"],"metadata":{"id":"k78PeFooXxwp","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9595e488-d3d0-4766-96a6-cd535f28ed61","executionInfo":{"status":"ok","timestamp":1754470568798,"user_tz":-330,"elapsed":21,"user":{"displayName":"Rania","userId":"13750069140470329881"}}},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Google ORG\n","1998 DATE\n","Larry Page PERSON\n","Sergey Brin PERSON\n","California GPE\n"]}]},{"cell_type":"code","source":["#Exercise 8.1: Run NER on this sentence and add at least two more sentences of your own.\n","ner_text = \"\"\"Google was founded in 1998 by Larry Page and Sergey Brin in California.\n","Apple Inc. is headquartered in Cupertino.\n","Elon Musk leads SpaceX and Tesla.\"\"\"\n","\n","doc3 = nlp(ner_text)\n","for ent in doc3.ents:\n","    print(ent.text, ent.label_)"],"metadata":{"id":"bwBm0NMJX3WN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1754470583126,"user_tz":-330,"elapsed":29,"user":{"displayName":"Rania","userId":"13750069140470329881"}},"outputId":"ed094789-86e1-4349-ec03-373500a1ee64"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Google ORG\n","1998 DATE\n","Larry Page PERSON\n","Sergey Brin PERSON\n","California GPE\n","Apple Inc. ORG\n","Cupertino GPE\n","Elon Musk PERSON\n","SpaceX PERSON\n","Tesla NORP\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"nU7ZdJSQmIpf"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}