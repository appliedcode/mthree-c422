{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO474hp4wdix97zyxFOMlA8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/appliedcode/mthree-c422/blob/mthree-422-srilatha/FE_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RpFSO72PY05C"
      },
      "outputs": [],
      "source": [
        "# Step 1: Install and Import Libraries\n",
        "!pip install pandas numpy scikit-learn -q\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.exceptions import DataConversionWarning\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for clearer output\n",
        "warnings.filterwarnings(action='ignore', category=DataConversionWarning)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load and Split the Dataset\n",
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Split into train and validation sets\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['Survived'])"
      ],
      "metadata": {
        "id": "7IDxyBPgawDI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XsRb2ZuDbS2D"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Define the Pipeline Functions\n",
        "def clean_data(df):\n",
        "    df = df.drop(columns=[\"PassengerId\",\"Ticket\",\"Cabin\"], errors=\"ignore\").copy()\n",
        "    # Impute Age and Embarked\n",
        "    df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].median())\n",
        "    df[\"Embarked\"] = df[\"Embarked\"].fillna(df[\"Embarked\"].mode()[0])\n",
        "    return df\n",
        "\n",
        "def engineer_features(df):\n",
        "    df = df.copy()\n",
        "    # Title extraction\n",
        "    df[\"Title\"] = df[\"Name\"].str.extract(r\",\\s*([^\\.]+)\\.\")\n",
        "    rare_titles = [\"Lady\",\"Countess\",\"Capt\",\"Col\",\"Don\",\"Dr\",\"Major\",\"Rev\",\"Sir\",\"Jonkheer\",\"Dona\"]\n",
        "    df[\"Title\"] = df[\"Title\"].replace(rare_titles, \"Rare\")\n",
        "    # Family size & is alone\n",
        "    df[\"FamilySize\"] = df[\"SibSp\"] + df[\"Parch\"] + 1\n",
        "    df[\"IsAlone\"] = (df[\"FamilySize\"] == 1).astype(int)\n",
        "    # Fare and Age bins\n",
        "    df[\"FareBin\"] = pd.qcut(df[\"Fare\"].fillna(0), 4, labels=False)\n",
        "    df[\"AgeBin\"]  = pd.cut(df[\"Age\"], bins=[0,12,20,40,60,100], labels=False)\n",
        "    # Drop unused columns\n",
        "    df = df.drop(columns=[\"Name\",\"SibSp\",\"Parch\"])\n",
        "    # One-hot encode\n",
        "    df = pd.get_dummies(df, columns=[\"Sex\",\"Embarked\",\"Title\"], drop_first=True)\n",
        "    return df\n",
        "\n",
        "def validate_data(df):\n",
        "    errors = []\n",
        "    # Check for nulls\n",
        "    null_counts = df.isnull().sum()\n",
        "    if null_counts.any():\n",
        "        errors.append(f\"Null values found:\\n{null_counts[null_counts>0]}\")\n",
        "    # Check expected columns\n",
        "    expected_cols = {\"Survived\",\"Pclass\",\"Age\",\"Fare\",\"FamilySize\",\"IsAlone\",\"FareBin\",\"AgeBin\"}\n",
        "    missing = expected_cols - set(df.columns)\n",
        "    if missing:\n",
        "        errors.append(f\"Missing columns: {missing}\")\n",
        "    return errors\n"
      ],
      "metadata": {
        "id": "MrPB0SBQumWn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Execute the Pipeline\n",
        "# Clean and feature-engineer training data\n",
        "train_clean = clean_data(train_df)\n",
        "train_feat  = engineer_features(train_clean)\n",
        "train_errors = validate_data(train_feat)\n",
        "print(\"Train validation errors:\", train_errors or \"None\")\n",
        "\n",
        "# Clean and feature-engineer validation data\n",
        "val_clean = clean_data(val_df)\n",
        "val_feat  = engineer_features(val_clean)\n",
        "val_errors = validate_data(val_feat)\n",
        "print(\"Validation validation errors:\", val_errors or \"None\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRoF-inKbwJr",
        "outputId": "7c766a17-9f98-41dc-a0b5-4075edef321c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train validation errors: None\n",
            "Validation validation errors: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Save Prepared Data\n",
        "train_feat.to_csv(\"titanic_train_prepared.csv\", index=False)\n",
        "val_feat.to_csv(\"titanic_val_prepared.csv\", index=False)"
      ],
      "metadata": {
        "id": "cJX9Oljgbzoa"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}