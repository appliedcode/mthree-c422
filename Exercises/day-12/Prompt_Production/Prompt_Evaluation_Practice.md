## **Selfâ€‘Practice Problem Set â€“ Prompt Integration \& Performance Evaluation**

***

### **Sectionâ€¯1 â€“ Integrating Prompts in Applications and Pipelines**

**Problemâ€¯1 â€“ Multiâ€‘Stage Prompt Pipeline**

- Design a twoâ€‘step pipeline where:

1. The first prompt summarizes an article.
2. The second prompt takes that summary and generates 3 quiz questions.
- Implement a function that automatically feeds the output of stepâ€¯1 as the input to stepâ€¯2.
- Test with at least two different articles to ensure reusability.

***

**Problemâ€¯2 â€“ Roleâ€‘Switching in Prompt Pipelines**

- Create a small application where the AI can operate in two distinct roles:

1. As a **technical explainer** (breaking a topic into bullet points).
2. As a **motivational mentor** (giving encouragement on the same topic).
- Implement a role selector in code so the same base question can be processed in either role.

***

**Problemâ€¯3 â€“ Prompt Version Control Simulation**

- Store three versions of a prompt for a travel chatbot:
    - Versionâ€¯1: Minimal instructions
    - Versionâ€¯2: More detailed instructions
    - Versionâ€¯3: Includes examples (fewâ€‘shot)
- Write code to allow â€œswitchingâ€ versions dynamically via a variable or function parameter.
- Compare outputs for the same test queries.

***

### **Sectionâ€¯2 â€“ Prompt Performance Measurement and Evaluation**

**Problemâ€¯4 â€“ Keywordâ€‘Based Scoring**

- Choose a test prompt (e.g., â€œExplain how photosynthesis worksâ€).
- Define a set of 5 keywords you expect in the response.
- Write a function to:
    - Run the prompt.
    - Score the output based on how many expected keywords appear.
- Display a â€œkeyword match scoreâ€ for each run.

***

**Problemâ€¯5 â€“ Response Length Evaluation**

- For a given prompt, run it at least 3 times.
- Record the **word count** for each output.
- Calculate:
    - Average length.
    - Minimum and maximum length.
- Reflect on whether differences in length affect clarity.

***

**Problemâ€¯6 â€“ Consistency Measurement**

- Run the same prompt 5 times with the same parameters.
- Compare each outputâ€™s similarity using a textâ€‘matching approach (e.g., `difflib.SequenceMatcher`).
- Calculate an average **consistency score**.
- Decide if the current prompt is â€œstableâ€ or needs refinement.

***

**Problemâ€¯7 â€“ Simulated User Feedback Loop**

- Create a simple rating system where after each AI response, a â€œuserâ€ assigns a rating from 1â€“5 (manually or randomly).
- Store prompt, response excerpt, and rating in a log.
- At the end, output the **average rating per prompt version** to identify betterâ€‘performing prompts.

***

**Problemâ€¯8 â€“ Latency Tracking for Prompt Performance**

- Time how long the API takes to return a response for each prompt call.
- Record:
    - Prompt text
    - Response time
    - Word count of output
- Plot or print the tradeâ€‘off between latency and length, noting any patterns.

***

### **ğŸ“Œ Instructions for Students**

For each problem:

1. Implement the code in Google Colab.
2. Try with at least **two different prompts or datasets**.
3. For performance evaluation tasks, **capture metrics** (keywords, similarity, latency) and store results in a dictionary or DataFrame.
4. At the end, write a short reflection on:
    - What prompt style worked best?
    - Which metrics were most useful for evaluation?
    - What tradeâ€‘offs you noticed between detail, speed, and consistency.

***

