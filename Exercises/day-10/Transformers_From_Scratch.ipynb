{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDTMsDm0Gf7zW8D/BLqQU+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/appliedcode/mthree-c422/blob/mthree-422-salleh/Exercises/day-10/Transformers_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem Statement: Sentiment Classification on IMDb Movie Reviews Using BERT\n",
        "Objective\n",
        "\n",
        "Build and fine-tune a Transformer-based model (BERT) to classify movie reviews from the IMDb dataset into positive or negative sentiment. This task involves data cleaning, tokenization, model training, evaluation, and analysis, following a similar pipeline demonstrated in the transformer tweet sentiment example.\n",
        "Dataset\n",
        "\n",
        "IMDb Movie Reviews\n",
        "\n",
        "    Publicly accessible via the Hugging Face Datasets library (no manual download or sign-in required).\n",
        "    Loading code snippet:\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"imdb\")\n",
        "train = dataset[\"train\"]\n",
        "test = dataset[\"test\"]\n",
        "\n",
        "    Dataset size: 25,000 training samples and 25,000 testing samples.\n",
        "    Structure: Each example contains a text field (the movie review) and a label field (0 = negative, 1 = positive).\n",
        "\n",
        "Learning Objectives\n",
        "\n",
        "    Clean and preprocess natural language movie reviews (remove HTML tags, special characters, unwanted whitespace).\n",
        "    Tokenize and encode text using BertTokenizer.\n",
        "    Fine-tune BertForSequenceClassification for binary sentiment classification.\n",
        "    Evaluate model with classification metrics (precision, recall, F1-score).\n",
        "    Analyze model predictions, including inspection of correctly and incorrectly classified samples.\n",
        "\n",
        "Tasks\n",
        "\n",
        "    Data Loading & Exploration\n",
        "        Load the IMDb dataset directly using Hugging Faceâ€™s load_dataset(\"imdb\") function.\n",
        "        Analyze dataset distribution and sample texts to understand the data.\n",
        "    Data Cleaning\n",
        "        Clean the review texts to remove noise such as HTML tags and punctuation.\n",
        "        Prepare the cleaned text for tokenization.\n",
        "    Dataset Preparation\n",
        "        Implement a PyTorch Dataset class similar to TweetDataset, which performs tokenization, padding, and truncation using BertTokenizer.\n",
        "        Ensure token sequences have a max length (e.g., 128) for efficient batching.\n",
        "    Model Setup and Training\n",
        "        Load the pretrained BERT base uncased model configured for sequence classification with two output labels.\n",
        "        Define training parameters such as batch size, epochs, and logging setup.\n",
        "        Use the Hugging Face Trainer API to train and validate the model on the IMDb data.\n",
        "    Evaluation and Reporting\n",
        "        Generate a detailed classification report with precision, recall, and F1-score.\n",
        "        Create a DataFrame comparing review texts, actual labels, and predicted labels for sample inspection.\n",
        "\n",
        "Deliverables\n",
        "\n",
        "    Python notebook or script containing fully documented code for the entire pipeline.\n",
        "    Classification report and insights into model performance and errors.\n",
        "    Examples of correct and incorrect predictions with analysis.\n",
        "\n",
        "Getting Started Example\n",
        "\n"
      ],
      "metadata": {
        "id": "_FtSLr3R5K7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load IMDb dataset\n",
        "dataset = load_dataset(\"imdb\")\n",
        "train = dataset[\"train\"]\n",
        "test = dataset[\"test\"]\n",
        "\n",
        "print(f\"Number of training samples: {len(train)}\")\n",
        "print(f\"Number of test samples: {len(test)}\")\n",
        "\n",
        "# Sample review and label\n",
        "print(\"Sample text:\", train[0][\"text\"][:200])\n",
        "print(\"Sample label:\", train[0][\"label\"])\n",
        "'''\n",
        "\n"
      ],
      "metadata": {
        "id": "BVEqgTJQ5ik6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This problem statement ensures you use a reliable, easy-to-access dataset with no external sign-in or manual downloads, perfectly fitting into a Transformer fine-tuning workflow.\n"
      ],
      "metadata": {
        "id": "UU6jbdXK5oa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary Libraries\n",
        "import re\n",
        "import numpy as np\n",
        "import string\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding, logging\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "logging.set_verbosity_error()"
      ],
      "metadata": {
        "id": "ozwDbubN6HML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load train and test data\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"imdb\")\n",
        "print(dataset)\n",
        "\n",
        "# Access train and test splits\n",
        "train = dataset[\"train\"]\n",
        "test = dataset[\"test\"]\n",
        "\n",
        "# Check train data\n",
        "print(train[0])           # First training example\n",
        "print(train.features)     # Schema (text + label)\n",
        "print(train.num_rows)     # Number of examples\n"
      ],
      "metadata": {
        "id": "Ru0vFWY86gWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU availability\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "wVuTAVI26pTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean text\n",
        "def clean_text(text):\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r\"<.*?>\", \" \", text)\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Clean dataset\n",
        "def preprocess_dataset(dataset):\n",
        "    return dataset.map(lambda example: {\"clean_text\": clean_text(example[\"text\"])})\n",
        "\n",
        "# Cleaned datasets\n",
        "train_clean = preprocess_dataset(train)\n",
        "test_clean = preprocess_dataset(test)\n",
        "\n",
        "# Check cleaned dataset\n",
        "print(train_clean[0][\"clean_text\"])"
      ],
      "metadata": {
        "id": "qEPUNXto6qZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_fn(my_dataset):\n",
        "    return tokenizer(my_dataset[\"clean_text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "train_tokenized = train_clean.map(tokenize_fn, batched=True)\n",
        "test_tokenized = test_clean.map(tokenize_fn, batched=True)"
      ],
      "metadata": {
        "id": "72WQzZk46vOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "num_labels = len(set(train[\"label\"]) | set(test[\"label\"]))\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n"
      ],
      "metadata": {
        "id": "87OOIH6V60oF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=16,    # Increase batch size if memory allows\n",
        "    per_device_eval_batch_size=64,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "id": "2h5F2HNs641h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized,\n",
        "    eval_dataset=test_tokenized,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer),\n",
        ")"
      ],
      "metadata": {
        "id": "OUbCLuLt7BOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "ZRojXwxe7FGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate\n",
        "predictions = trainer.predict(test_tokenized)\n",
        "preds = torch.argmax(torch.tensor(predictions.predictions), axis=1)"
      ],
      "metadata": {
        "id": "S2sJxqlb7JGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Report\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(test_tokenized[\"label\"], preds))"
      ],
      "metadata": {
        "id": "Z-cM8yzT7J7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert predictions and actual labels to lists\n",
        "predicted_labels = preds.tolist()\n",
        "actual_labels = test_tokenized[\"label\"]"
      ],
      "metadata": {
        "id": "wFyS8hcg7RWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare in a DataFrame\n",
        "comparison_df = pd.DataFrame({\n",
        "    \"text\": test_tokenized[\"clean_text\"],\n",
        "    \"actual\": actual_labels,\n",
        "    \"predicted\": predicted_labels\n",
        "})\n",
        "\n",
        "# Print a sample comparison\n",
        "comparison_df.head(20)\n"
      ],
      "metadata": {
        "id": "6jyIEnPI7Sa3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}