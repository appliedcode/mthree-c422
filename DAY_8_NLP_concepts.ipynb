{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvYfyEsSySZZGU2gaPG+/P"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pihWzDWj87R"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "from nltk.stem.porter import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p_stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "2YxalZAtkonJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['run','runner','running','ran','runs','easily','fairly']"
      ],
      "metadata": {
        "id": "jhUXZ9hTktOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    print(word+' --> '+p_stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAL50m4Vk4gx",
        "outputId": "8ec5d44d-bbcf-4eed-d8b1-d9216a192e67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run --> run\n",
            "runner --> runner\n",
            "running --> run\n",
            "ran --> ran\n",
            "runs --> run\n",
            "easily --> easili\n",
            "fairly --> fairli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "# The Snowball Stemmer requires that you pass a language parameter\n",
        "s_stemmer = SnowballStemmer(language='english')\n",
        "words = ['run','runner','running','ran','runs','easily','fairly']\n",
        "# words = ['generous','generation','generously','generate']\n",
        "for word in words:\n",
        "    print(word+' --> '+s_stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZHfgo1MlD4H",
        "outputId": "a426308d-baa6-4f04-dfb6-16d2a13ddf9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run --> run\n",
            "runner --> runner\n",
            "running --> run\n",
            "ran --> ran\n",
            "runs --> run\n",
            "easily --> easili\n",
            "fairly --> fair\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['consolingly']\n",
        "print('Porter Stemmer:')\n",
        "for word in words:\n",
        "    print(word+' --> '+p_stemmer.stem(word))words = ['consolingly']\n",
        "print('Porter Stemmer:')\n",
        "for word in words:\n",
        "    print(word+' --> '+p_stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lK6GLL_lRc_",
        "outputId": "44f4fa6c-35cc-4316-b628-840d07a9ef3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter Stemmer:\n",
            "consolingly --> consolingli\n",
            "Porter2 Stemmer:\n",
            "consolingly --> consol\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['according']\n",
        "print('Porter Stemmer:')\n",
        "for word in words:\n",
        "    print(word+' --> '+p_stemmer.stem(word))\n",
        "print('Porter2 Stemmer:')\n",
        "for word in words:\n",
        "    print(word+' --> '+s_stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8gsly1ilf-_",
        "outputId": "66846f02-fedc-45da-d547-bf2c32ed6660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter Stemmer:\n",
            "according --> accord\n",
            "Porter2 Stemmer:\n",
            "according --> accord\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform standard imports:\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "# Print the set of spaCy's default stop words (remember that sets are unordered):\n",
        "print(nlp.Defaults.stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XL_IUwPvmB1p",
        "outputId": "76735bf3-69a9-456c-b640-593217b87ce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'namely', 'very', 'a', 'you', 'why', 'these', 'say', 'had', 'noone', 'ourselves', 'such', 'becoming', \"'d\", 'no', 'themselves', 'in', 'there', 'this', 'third', 'ours', 'nevertheless', 'mostly', 'upon', 'every', 'bottom', 'hereafter', 'something', 'take', '’m', '’ll', 'off', 'serious', 'sometime', 'i', 'around', 'two', 'his', 'whenever', 'some', \"'ll\", 'please', 'may', 'back', 'thus', 'at', 'us', 'even', 'do', 'him', 'again', 'are', 'empty', 'but', 'among', 'side', 'almost', 'therefore', 'onto', 'never', 'n‘t', 'he', 'whatever', 'am', 'both', 'whither', 'my', 'beforehand', 'might', 'whoever', 'since', 'across', 'which', 'should', 'became', 'own', 'really', \"'s\", 'above', 'either', \"'m\", 'me', 'those', \"'ve\", 'still', 'from', 'through', 'than', 'anywhere', 'the', 'whole', 'of', 'fifty', 'sometimes', 'sixty', 'hers', 'under', 'except', 'be', 'has', 'also', 'other', 'often', 'done', 'whereafter', 'moreover', 'on', 'n’t', '’s', 'everything', 'nowhere', 'next', 'ten', 'too', 'because', 'if', 'therein', '‘ll', 'everyone', 'now', 'thereby', 'another', 'within', 'thru', 'so', 'besides', 'somewhere', 'can', 'to', 'hence', 'unless', 'keep', 'much', 'three', 'hereupon', 'himself', 'will', 'did', 'nine', 'it', 'eleven', 'seems', 'nor', 'twenty', 'together', 'for', 'was', 'go', 'an', 'been', 'amount', 'thereupon', 'several', 'few', 'who', 'whether', 'anything', 'quite', 'your', 'anyway', 'fifteen', 'however', 'during', 'everywhere', 'wherever', 'and', 'many', 'against', 'does', 'yours', 'hereby', 'nothing', 'well', 'were', 'out', 'whereupon', 'alone', 'wherein', 'mine', 'latter', 'same', 'until', 'beyond', 'already', 'enough', 'that', 'someone', 'elsewhere', 'meanwhile', 'neither', 'amongst', 'less', 'nobody', 'latterly', 'via', 'behind', 'otherwise', 'eight', 'name', 'beside', 'up', 'six', 'myself', \"'re\", 'ca', 'could', 'we', '’re', 'they', 'give', 'while', 'here', 'with', 'is', 'must', 'into', 'whose', 'our', '‘d', 'anyhow', 'front', 'how', 'then', 'further', 'always', 'only', 'hundred', 'where', 'being', 'various', 'toward', 'towards', 'once', 'more', 'yourselves', 'before', 'rather', 'per', 'whereby', 'somehow', 'none', 'her', 'formerly', 'seeming', 'afterwards', 'any', '’d', 'though', '‘ve', 'she', 'between', 'least', 'see', 'using', 'seem', 'get', 'about', 'made', 'whence', 'anyone', 'due', 'all', 'its', 'cannot', 'doing', 'first', 'regarding', 'herself', 'one', 'ever', '‘m', 'although', 'yet', 'them', '‘s', 'by', 'what', 'whereas', 're', 'last', 'others', 'without', 'twelve', 'move', 'their', 'as', 'each', 'put', '’ve', 'becomes', 'most', 'throughout', '‘re', 'make', 'else', 'five', 'would', 'when', 'used', 'down', 'perhaps', 'just', 'over', 'not', 'itself', 'become', 'former', 'herein', 'call', 'part', 'thereafter', 'below', 'thence', 'or', 'full', 'yourself', 'have', \"n't\", 'top', 'forty', 'whom', 'indeed', 'along', 'four', 'show', 'after', 'seemed'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(nlp.Defaults.stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bz0vn70rmMZD",
        "outputId": "cb34d8ab-631d-4e89-923a-b1614cb7ef24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "326"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.vocab['myself'].is_stop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phzIh5eomOXa",
        "outputId": "20c1eb9a-9f0a-4370-ce65-2808d681f857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.vocab['mouni'].is_stop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQ0S0wGymWSV",
        "outputId": "9491aff2-496b-40a0-90af-c60be0b1db6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the word to the set of stop words. Use lowercase!\n",
        "nlp.Defaults.stop_words.add('btw')\n",
        "\n",
        "# Set the stop_word tag on the lexeme\n",
        "nlp.vocab['btw'].is_stop = True"
      ],
      "metadata": {
        "id": "OJQ0sGAgmdTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(nlp.Defaults.stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pEPWqSemjz1",
        "outputId": "171de2a9-5225-4caf-fa88-0ad1925b9a02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "327"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.vocab['btw'].is_stop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k68l7yKXmpYn",
        "outputId": "1f8a3ac2-b713-40ca-b2a9-6588b2edb576"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the word from the set of stop words\n",
        "nlp.Defaults.stop_words.remove('beyond')\n",
        "\n",
        "# Remove the stop_word tag from the lexeme\n",
        "nlp.vocab['beyond'].is_stop = False\n",
        "len(nlp.Defaults.stop_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vpdi-9Kym0XI",
        "outputId": "e2c76f62-8181-4f92-e916-420e000bf37e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "326"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.vocab['beyond'].is_stop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xgy7DnHjm3T4",
        "outputId": "8ad0bc7d-9054-45dc-ea01-496fc04cea6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization\n"
      ],
      "metadata": {
        "id": "srge6775oRKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import spaCy and load the language library\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "sTcQyGKboal5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a string that includes opening and closing quotation marks\n",
        "mystring = '\"We\\'re moving to L.A.!\"'\n",
        "print(mystring)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noEtCRVdofP7",
        "outputId": "58dbea11-d150-4744-d377-2481a5c355a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"We're moving to L.A.!\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Doc object and explore tokens\n",
        "doc = nlp(mystring)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, end=' | ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhsNrsDgok1S",
        "outputId": "aaacf94f-fb7b-49b2-954f-5f573fed8a6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\" | We | 're | moving | to | L.A. | ! | \" | "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc2 = nlp(u\"We're here to help! Send snail-mail, email support@oursite.com or visit us at http://www.oursite.com!\")\n",
        "\n",
        "for t in doc2:\n",
        "    print(t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bew3hqdCoqEm",
        "outputId": "9787f3f0-19b6-4e99-914a-72e158ce3b36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We\n",
            "'re\n",
            "here\n",
            "to\n",
            "help\n",
            "!\n",
            "Send\n",
            "snail\n",
            "-\n",
            "mail\n",
            ",\n",
            "email\n",
            "support@oursite.com\n",
            "or\n",
            "visit\n",
            "us\n",
            "at\n",
            "http://www.oursite.com\n",
            "!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc3 = nlp(u'A 5km NYC cab ride costs $10.30')\n",
        "\n",
        "for t in doc3:\n",
        "    print(t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtDNFASup2Zi",
        "outputId": "213ba635-7f8c-4c81-df0c-cbd14fe80ae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A\n",
            "5\n",
            "km\n",
            "NYC\n",
            "cab\n",
            "ride\n",
            "costs\n",
            "$\n",
            "10.30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc4 = nlp(u\"Let's visit St. Louis in the U.S. next year.\")\n",
        "\n",
        "for t in doc4:\n",
        "    print(t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtsTtPgIp84F",
        "outputId": "e8d7994c-7243-47ac-c540-6ee056df1ad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let\n",
            "'s\n",
            "visit\n",
            "St.\n",
            "Louis\n",
            "in\n",
            "the\n",
            "U.S.\n",
            "next\n",
            "year\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_ZMV8edqBwZ",
        "outputId": "89b1f408-8ce4-4eda-b79f-72205a490f3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(doc.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xMLVG2FqHQl",
        "outputId": "c934f5f5-32ee-4618-c14a-a3af64832f6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "794"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc5 = nlp(u'It is better to give than to receive.')\n",
        "\n",
        "# Retrieve the third token:\n",
        "doc5[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ot_88DGqLcK",
        "outputId": "13d34c78-6f85-4eda-edd0-0979b1cd2430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "better"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "practice"
      ],
      "metadata": {
        "id": "L77ZYBjvq7Jy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install --upgrade pip setuptools wheel -q\n",
        "!pip install --quiet nltk spacy textblob sklearn\n",
        "\n",
        "# Download NLTK data and spaCy model\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "!python -m spacy download en_core_web_sm -q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PokJpAarE8g",
        "outputId": "a4ce2aa1-4673-4cc4-a493-9d177948763d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/1.8 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Tokenization\n",
        "# Goal: Split text into tokens (words and punctuation).\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "text = \"Natural Language Processing enables machines to understand human language.\"\n",
        "print(\"Sentences:\", sent_tokenize(text))\n",
        "print(\"Tokens:\", word_tokenize(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvlvVAu4rc9M",
        "outputId": "58bbec2d-9196-43e9-a815-8895b8147750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences: ['Natural Language Processing enables machines to understand human language.']\n",
            "Tokens: ['Natural', 'Language', 'Processing', 'enables', 'machines', 'to', 'understand', 'human', 'language', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 1.1: Tokenize the following paragraph into words and sentences:\n",
        "\n",
        "paragraph = \"Machine learning models power many NLP tasks. They learn patterns from data!\"\n",
        "\n",
        "\n",
        "\n",
        "# 2. Part-of-Speech Tagging\n",
        "# Goal: Assign grammatical tags to each token.\n",
        "import nltk\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(pos_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsSnznzurfP6",
        "outputId": "ed7ae829-705f-458c-9d55-afcf00b9e63a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('enables', 'VBZ'), ('machines', 'NNS'), ('to', 'TO'), ('understand', 'VB'), ('human', 'JJ'), ('language', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2.1: Tag POS for tokens from your Exercise 1.1.\n",
        "\n",
        "\n",
        "# 3. Stemming\n",
        "# Goal: Reduce words to their root forms (may be non-dictionary).\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "words = [\"running\", \"runs\", \"ran\", \"easily\", \"fairly\"]\n",
        "print({w: stemmer.stem(w) for w in words})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfMhcw9urmc4",
        "outputId": "56e35cc8-2073-4e65-ff5d-fa2476618029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'running': 'run', 'runs': 'run', 'ran': 'ran', 'easily': 'easili', 'fairly': 'fairli'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Stop-Word Filtering\n",
        "# Goal: Remove common, low-value words.\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = word_tokenize(text.lower())\n",
        "filtered = [w for w in tokens if w.isalpha() and w not in stop_words]\n",
        "print(filtered)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVcLMh9VrvDx",
        "outputId": "ff3467d6-fd5c-490c-efe2-b2cd7149c9c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natural', 'language', 'processing', 'enables', 'machines', 'understand', 'human', 'language']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 4.1: Filter stop words from your Exercise 1.1 tokens.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 5. Vocabulary Matching\n",
        "# Goal: Check tokens against a predefined vocabulary.\n",
        "\n",
        "vocab = {\"natural\", \"language\", \"machine\", \"data\", \"processing\"}\n",
        "tokens = [w.lower() for w in word_tokenize(text)]\n",
        "in_vocab = [w for w in tokens if w.isalpha() and w in vocab]\n",
        "print(\"In-vocab tokens:\", in_vocab)\n",
        "print(\"OOV tokens:\", [w for w in tokens if w.isalpha() and w not in vocab])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHA6PnKkr0-G",
        "outputId": "2145494e-d382-41c0-8742-909f5405860e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In-vocab tokens: ['natural', 'language', 'processing', 'language']\n",
            "OOV tokens: ['enables', 'machines', 'to', 'understand', 'human']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Lemmatization\n",
        "# Goal: Convert words to their dictionary form.\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [\"running\", \"better\", \"wolves\"]\n",
        "print({w: lemmatizer.lemmatize(w) for w in words})\n",
        "# For verbs:\n",
        "print(\"run (verb):\", lemmatizer.lemmatize(\"running\", pos='v'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVWgSsKgr7DE",
        "outputId": "1050b51f-ee30-4592-b4c7-264eb7d896d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'running': 'running', 'better': 'better', 'wolves': 'wolf'}\n",
            "run (verb): run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Dependency Parsing\n",
        "# Goal: Identify syntactic relationships between tokens.\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "    print(token.text, token.dep_, token.head.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57fm7vu1sCX1",
        "outputId": "9c895af7-d38d-470d-b144-cf4bc6b43a4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural compound Language\n",
            "Language compound Processing\n",
            "Processing nsubj enables\n",
            "enables ROOT enables\n",
            "machines nsubj understand\n",
            "to aux understand\n",
            "understand ccomp enables\n",
            "human amod language\n",
            "language dobj understand\n",
            ". punct enables\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 7.1: Parse the sentence “They learn patterns from data” and list each token’s dependency label and head.\n",
        "\n",
        "\n",
        "# 8. Named-Entity Recognition (NER)\n",
        "# Goal: Extract real-world entities from text.\n",
        "doc = nlp(\"Google was founded in 1998 by Larry Page and Sergey Brin in California.\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYd6kjuMsZS0",
        "outputId": "54140a17-538f-4b56-8bae-2dd48a9ef88f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google ORG\n",
            "1998 DATE\n",
            "Larry Page PERSON\n",
            "Sergey Brin PERSON\n",
            "California GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc2 = nlp(\"Elon Musk announced that SpaceX will launch Starship from Texas in 2025.\")\n",
        "doc3 = nlp(\"Apple Inc. acquired Beats Electronics for $3 billion in 2014.\")\n"
      ],
      "metadata": {
        "id": "Z8dTR8dAsbDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization"
      ],
      "metadata": {
        "id": "V4mUMosstN3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform standard imports:\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc1 = nlp(u\"I am a runner running in a race because I love to run since I ran today\")\n",
        "\n",
        "for token in doc1:\n",
        "    print(token.text, '\\t', token.pos_, '\\t', token.lemma, '\\t', token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7Z43PhctPUl",
        "outputId": "097a0a3c-3ae1-4ad2-eacb-914ed358fa90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I \t PRON \t 4690420944186131903 \t I\n",
            "am \t AUX \t 10382539506755952630 \t be\n",
            "a \t DET \t 11901859001352538922 \t a\n",
            "runner \t NOUN \t 12640964157389618806 \t runner\n",
            "running \t VERB \t 12767647472892411841 \t run\n",
            "in \t ADP \t 3002984154512732771 \t in\n",
            "a \t DET \t 11901859001352538922 \t a\n",
            "race \t NOUN \t 8048469955494714898 \t race\n",
            "because \t SCONJ \t 16950148841647037698 \t because\n",
            "I \t PRON \t 4690420944186131903 \t I\n",
            "love \t VERB \t 3702023516439754181 \t love\n",
            "to \t PART \t 3791531372978436496 \t to\n",
            "run \t VERB \t 12767647472892411841 \t run\n",
            "since \t SCONJ \t 10066841407251338481 \t since\n",
            "I \t PRON \t 4690420944186131903 \t I\n",
            "ran \t VERB \t 12767647472892411841 \t run\n",
            "today \t NOUN \t 11042482332948150395 \t today\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_lemmas(text):\n",
        "    for token in text:\n",
        "        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}')"
      ],
      "metadata": {
        "id": "1B_qVWjytap8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc2 = nlp(u\"I saw eighteen mice today!\")\n",
        "\n",
        "show_lemmas(doc2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6fCgI7DtkxW",
        "outputId": "3bb89607-d6e2-4c76-a526-90f49755ca00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I            PRON   4690420944186131903    I\n",
            "saw          VERB   11925638236994514241   see\n",
            "eighteen     NUM    9609336664675087640    eighteen\n",
            "mice         NOUN   1384165645700560590    mouse\n",
            "today        NOUN   11042482332948150395   today\n",
            "!            PUNCT  17494803046312582752   !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc3 = nlp(u\"I am meeting him tomorrow at the meeting.\")\n",
        "\n",
        "show_lemmas(doc3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOpdxPrQtp7B",
        "outputId": "c98fb1df-68a2-4168-acc5-d5b0ed573743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I            PRON   4690420944186131903    I\n",
            "am           AUX    10382539506755952630   be\n",
            "meeting      VERB   6880656908171229526    meet\n",
            "him          PRON   1655312771067108281    he\n",
            "tomorrow     NOUN   3573583789758258062    tomorrow\n",
            "at           ADP    11667289587015813222   at\n",
            "the          DET    7425985699627899538    the\n",
            "meeting      NOUN   14798207169164081740   meeting\n",
            ".            PUNCT  12646065887601541794   .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc4 = nlp(u\"That's an enormous automobile\")\n",
        "\n",
        "show_lemmas(doc4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKlxleAdtwFM",
        "outputId": "bea5c519-a573-4a2c-cae1-3605a2509a17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "That         PRON   4380130941430378203    that\n",
            "'s           AUX    10382539506755952630   be\n",
            "an           DET    15099054000809333061   an\n",
            "enormous     ADJ    17917224542039855524   enormous\n",
            "automobile   NOUN   7211811266693931283    automobile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vocabulary and Matching"
      ],
      "metadata": {
        "id": "Lg7La9yauwyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform standard imports\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "# Import the Matcher library\n",
        "from spacy.matcher import Matcher\n",
        "matcher = Matcher(nlp.vocab)"
      ],
      "metadata": {
        "id": "VhLUnvuzuyBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern1 = [{'LOWER': 'solarpower'}]\n",
        "pattern2 = [{'LOWER': 'solar'}, {'LOWER': 'power'}]\n",
        "pattern3 = [{'LOWER': 'solar'}, {'IS_PUNCT': True}, {'LOWER': 'power'}]\n",
        "\n",
        "matcher.add('SolarPower', [pattern1, pattern2, pattern3])"
      ],
      "metadata": {
        "id": "c87APqftu3gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97361730",
        "outputId": "9666a4d8-a3ab-4307-d4e0-e260a116e981"
      },
      "source": [
        "doc = nlp(u'The solar power industry is booming! Solar-power is amazing. We need more solar power.')\n",
        "\n",
        "found_matches = matcher(doc)\n",
        "print(found_matches)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(8656102463236116519, 1, 3), (8656102463236116519, 7, 10), (8656102463236116519, 16, 18)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for match_id, start, end in found_matches:\n",
        "    string_id = nlp.vocab.strings[match_id]  # get string representation\n",
        "    span = doc[start:end]                    # get the matched span\n",
        "    print(match_id, string_id, start, end, span.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nUzr73yvSkM",
        "outputId": "617444d6-074c-40ba-f888-b156260f8276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8656102463236116519 SolarPower 1 3 solar power\n",
            "8656102463236116519 SolarPower 7 10 Solar-power\n",
            "8656102463236116519 SolarPower 16 18 solar power\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59cfad67",
        "outputId": "b40dd993-17b8-4a2e-d11b-c38efd8fda61"
      },
      "source": [
        "# Re-add the pattern to ensure it exists before attempting removal\n",
        "matcher.add('SolarPower', [pattern1, pattern2, pattern3])\n",
        "\n",
        "# Remove the pattern\n",
        "matcher.remove('SolarPower')\n",
        "found_matches = matcher(doc)\n",
        "print(found_matches)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3042078810.py:6: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
            "  found_matches = matcher(doc)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u'The Solar Power industry continues to grow as demand \\\n",
        "for solarpower increases. Solar-power cars are gaining popularity.')\n",
        "found_matches = matcher(doc)\n",
        "print(found_matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxPNHuMa3KLR",
        "outputId": "0bb2691e-679d-4317-efeb-81022f112248"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2667616934.py:3: UserWarning: [W036] The component 'matcher' does not have any patterns defined.\n",
            "  found_matches = matcher(doc)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for match_id, start, end in found_matches:\n",
        "    string_id = nlp.vocab.strings[match_id]  # get string representation\n",
        "    span = doc[start:end]                    # get the matched span\n",
        "    print(match_id, string_id, start, end, span.text)"
      ],
      "metadata": {
        "id": "exlF8T1J3QAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Redefine the patterns:\n",
        "pattern1 = [{'LOWER': 'solarpower'}]\n",
        "pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'power'}]\n",
        "\n",
        "# Add the new set of patterns to the 'SolarPower' matcher:\n",
        "matcher.add('SolarPower', [pattern1, pattern2]) # Add patterns as a list\n",
        "\n",
        "doc = nlp(u'The Solar Power industry continues to grow as demand \\\n",
        "for solarpower increases. Solar-power cars are gaining popularity.')\n",
        "found_matches = matcher(doc)\n",
        "print(found_matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-ibE7_n3Rt1",
        "outputId": "c3d5e5fb-20ae-4296-8356-3fd05dd5243b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(8656102463236116519, 1, 3), (8656102463236116519, 10, 11), (8656102463236116519, 13, 16)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pattern1 = [{'LOWER': 'solarpower'}]\n",
        "pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LEMMA': 'power'}] # CHANGE THIS PATTERN\n",
        "\n",
        "# Add the new set of patterns to the 'SolarPower' matcher:\n",
        "matcher.add('SolarPower', [pattern1, pattern2]) # Corrected arguments passed as a list\n",
        "doc2 = nlp(u'Solar-powered energy runs solar-powered cars.')\n",
        "found_matches = matcher(doc2)\n",
        "print(found_matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR0vN76Y3itn",
        "outputId": "803a1d0a-8cd7-423b-cf80-80ae0ec6c624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(8656102463236116519, 0, 3), (8656102463236116519, 5, 8)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pattern1 = [{'LOWER': 'solarpower'}]\n",
        "pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'power'}]\n",
        "pattern3 = [{'LOWER': 'solarpowered'}]\n",
        "pattern4 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'powered'}]\n",
        "\n",
        "# Add the new set of patterns to the 'SolarPower' matcher:\n",
        "matcher.add('SolarPower', [pattern1, pattern2, pattern3, pattern4])\n",
        "found_matches = matcher(doc2)\n",
        "print(found_matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bltP9Pl33wZN",
        "outputId": "ca4133c2-d2c9-42be-93e1-ba1e87b709e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(8656102463236116519, 0, 3), (8656102463236116519, 5, 8)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform standard imports, reset nlp\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "# Import the PhraseMatcher library\n",
        "from spacy.matcher import PhraseMatcher\n",
        "matcher = PhraseMatcher(nlp.vocab)"
      ],
      "metadata": {
        "id": "4y5qNdp13_XE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc3[665:685]  # Note that the fifth match starts at doc3[673]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lq93c_7R4bFE",
        "outputId": "1d3a7dc6-460e-4bc7-b572-85fb006eff5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc3[2975:2995]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhuA0UGd4gEs",
        "outputId": "ded415e7-388e-4a44-d9cf-aabebb106ea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a list of sentences\n",
        "sents = [sent for sent in doc3.sents]\n",
        "\n",
        "# In the next section we'll see that sentences contain start and end token values:\n",
        "print(sents[0].start, sents[0].end)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJ5UmS_k4iuv",
        "outputId": "af485250-8361-40b4-ef8b-923785cc5819"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over the sentence list until the sentence end value exceeds a match start value:\n",
        "for sent in sents:\n",
        "    # Iterate through each found match\n",
        "    for match_id, start, end in found_matches:\n",
        "        # Check if the match starts within the current sentence\n",
        "        if start < sent.end and start >= sent.start:\n",
        "            print(sent)\n",
        "            break  # Print the sentence and move to the next sentence\n",
        "    else:\n",
        "        continue # if no match was found in this sentence, continue to the next sentence\n",
        "    break # if a match was found and the sentence printed, break the outer loop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHVJJv0D4qS0",
        "outputId": "3e47681e-dc53-4d95-b32b-1619e79096c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am meeting him tomorrow at the meeting.\n"
          ]
        }
      ]
    }
  ]
}