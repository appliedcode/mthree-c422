{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMe0lT2n2tbbtLRx4EnALCC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/appliedcode/mthree-c422/blob/mthree-c422-Likhitha/FE_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ms1OQRl8ZZ3S"
      },
      "outputs": [],
      "source": [
        "!pip install pandas numpy scikit-learn -q\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.exceptions import DataConversionWarning\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings(action='ignore', category=DataConversionWarning)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['Survived'])"
      ],
      "metadata": {
        "id": "p_DkdBc9ZbLi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_unused_columns(df):\n",
        "    \"\"\"Drop irrelevant columns.\"\"\"\n",
        "    return df.drop(columns=[\"PassengerId\", \"Ticket\", \"Cabin\"], errors=\"ignore\").copy()\n",
        "\n",
        "def clean_data(df):\n",
        "    \"\"\"Clean missing values and format basic fields.\"\"\"\n",
        "    df = drop_unused_columns(df)\n",
        "    df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].median())\n",
        "    df[\"Fare\"] = df[\"Fare\"].fillna(df[\"Fare\"].median())\n",
        "    df[\"Embarked\"] = df[\"Embarked\"].fillna(df[\"Embarked\"].mode()[0])\n",
        "    return df"
      ],
      "metadata": {
        "id": "wrUJTgHmZnOT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def engineer_features(df):\n",
        "    \"\"\"Create new features and encode categorical variables.\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Extract title from name\n",
        "    df[\"Title\"] = df[\"Name\"].str.extract(r\",\\s*([^\\.]+)\\.\")\n",
        "    rare_titles = [\"Lady\", \"Countess\", \"Capt\", \"Col\", \"Don\", \"Dr\", \"Major\",\n",
        "                   \"Rev\", \"Sir\", \"Jonkheer\", \"Dona\"]\n",
        "    df[\"Title\"] = df[\"Title\"].replace(rare_titles, \"Rare\")\n",
        "    df[\"Title\"] = df[\"Title\"].replace({\"Mlle\": \"Miss\", \"Ms\": \"Miss\", \"Mme\": \"Mrs\"})\n",
        "\n",
        "    # Family size and is alone\n",
        "    df[\"FamilySize\"] = df[\"SibSp\"] + df[\"Parch\"] + 1\n",
        "    df[\"IsAlone\"] = (df[\"FamilySize\"] == 1).astype(int)\n",
        "\n",
        "    # Age and Fare bins\n",
        "    df[\"FareBin\"] = pd.qcut(df[\"Fare\"], 4, labels=False)\n",
        "    df[\"AgeBin\"] = pd.cut(df[\"Age\"], bins=[0, 12, 20, 40, 60, 100], labels=False)\n",
        "    df = df.drop(columns=[\"Name\", \"SibSp\", \"Parch\"])\n",
        "\n",
        "    # One-hot encoding\n",
        "    df = pd.get_dummies(df, columns=[\"Sex\", \"Embarked\", \"Title\"], drop_first=True)\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "-SDh-NHYZuyo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_data(df):\n",
        "    \"\"\"Validate data integrity and completeness.\"\"\"\n",
        "    errors = []\n",
        "    if df.isnull().any().any():\n",
        "        errors.append(\"Missing values detected.\")\n",
        "\n",
        "    expected_columns = {\"Survived\", \"Pclass\", \"Age\", \"Fare\", \"FamilySize\",\n",
        "                        \"IsAlone\", \"FareBin\", \"AgeBin\"}\n",
        "    missing_cols = expected_columns - set(df.columns)\n",
        "    if missing_cols:\n",
        "        errors.append(f\"Missing columns: {missing_cols}\")\n",
        "    return errors"
      ],
      "metadata": {
        "id": "_51_75O1Z77J"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(df):\n",
        "    \"\"\"Pipeline: clean, engineer, validate dataset.\"\"\"\n",
        "    cleaned = clean_data(df)\n",
        "    features = engineer_features(cleaned)\n",
        "    validation_errors = validate_data(features)\n",
        "    return features, validation_errors\n",
        ""
      ],
      "metadata": {
        "id": "AJmdNjoBaCUG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_prepared, train_errors = prepare_dataset(train_df)\n",
        "print(\"Train validation errors:\", train_errors or \"None\")\n",
        "\n",
        "# Validation Set\n",
        "val_prepared, val_errors = prepare_dataset(val_df)\n",
        "print(\"Validation validation errors:\", val_errors or \"None\")\n",
        "\n",
        "# Step 5: Save Output\n",
        "train_prepared.to_csv(\"titanic_train_prepared.csv\", index=False)\n",
        "val_prepared.to_csv(\"titanic_val_prepared.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2bs4sqYaJaT",
        "outputId": "f9061fd3-3f63-440f-e0e4-67f5cca03589"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train validation errors: None\n",
            "Validation validation errors: None\n"
          ]
        }
      ]
    }
  ]
}