{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5gJvyYTIM05C68aKpN7oW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/appliedcode/mthree-c422/blob/mthree-c422-Prashanth/Text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "_0VlbQVA80_g",
        "outputId": "4af9b12b-16b7-432e-b040-adbae36acd3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4062650802.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextPreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'processed_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;31m# 3. Basic Text Classification Pipeline (Naive Bayes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4922\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4923\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4924\u001b[0;31m         ).apply()\n\u001b[0m\u001b[1;32m   4925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4926\u001b[0m     def _reindex_indexer(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0;31m# self.func is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1427\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;31m#  Categorical (GH51645).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m         mapped = obj._map_values(\n\u001b[0m\u001b[1;32m   1508\u001b[0m             \u001b[0mmapper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurried\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mna_action\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m         return lib.map_infer_mask(\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4062650802.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_words\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "# Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                             classification_report, confusion_matrix, roc_curve, auc)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# 1. Create Sample Movie Reviews Dataset\n",
        "def create_movie_review_dataset():\n",
        "    # Positive reviews\n",
        "    positive_reviews = [\n",
        "        \"I absolutely loved this film! The story was compelling and the acting superb.\",\n",
        "        \"An amazing cinematic experience with brilliant performances.\",\n",
        "        \"A heartwarming story that touched me deeply.\",\n",
        "        \"Fantastic direction and stunning visuals. Highly recommended!\",\n",
        "        \"Top-notch screenplay and incredible soundtrack.\"\n",
        "    ]\n",
        "    # Negative reviews\n",
        "    negative_reviews = [\n",
        "        \"The movie was boring and slow-paced; I wouldn‚Äôt recommend it.\",\n",
        "        \"Terrible acting and a predictable script ruined it for me.\",\n",
        "        \"Waste of time. The plot made no sense at all.\",\n",
        "        \"Poor editing and bad special effects spoiled the experience.\",\n",
        "        \"One of the worst movies I have ever watched.\"\n",
        "    ]\n",
        "    # Neutral reviews\n",
        "    neutral_reviews = [\n",
        "        \"It was an okay movie ‚Äî nothing special but watchable.\",\n",
        "        \"The storyline was average and the characters were decent.\",\n",
        "        \"Some parts were entertaining, others quite dull.\",\n",
        "        \"Not great, not terrible, just an average film experience.\",\n",
        "        \"A middling movie, not memorable but not horrible either.\"\n",
        "    ]\n",
        "    texts = positive_reviews + negative_reviews + neutral_reviews\n",
        "    labels = (['Positive'] * len(positive_reviews) +\n",
        "              ['Negative'] * len(negative_reviews) +\n",
        "              ['Neutral'] * len(neutral_reviews))\n",
        "    return pd.DataFrame({'text': texts, 'sentiment': labels})\n",
        "\n",
        "df = create_movie_review_dataset()\n",
        "\n",
        "# 2. Text Preprocessing Pipeline\n",
        "class TextPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'http\\S+', '', text)\n",
        "        text = re.sub(r'\\S+@\\S+', '', text)\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "        text = text.strip()\n",
        "        return text\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        text = self.clean_text(text)\n",
        "        tokens = word_tokenize(text)\n",
        "        tokens = [t for t in tokens if t not in self.stop_words and len(t) > 2]\n",
        "        tokens = [self.lemmatizer.lemmatize(t) for t in tokens]\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "preprocessor = TextPreprocessor()\n",
        "df['processed_text'] = df['text'].apply(preprocessor.preprocess)\n",
        "\n",
        "# 3. Basic Text Classification Pipeline (Naive Bayes)\n",
        "\n",
        "# Vectorize text\n",
        "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1,2))\n",
        "X = vectorizer.fit_transform(df['processed_text'])\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(df['sentiment'])\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Train Naive Bayes\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = nb_model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"=== Naive Bayes Classification Report ===\")\n",
        "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize Confusion Matrix\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - Naive Bayes')\n",
        "plt.show()\n",
        "\n",
        "# 4. Model Comparison Framework with Cross-Validation\n",
        "\n",
        "models = {\n",
        "    'Naive Bayes': MultinomialNB(),\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
        "    'SVM (Linear)': SVC(kernel='linear', probability=True),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier()\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining and cross-validating {name} ...\")\n",
        "    pipeline = Pipeline([\n",
        "        ('vect', TfidfVectorizer(max_features=1000, ngram_range=(1,2))),\n",
        "        ('clf', model)\n",
        "    ])\n",
        "    scores = cross_val_score(pipeline, df['processed_text'], y, cv=cv, scoring='f1_macro')\n",
        "    print(f\"F1 Macro CV Mean: {scores.mean():.3f} | Std: {scores.std():.3f}\")\n",
        "    results[name] = scores.mean()\n",
        "\n",
        "# Visualize Model Comparison\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(x=list(results.keys()), y=list(results.values()))\n",
        "plt.ylabel('Mean F1 Macro Score (5-fold CV)')\n",
        "plt.title('Model Comparison')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# 5. Advanced Feature Engineering - Comparing Vectorizers\n",
        "\n",
        "vectorizers = {\n",
        "    'CountVectorizer (unigrams)': CountVectorizer(max_features=1000, ngram_range=(1,1)),\n",
        "    'TF-IDF (unigrams)': TfidfVectorizer(max_features=1000, ngram_range=(1,1)),\n",
        "    'TF-IDF (bigrams)': TfidfVectorizer(max_features=1000, ngram_range=(2,2)),\n",
        "    'TF-IDF (unigrams + bigrams)': TfidfVectorizer(max_features=1000, ngram_range=(1,2)),\n",
        "    'TF-IDF (char 3-5 grams)': TfidfVectorizer(max_features=1000, analyzer='char', ngram_range=(3,5))\n",
        "}\n",
        "\n",
        "feature_results = {}\n",
        "model_fe = LogisticRegression(max_iter=1000)\n",
        "for name, vect in vectorizers.items():\n",
        "    X_vect = vect.fit_transform(df['processed_text'])\n",
        "    scores = cross_val_score(model_fe, X_vect, y, cv=cv, scoring='f1_macro')\n",
        "    print(f\"{name}: F1 Macro CV Mean={scores.mean():.3f}\")\n",
        "    feature_results[name] = scores.mean()\n",
        "\n",
        "# Plot Feature Engineering Results\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=list(feature_results.keys()), y=list(feature_results.values()))\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('Mean F1 Macro (Cross-Validation)')\n",
        "plt.title('Feature Extraction Method Comparison')\n",
        "plt.show()\n",
        "\n",
        "# 6. Sentiment Classification with Prediction Confidence (Logistic Regression)\n",
        "\n",
        "pipeline_lr = Pipeline([\n",
        "    ('vect', TfidfVectorizer(max_features=1000, ngram_range=(1,2))),\n",
        "    ('clf', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "pipeline_lr.fit(df['processed_text'], y)\n",
        "y_proba = pipeline_lr.predict_proba(df['processed_text'])\n",
        "max_confidence = np.max(y_proba, axis=1)\n",
        "\n",
        "# Confidence Distribution Plot\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.hist(max_confidence, bins=20, alpha=0.7)\n",
        "plt.xlabel('Prediction Confidence')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Prediction Confidence Distribution (Logistic Regression)')\n",
        "plt.show()\n",
        "\n",
        "# Predict with confidence function\n",
        "def predict_with_confidence(text, pipeline, label_encoder):\n",
        "    text_proc = preprocessor.preprocess(text)\n",
        "    probs = pipeline.predict_proba([text_proc])[0]\n",
        "    pred_idx = np.argmax(probs)\n",
        "    pred_label = label_encoder.inverse_transform([pred_idx])[0]\n",
        "    confidence = probs[pred_idx]\n",
        "    return pred_label, confidence\n",
        "\n",
        "# Test on new reviews\n",
        "test_texts = [\n",
        "    \"What a fantastic movie! Loved every minute of it.\",\n",
        "    \"It was dull and uninteresting. Not worth watching.\",\n",
        "    \"Some scenes were good, but overall just okay.\"\n",
        "]\n",
        "\n",
        "for txt in test_texts:\n",
        "    pred, conf = predict_with_confidence(txt, pipeline_lr, le)\n",
        "    print(f\"Review: {txt}\\nPredicted Sentiment: {pred}, Confidence: {conf:.3f}\\n\")\n",
        "\n",
        "# 7. Hyperparameter Tuning Example with GridSearchCV (Logistic Regression)\n",
        "\n",
        "param_grid = {\n",
        "    'clf__C': [0.01, 0.1, 1, 10],\n",
        "    'clf__penalty': ['l2'],\n",
        "    'clf__solver': ['lbfgs']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipeline_lr, param_grid, cv=cv, scoring='f1_macro', n_jobs=-1)\n",
        "grid_search.fit(df['processed_text'], y)\n",
        "\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(f\"Best cross-validated F1 Macro: {grid_search.best_score_:.3f}\")\n",
        "\n",
        "# Use best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Done! You now have a full working pipeline for movie reviews sentiment classification.\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Core libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Text processing\n",
        "import re\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Classification models\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report, roc_auc_score,\n",
        "    roc_curve, precision_recall_curve\n",
        ")\n",
        "\n",
        "# NLP libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download required NLTK data\n",
        "import ssl\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "# Set style for plots\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n",
        "print(f\"üêç NumPy version: {np.__version__}\")\n",
        "print(f\"üêº Pandas version: {pd.__version__}\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSSt8D5O887a",
        "outputId": "4fdeb341-a0d2-4e30-a9eb-a07dcfead456"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All libraries imported successfully!\n",
            "üêç NumPy version: 2.0.2\n",
            "üêº Pandas version: 2.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_news_dataset():\n",
        "    \"\"\"Create a sample news classification dataset\"\"\"\n",
        "\n",
        "    # Technology news\n",
        "    tech_news = [\n",
        "        \"New AI breakthrough enables faster machine learning algorithms\",\n",
        "        \"Tech giant releases latest smartphone with advanced camera features\",\n",
        "        \"Cybersecurity experts warn about new malware threats\",\n",
        "        \"Cloud computing adoption accelerates across enterprises\",\n",
        "        \"Quantum computing research shows promising results\",\n",
        "        \"Software update fixes critical security vulnerabilities\",\n",
        "        \"Virtual reality technology transforms gaming industry\",\n",
        "        \"Internet of Things devices increase smart home adoption\",\n",
        "        \"Blockchain technology revolutionizes financial transactions\",\n",
        "        \"Artificial intelligence improves medical diagnosis accuracy\"\n",
        "    ]\n",
        "\n",
        "    # Sports news\n",
        "    sports_news = [\n",
        "        \"Championship game ends with dramatic overtime victory\",\n",
        "        \"Star athlete breaks long-standing world record\",\n",
        "        \"Team trades popular player to rival franchise\",\n",
        "        \"Olympic preparations continue despite venue challenges\",\n",
        "        \"Coach announces retirement after successful career\",\n",
        "        \"Rookie player shows exceptional talent in debut season\",\n",
        "        \"Stadium renovations completed before season opener\",\n",
        "        \"League implements new rules to improve player safety\",\n",
        "        \"International tournament draws record viewership\",\n",
        "        \"Injury forces veteran player to miss championship\"\n",
        "    ]\n",
        "\n",
        "    # Business news\n",
        "    business_news = [\n",
        "        \"Stock market reaches all-time high amid economic optimism\",\n",
        "        \"Major corporation announces significant layoffs\",\n",
        "        \"Startup secures massive funding round from investors\",\n",
        "        \"Merger creates industry-leading company\",\n",
        "        \"Economic indicators suggest potential recession\",\n",
        "        \"Company reports record quarterly profits\",\n",
        "        \"Trade negotiations impact global supply chains\",\n",
        "        \"Central bank adjusts interest rates\",\n",
        "        \"Retail sales decline during holiday season\",\n",
        "        \"Cryptocurrency market experiences volatile trading\"\n",
        "    ]\n",
        "\n",
        "    # Health news\n",
        "    health_news = [\n",
        "        \"New vaccine shows high effectiveness in clinical trials\",\n",
        "        \"Medical breakthrough offers hope for cancer patients\",\n",
        "        \"Health officials recommend updated safety guidelines\",\n",
        "        \"Research reveals benefits of Mediterranean diet\",\n",
        "        \"Mental health awareness campaign launches nationwide\",\n",
        "        \"Gene therapy treatment approved for rare disease\",\n",
        "        \"Exercise study shows surprising cardiovascular benefits\",\n",
        "        \"Pharmaceutical company recalls contaminated medication\",\n",
        "        \"Telemedicine adoption grows in rural communities\",\n",
        "        \"Sleep disorder research identifies new treatment options\"\n",
        "    ]\n",
        "\n",
        "    # Combine all categories\n",
        "    texts = tech_news + sports_news + business_news + health_news\n",
        "    labels = (['Technology'] * len(tech_news) +\n",
        "             ['Sports'] * len(sports_news) +\n",
        "             ['Business'] * len(business_news) +\n",
        "             ['Health'] * len(health_news))\n",
        "\n",
        "    return pd.DataFrame({'text': texts, 'category': labels})\n",
        "\n",
        "def create_sentiment_dataset():\n",
        "    \"\"\"Create a sample sentiment classification dataset\"\"\"\n",
        "\n",
        "    positive_reviews = [\n",
        "        \"This product is absolutely amazing! Highly recommend it.\",\n",
        "        \"Outstanding quality and excellent customer service.\",\n",
        "        \"Best purchase I've made this year. Love it!\",\n",
        "        \"Incredible value for money. Very satisfied.\",\n",
        "        \"Perfect product, fast shipping, great experience.\",\n",
        "        \"Exceeded my expectations in every way possible.\",\n",
        "        \"Fantastic build quality and beautiful design.\",\n",
        "        \"Works perfectly and arrived ahead of schedule.\",\n",
        "        \"Brilliant product with innovative features.\",\n",
        "        \"Absolutely delighted with this purchase.\"\n",
        "    ]\n",
        "\n",
        "    negative_reviews = [\n",
        "        \"Terrible product, completely waste of money.\",\n",
        "        \"Poor quality and horrible customer support.\",\n",
        "        \"Worst purchase ever. Do not recommend.\",\n",
        "        \"Overpriced and underdelivered. Very disappointed.\",\n",
        "        \"Product broke after one week. Awful quality.\",\n",
        "        \"Completely useless and poorly designed.\",\n",
        "        \"Cheap materials and terrible build quality.\",\n",
        "        \"Doesn't work as advertised. Very frustrating.\",\n",
        "        \"Defective product with no customer support.\",\n",
        "        \"Regret buying this. Save your money.\"\n",
        "    ]\n",
        "\n",
        "    neutral_reviews = [\n",
        "        \"Product is okay, nothing special but works fine.\",\n",
        "        \"Average quality for the price point.\",\n",
        "        \"It's decent but could be better.\",\n",
        "        \"Standard product with basic features.\",\n",
        "        \"Neither good nor bad, just mediocre.\",\n",
        "        \"Works as expected, no surprises.\",\n",
        "        \"Reasonable quality for a budget option.\",\n",
        "        \"It's fine but not outstanding.\",\n",
        "        \"Acceptable product with room for improvement.\",\n",
        "        \"Does the job but nothing impressive.\"\n",
        "    ]\n",
        "\n",
        "    texts = positive_reviews + negative_reviews + neutral_reviews\n",
        "    labels = (['Positive'] * len(positive_reviews) +\n",
        "             ['Negative'] * len(negative_reviews) +\n",
        "             ['Neutral'] * len(neutral_reviews))\n",
        "\n",
        "    return pd.DataFrame({'text': texts, 'sentiment': labels})\n",
        "\n",
        "# Create datasets\n",
        "news_df = create_news_dataset()\n",
        "sentiment_df = create_sentiment_dataset()\n",
        "\n",
        "print(\"üì∞ News Dataset:\")\n",
        "print(f\"   Total samples: {len(news_df)}\")\n",
        "print(f\"   Categories: {news_df['category'].value_counts().to_dict()}\")\n",
        "\n",
        "print(\"\\nüòä Sentiment Dataset:\")\n",
        "print(f\"   Total samples: {len(sentiment_df)}\")\n",
        "print(f\"   Sentiments: {sentiment_df['sentiment'].value_counts().to_dict()}\")\n",
        "\n",
        "# Display sample data\n",
        "print(\"\\nüìã Sample News Data:\")\n",
        "display(news_df.head())\n",
        "\n",
        "print(\"\\nüìã Sample Sentiment Data:\")\n",
        "display(sentiment_df.head())\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "id": "k6ZhYJBM9IHv",
        "outputId": "09d2d036-4175-4f7e-c62f-67c6f50cc1a4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì∞ News Dataset:\n",
            "   Total samples: 40\n",
            "   Categories: {'Technology': 10, 'Sports': 10, 'Business': 10, 'Health': 10}\n",
            "\n",
            "üòä Sentiment Dataset:\n",
            "   Total samples: 30\n",
            "   Sentiments: {'Positive': 10, 'Negative': 10, 'Neutral': 10}\n",
            "\n",
            "üìã Sample News Data:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                text    category\n",
              "0  New AI breakthrough enables faster machine lea...  Technology\n",
              "1  Tech giant releases latest smartphone with adv...  Technology\n",
              "2  Cybersecurity experts warn about new malware t...  Technology\n",
              "3  Cloud computing adoption accelerates across en...  Technology\n",
              "4  Quantum computing research shows promising res...  Technology"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b4ce9f03-2d40-44a6-a834-a8f1c90cad5e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>New AI breakthrough enables faster machine lea...</td>\n",
              "      <td>Technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Tech giant releases latest smartphone with adv...</td>\n",
              "      <td>Technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Cybersecurity experts warn about new malware t...</td>\n",
              "      <td>Technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Cloud computing adoption accelerates across en...</td>\n",
              "      <td>Technology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Quantum computing research shows promising res...</td>\n",
              "      <td>Technology</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b4ce9f03-2d40-44a6-a834-a8f1c90cad5e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b4ce9f03-2d40-44a6-a834-a8f1c90cad5e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b4ce9f03-2d40-44a6-a834-a8f1c90cad5e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-2a63bc45-2632-4baa-b38f-b515d183dcfd\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2a63bc45-2632-4baa-b38f-b515d183dcfd')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-2a63bc45-2632-4baa-b38f-b515d183dcfd button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(sentiment_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Tech giant releases latest smartphone with advanced camera features\",\n          \"Quantum computing research shows promising results\",\n          \"Cybersecurity experts warn about new malware threats\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Technology\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìã Sample Sentiment Data:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                text sentiment\n",
              "0  This product is absolutely amazing! Highly rec...  Positive\n",
              "1  Outstanding quality and excellent customer ser...  Positive\n",
              "2        Best purchase I've made this year. Love it!  Positive\n",
              "3        Incredible value for money. Very satisfied.  Positive\n",
              "4  Perfect product, fast shipping, great experience.  Positive"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-24bd51a0-8ff8-4aea-802e-dbc0dab4bda7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This product is absolutely amazing! Highly rec...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Outstanding quality and excellent customer ser...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Best purchase I've made this year. Love it!</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Incredible value for money. Very satisfied.</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Perfect product, fast shipping, great experience.</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-24bd51a0-8ff8-4aea-802e-dbc0dab4bda7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-24bd51a0-8ff8-4aea-802e-dbc0dab4bda7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-24bd51a0-8ff8-4aea-802e-dbc0dab4bda7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-55fbfbbf-5117-471f-ae2d-a87024c864f8\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-55fbfbbf-5117-471f-ae2d-a87024c864f8')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-55fbfbbf-5117-471f-ae2d-a87024c864f8 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(sentiment_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Outstanding quality and excellent customer service.\",\n          \"Perfect product, fast shipping, great experience.\",\n          \"Best purchase I've made this year. Love it!\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextPreprocessor:\n",
        "    \"\"\"Comprehensive text preprocessing pipeline for classification\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Basic text cleaning\"\"\"\n",
        "        if pd.isna(text):\n",
        "            return \"\"\n",
        "\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove URLs\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "        # Remove email addresses\n",
        "        text = re.sub(r'\\S+@\\S+', '', text)\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def remove_punctuation(self, text):\n",
        "        \"\"\"Remove punctuation while preserving sentence structure\"\"\"\n",
        "        # Keep periods, exclamation marks, question marks for sentence boundaries\n",
        "        punctuation_to_remove = string.punctuation.replace('.', '').replace('!', '').replace('?', '')\n",
        "        text = text.translate(str.maketrans('', '', punctuation_to_remove))\n",
        "        return text\n",
        "\n",
        "    def tokenize_and_filter(self, text, remove_stopwords=True, apply_stemming=False, apply_lemmatization=True):\n",
        "        \"\"\"Tokenize text and apply filtering options\"\"\"\n",
        "        # Tokenize\n",
        "        tokens = word_tokenize(text)\n",
        "\n",
        "        # Filter out short tokens and numbers\n",
        "        tokens = [token for token in tokens if len(token) > 2 and not token.isdigit()]\n",
        "\n",
        "        # Remove stopwords\n",
        "        if remove_stopwords:\n",
        "            tokens = [token for token in tokens if token not in self.stop_words]\n",
        "\n",
        "        # Apply stemming\n",
        "        if apply_stemming:\n",
        "            tokens = [self.stemmer.stem(token) for token in tokens]\n",
        "\n",
        "        # Apply lemmatization\n",
        "        if apply_lemmatization:\n",
        "            tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def preprocess_pipeline(self, text, remove_stopwords=True, apply_stemming=False, apply_lemmatization=True):\n",
        "        \"\"\"Complete preprocessing pipeline\"\"\"\n",
        "        # Clean text\n",
        "        text = self.clean_text(text)\n",
        "\n",
        "        # Remove punctuation\n",
        "        text = self.remove_punctuation(text)\n",
        "\n",
        "        # Tokenize and filter\n",
        "        tokens = self.tokenize_and_filter(\n",
        "            text,\n",
        "            remove_stopwords=remove_stopwords,\n",
        "            apply_stemming=apply_stemming,\n",
        "            apply_lemmatization=apply_lemmatization\n",
        "        )\n",
        "\n",
        "        # Join tokens back to text\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    def preprocess_dataframe(self, df, text_column, **kwargs):\n",
        "        \"\"\"Apply preprocessing to entire dataframe\"\"\"\n",
        "        df_processed = df.copy()\n",
        "        df_processed[f'{text_column}_processed'] = df_processed[text_column].apply(\n",
        "            lambda x: self.preprocess_pipeline(x, **kwargs)\n",
        "        )\n",
        "        return df_processed\n",
        "\n",
        "# Initialize preprocessor\n",
        "preprocessor = TextPreprocessor()\n",
        "\n",
        "# Test preprocessing\n",
        "sample_text = \"This is a GREAT product!!! I highly recommend it. Visit https://example.com for more info.\"\n",
        "print(\"üìù Original text:\")\n",
        "print(f\"   {sample_text}\")\n",
        "\n",
        "print(\"\\nüîß Preprocessed text:\")\n",
        "processed_text = preprocessor.preprocess_pipeline(sample_text)\n",
        "print(f\"   {processed_text}\")\n",
        "\n",
        "print(\"\\n‚úÖ Text preprocessing functions ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKfQhvGS9NSQ",
        "outputId": "40a4f444-74c0-4994-91bc-df17421f8fee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù Original text:\n",
            "   This is a GREAT product!!! I highly recommend it. Visit https://example.com for more info.\n",
            "\n",
            "üîß Preprocessed text:\n",
            "   great product highly recommend visit info\n",
            "\n",
            "‚úÖ Text preprocessing functions ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement basic text classification pipeline\n",
        "\n",
        "def basic_classification_pipeline(df, text_column, label_column, test_size=0.2, random_state=42):\n",
        "    \"\"\"\n",
        "    Complete text classification pipeline\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : DataFrame\n",
        "        Input dataset\n",
        "    text_column : str\n",
        "        Name of text column\n",
        "    label_column : str\n",
        "        Name of label column\n",
        "    test_size : float\n",
        "        Proportion of test set\n",
        "    random_state : int\n",
        "        Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict : Dictionary containing trained model, vectorizer, and evaluation results\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Preprocess text data\n",
        "    print(\"üîß Step 1: Preprocessing text data...\")\n",
        "    # Your code here\n",
        "\n",
        "    # Step 2: Create TF-IDF vectors\n",
        "    print(\"üìä Step 2: Creating TF-IDF vectors...\")\n",
        "    # Your code here\n",
        "\n",
        "    # Step 3: Split data into train/test sets\n",
        "    print(\"üîÄ Step 3: Splitting data...\")\n",
        "    # Your code here\n",
        "\n",
        "    # Step 4: Train Multinomial Naive Bayes classifier\n",
        "    print(\"ü§ñ Step 4: Training classifier...\")\n",
        "    # Your code here\n",
        "\n",
        "    # Step 5: Make predictions\n",
        "    print(\"üéØ Step 5: Making predictions...\")\n",
        "    # Your code here\n",
        "\n",
        "    # Step 6: Evaluate model\n",
        "    print(\"üìà Step 6: Evaluating model...\")\n",
        "    # Your code here\n",
        "\n",
        "    # Return results\n",
        "    return {\n",
        "        'model': None,  # Replace with trained model\n",
        "        'vectorizer': None,  # Replace with fitted vectorizer\n",
        "        'accuracy': 0.0,  # Replace with actual accuracy\n",
        "        'classification_report': None,  # Replace with classification report\n",
        "        'confusion_matrix': None,  # Replace with confusion matrix\n",
        "        'predictions': None,  # Replace with predictions\n",
        "        'test_labels': None  # Replace with test labels\n",
        "    }\n",
        "\n",
        "# Test the pipeline\n",
        "print(\"üèãÔ∏è Testing Basic Classification Pipeline\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "results = basic_classification_pipeline(news_df, 'text', 'category')\n",
        "\n",
        "# Display results\n",
        "print(f\"\\nüìä Model Performance:\")\n",
        "print(f\"   Accuracy: {results['accuracy']:.3f}\")\n",
        "\n",
        "if results['classification_report']:\n",
        "    print(\"\\nüìã Classification Report:\")\n",
        "    print(results['classification_report'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpLoh5dM9S6c",
        "outputId": "dde1d05b-fb4c-4c4b-d66b-e8cca85fd97b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üèãÔ∏è Testing Basic Classification Pipeline\n",
            "==================================================\n",
            "üîß Step 1: Preprocessing text data...\n",
            "üìä Step 2: Creating TF-IDF vectors...\n",
            "üîÄ Step 3: Splitting data...\n",
            "ü§ñ Step 4: Training classifier...\n",
            "üéØ Step 5: Making predictions...\n",
            "üìà Step 6: Evaluating model...\n",
            "\n",
            "üìä Model Performance:\n",
            "   Accuracy: 0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement model comparison framework\n",
        "\n",
        "class ModelComparator:\n",
        "    \"\"\"Compare multiple classification models\"\"\"\n",
        "\n",
        "    def __init__(self, random_state=42):\n",
        "        self.random_state = random_state\n",
        "        self.models = {\n",
        "            'Multinomial NB': MultinomialNB(),\n",
        "            'Logistic Regression': LogisticRegression(random_state=random_state, max_iter=1000),\n",
        "            'SVM (Linear)': SVC(kernel='linear', random_state=random_state, probability=True),\n",
        "            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=random_state),\n",
        "            'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5)\n",
        "        }\n",
        "        self.results = {}\n",
        "\n",
        "    def prepare_data(self, df, text_column, label_column, test_size=0.2):\n",
        "        \"\"\"\n",
        "        Prepare data for classification\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : DataFrame\n",
        "            Input dataset\n",
        "        text_column : str\n",
        "            Name of text column\n",
        "        label_column : str\n",
        "            Name of label column\n",
        "        test_size : float\n",
        "            Proportion of test set\n",
        "        \"\"\"\n",
        "        # Your code here - preprocess data and create train/test splits\n",
        "        pass\n",
        "\n",
        "    def train_and_evaluate_models(self, cv_folds=5):\n",
        "        \"\"\"\n",
        "        Train and evaluate all models using cross-validation\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        cv_folds : int\n",
        "            Number of cross-validation folds\n",
        "        \"\"\"\n",
        "        # Your code here - implement model training and evaluation\n",
        "        pass\n",
        "\n",
        "    def get_detailed_metrics(self, model_name):\n",
        "        \"\"\"\n",
        "        Get detailed metrics for a specific model\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        model_name : str\n",
        "            Name of the model\n",
        "        \"\"\"\n",
        "        # Your code here - calculate detailed metrics\n",
        "        pass\n",
        "\n",
        "    def visualize_results(self):\n",
        "        \"\"\"\n",
        "        Create visualizations of model comparison results\n",
        "        \"\"\"\n",
        "        # Your code here - create comparison visualizations\n",
        "        pass\n",
        "\n",
        "    def predict_new_text(self, text, best_model_name=None):\n",
        "        \"\"\"\n",
        "        Predict class for new text using the best model\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        text : str\n",
        "            Text to classify\n",
        "        best_model_name : str\n",
        "            Name of best model to use (if None, auto-select)\n",
        "        \"\"\"\n",
        "        # Your code here - implement prediction for new text\n",
        "        pass\n",
        "\n",
        "# Test the model comparator\n",
        "print(\"ü§ñ Testing Model Comparison Framework\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "comparator = ModelComparator()\n",
        "\n",
        "# Prepare data\n",
        "print(\"üìä Preparing data...\")\n",
        "comparator.prepare_data(news_df, 'text', 'category')\n",
        "\n",
        "# Train and evaluate models\n",
        "print(\"üîÑ Training and evaluating models...\")\n",
        "comparator.train_and_evaluate_models()\n",
        "\n",
        "# Visualize results\n",
        "print(\"üìà Creating visualizations...\")\n",
        "comparator.visualize_results()\n",
        "\n",
        "# Test prediction on new text\n",
        "new_text = \"Scientists develop revolutionary artificial intelligence algorithm for medical diagnosis\"\n",
        "print(f\"\\nüéØ Predicting category for: '{new_text}'\")\n",
        "prediction = comparator.predict_new_text(new_text)\n",
        "print(f\"   Predicted category: {prediction}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znDofsKQ9cs6",
        "outputId": "b84bfa1e-6796-4af5-9517-0e619638dbf3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ Testing Model Comparison Framework\n",
            "==================================================\n",
            "üìä Preparing data...\n",
            "üîÑ Training and evaluating models...\n",
            "üìà Creating visualizations...\n",
            "\n",
            "üéØ Predicting category for: 'Scientists develop revolutionary artificial intelligence algorithm for medical diagnosis'\n",
            "   Predicted category: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureEngineering:\n",
        "    \"\"\"Advanced feature engineering for text classification\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.experiments = {}\n",
        "        self.preprocessor = TextPreprocessor()\n",
        "\n",
        "    def create_vectorizers(self, max_features=1000):\n",
        "        \"\"\"\n",
        "        Create different vectorizers for comparison\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        max_features : int\n",
        "            Maximum number of features to extract\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict : Dictionary of vectorizers\n",
        "        \"\"\"\n",
        "        # Your code here - create various vectorizers\n",
        "        vectorizers = {\n",
        "            'CountVectorizer (unigram)': None,\n",
        "            'CountVectorizer (bigram)': None,\n",
        "            'TF-IDF (unigram)': None,\n",
        "            'TF-IDF (bigram)': None,\n",
        "            'TF-IDF (trigram)': None,\n",
        "            'Character n-grams': None\n",
        "        }\n",
        "\n",
        "        return vectorizers\n",
        "\n",
        "    def experiment_preprocessing(self, df, text_column, label_column):\n",
        "        \"\"\"\n",
        "        Experiment with different preprocessing options\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : DataFrame\n",
        "            Input dataset\n",
        "        text_column : str\n",
        "            Name of text column\n",
        "        label_column : str\n",
        "            Name of label column\n",
        "        \"\"\"\n",
        "        # Your code here - test different preprocessing configurations\n",
        "        preprocessing_configs = [\n",
        "            {'remove_stopwords': True, 'apply_lemmatization': True, 'apply_stemming': False},\n",
        "            {'remove_stopwords': False, 'apply_lemmatization': True, 'apply_stemming': False},\n",
        "            {'remove_stopwords': True, 'apply_lemmatization': False, 'apply_stemming': True},\n",
        "            {'remove_stopwords': True, 'apply_lemmatization': False, 'apply_stemming': False}\n",
        "        ]\n",
        "\n",
        "        # Test each configuration\n",
        "        # Your implementation here\n",
        "        pass\n",
        "\n",
        "    def analyze_feature_importance(self, model, vectorizer, feature_names, top_k=20):\n",
        "        \"\"\"\n",
        "        Analyze feature importance for trained model\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        model : sklearn model\n",
        "            Trained classification model\n",
        "        vectorizer : sklearn vectorizer\n",
        "            Fitted vectorizer\n",
        "        feature_names : array\n",
        "            Feature names from vectorizer\n",
        "        top_k : int\n",
        "            Number of top features to analyze\n",
        "        \"\"\"\n",
        "        # Your code here - analyze and visualize feature importance\n",
        "        pass\n",
        "\n",
        "    def compare_vectorization_methods(self, df, text_column, label_column):\n",
        "        \"\"\"\n",
        "        Compare different vectorization methods\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : DataFrame\n",
        "            Input dataset\n",
        "        text_column : str\n",
        "            Name of text column\n",
        "        label_column : str\n",
        "            Name of label column\n",
        "        \"\"\"\n",
        "        # Your code here - compare vectorization methods\n",
        "        pass\n",
        "\n",
        "    def visualize_experiments(self):\n",
        "        \"\"\"\n",
        "        Create visualizations of feature engineering experiments\n",
        "        \"\"\"\n",
        "        # Your code here - create comprehensive visualizations\n",
        "        pass\n",
        "\n",
        "# Test feature engineering\n",
        "print(\"üî¨ Testing Advanced Feature Engineering\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "feature_engineer = FeatureEngineering()\n",
        "\n",
        "# Test different vectorizers\n",
        "print(\"üìä Comparing vectorization methods...\")\n",
        "feature_engineer.compare_vectorization_methods(news_df, 'text', 'category')\n",
        "\n",
        "# Test preprocessing options\n",
        "print(\"üîß Experimenting with preprocessing...\")\n",
        "feature_engineer.experiment_preprocessing(news_df, 'text', 'category')\n",
        "\n",
        "# Visualize results\n",
        "print(\"üìà Creating visualizations...\")\n",
        "feature_engineer.visualize_experiments()\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d516iOnE9hDV",
        "outputId": "d2f49a88-de05-4f88-8736-6f7ddfcf2945"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî¨ Testing Advanced Feature Engineering\n",
            "==================================================\n",
            "üìä Comparing vectorization methods...\n",
            "üîß Experimenting with preprocessing...\n",
            "üìà Creating visualizations...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement sentiment classification challenge\n",
        "\n",
        "class SentimentClassifier:\n",
        "    \"\"\"Advanced sentiment classification with confidence analysis\"\"\"\n",
        "\n",
        "    def __init__(self, random_state=42):\n",
        "        self.random_state = random_state\n",
        "        self.models = {}\n",
        "        self.vectorizer = None\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.results = {}\n",
        "\n",
        "    def prepare_sentiment_data(self, df, text_column, label_column, test_size=0.2):\n",
        "        \"\"\"\n",
        "        Prepare sentiment data for classification\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : DataFrame\n",
        "            Input sentiment dataset\n",
        "        text_column : str\n",
        "            Name of text column\n",
        "        label_column : str\n",
        "            Name of sentiment label column\n",
        "        test_size : float\n",
        "            Proportion of test set\n",
        "        \"\"\"\n",
        "        # Your code here - prepare sentiment data\n",
        "        pass\n",
        "\n",
        "    def train_sentiment_models(self):\n",
        "        \"\"\"\n",
        "        Train multiple models for sentiment classification\n",
        "        \"\"\"\n",
        "        # Your code here - train various models\n",
        "        self.models = {\n",
        "            'Naive Bayes': MultinomialNB(),\n",
        "            'Logistic Regression': LogisticRegression(random_state=self.random_state),\n",
        "            'SVM': SVC(probability=True, random_state=self.random_state)\n",
        "        }\n",
        "\n",
        "        # Train each model\n",
        "        # Your implementation here\n",
        "        pass\n",
        "\n",
        "    def evaluate_with_roc_curves(self):\n",
        "        \"\"\"\n",
        "        Evaluate models using ROC curves and AUC scores\n",
        "        \"\"\"\n",
        "        # Your code here - create ROC curves and calculate AUC\n",
        "        pass\n",
        "\n",
        "    def analyze_prediction_confidence(self, model_name):\n",
        "        \"\"\"\n",
        "        Analyze prediction confidence for a specific model\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        model_name : str\n",
        "            Name of the model to analyze\n",
        "        \"\"\"\n",
        "        # Your code here - analyze prediction confidence\n",
        "        pass\n",
        "\n",
        "    def predict_sentiment_with_confidence(self, text, model_name='Logistic Regression'):\n",
        "        \"\"\"\n",
        "        Predict sentiment with confidence score\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        text : str\n",
        "            Text to analyze\n",
        "        model_name : str\n",
        "            Model to use for prediction\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict : Prediction results with confidence\n",
        "        \"\"\"\n",
        "        # Your code here - implement prediction with confidence\n",
        "        return {\n",
        "            'text': text,\n",
        "            'predicted_sentiment': None,\n",
        "            'confidence': 0.0,\n",
        "            'probabilities': {}\n",
        "        }\n",
        "\n",
        "    def create_confusion_matrices(self):\n",
        "        \"\"\"\n",
        "        Create confusion matrices for all models\n",
        "        \"\"\"\n",
        "        # Your code here - create and visualize confusion matrices\n",
        "        pass\n",
        "\n",
        "# Test sentiment classifier\n",
        "print(\"üòä Testing Sentiment Classification Challenge\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "sentiment_classifier = SentimentClassifier()\n",
        "\n",
        "# Prepare data\n",
        "print(\"üìä Preparing sentiment data...\")\n",
        "sentiment_classifier.prepare_sentiment_data(sentiment_df, 'text', 'sentiment')\n",
        "\n",
        "# Train models\n",
        "print(\"ü§ñ Training sentiment models...\")\n",
        "sentiment_classifier.train_sentiment_models()\n",
        "\n",
        "# Evaluate with ROC curves\n",
        "print(\"üìà Creating ROC curves...\")\n",
        "sentiment_classifier.evaluate_with_roc_curves()\n",
        "\n",
        "# Analyze confidence\n",
        "print(\"üéØ Analyzing prediction confidence...\")\n",
        "sentiment_classifier.analyze_prediction_confidence('Logistic Regression')\n",
        "\n",
        "# Test custom examples\n",
        "test_examples = [\n",
        "    \"This product is absolutely fantastic! Best purchase ever!\",\n",
        "    \"Terrible quality, waste of money. Very disappointed.\",\n",
        "    \"It's okay, nothing special but works fine.\"\n",
        "]\n",
        "\n",
        "print(\"\\nüß™ Testing custom examples:\")\n",
        "for example in test_examples:\n",
        "    result = sentiment_classifier.predict_sentiment_with_confidence(example)\n",
        "    print(f\"   Text: '{example[:50]}...'\")\n",
        "    print(f\"   Prediction: {result['predicted_sentiment']} (confidence: {result['confidence']:.3f})\")\n",
        "\n",
        "# Create confusion matrices\n",
        "print(\"\\nüìä Creating confusion matrices...\")\n",
        "sentiment_classifier.create_confusion_matrices()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcnKeOEg9lDG",
        "outputId": "57459c10-a5e8-4ca8-ba1b-ed8914ea95ce"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üòä Testing Sentiment Classification Challenge\n",
            "==================================================\n",
            "üìä Preparing sentiment data...\n",
            "ü§ñ Training sentiment models...\n",
            "üìà Creating ROC curves...\n",
            "üéØ Analyzing prediction confidence...\n",
            "\n",
            "üß™ Testing custom examples:\n",
            "   Text: 'This product is absolutely fantastic! Best purchas...'\n",
            "   Prediction: None (confidence: 0.000)\n",
            "   Text: 'Terrible quality, waste of money. Very disappointe...'\n",
            "   Prediction: None (confidence: 0.000)\n",
            "   Text: 'It's okay, nothing special but works fine....'\n",
            "   Prediction: None (confidence: 0.000)\n",
            "\n",
            "üìä Creating confusion matrices...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement cross-validation and model selection\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "class ModelSelector:\n",
        "    \"\"\"Comprehensive model selection with cross-validation\"\"\"\n",
        "\n",
        "    def __init__(self, cv_folds=5, random_state=42):\n",
        "        self.cv_folds = cv_folds\n",
        "        self.random_state = random_state\n",
        "        self.cv_strategy = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)\n",
        "        self.pipelines = {}\n",
        "        self.grid_search_results = {}\n",
        "        self.cv_results = {}\n",
        "\n",
        "    def create_pipelines(self):\n",
        "        \"\"\"\n",
        "        Create sklearn pipelines for different models\n",
        "        \"\"\"\n",
        "        # Your code here - create pipelines with preprocessing and models\n",
        "        self.pipelines = {\n",
        "            'nb_pipeline': None,  # Naive Bayes pipeline\n",
        "            'lr_pipeline': None,  # Logistic Regression pipeline\n",
        "            'svm_pipeline': None,  # SVM pipeline\n",
        "            'rf_pipeline': None   # Random Forest pipeline\n",
        "        }\n",
        "        pass\n",
        "\n",
        "    def define_hyperparameter_grids(self):\n",
        "        \"\"\"\n",
        "        Define hyperparameter grids for grid search\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict : Hyperparameter grids for each model\n",
        "        \"\"\"\n",
        "        # Your code here - define hyperparameter grids\n",
        "        param_grids = {\n",
        "            'nb_pipeline': {\n",
        "                # Naive Bayes parameters\n",
        "            },\n",
        "            'lr_pipeline': {\n",
        "                # Logistic Regression parameters\n",
        "            },\n",
        "            'svm_pipeline': {\n",
        "                # SVM parameters\n",
        "            },\n",
        "            'rf_pipeline': {\n",
        "                # Random Forest parameters\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return param_grids\n",
        "\n",
        "    def perform_cross_validation(self, X, y, scoring=['accuracy', 'f1_macro', 'precision_macro', 'recall_macro']):\n",
        "        \"\"\"\n",
        "        Perform cross-validation for all models\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array-like\n",
        "            Feature matrix\n",
        "        y : array-like\n",
        "            Target labels\n",
        "        scoring : list\n",
        "            List of scoring metrics\n",
        "        \"\"\"\n",
        "        # Your code here - perform cross-validation\n",
        "        pass\n",
        "\n",
        "    def hyperparameter_tuning(self, X, y, scoring='f1_macro'):\n",
        "        \"\"\"\n",
        "        Perform hyperparameter tuning using grid search\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array-like\n",
        "            Feature matrix\n",
        "        y : array-like\n",
        "            Target labels\n",
        "        scoring : str\n",
        "            Scoring metric for optimization\n",
        "        \"\"\"\n",
        "        # Your code here - perform grid search\n",
        "        pass\n",
        "\n",
        "    def create_evaluation_report(self):\n",
        "        \"\"\"\n",
        "        Create comprehensive evaluation report\n",
        "        \"\"\"\n",
        "        # Your code here - create detailed evaluation report\n",
        "        pass\n",
        "\n",
        "    def visualize_cv_results(self):\n",
        "        \"\"\"\n",
        "        Visualize cross-validation results\n",
        "        \"\"\"\n",
        "        # Your code here - create cross-validation visualizations\n",
        "        pass\n",
        "\n",
        "    def get_best_model(self, metric='f1_macro'):\n",
        "        \"\"\"\n",
        "        Get the best performing model based on specified metric\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        metric : str\n",
        "            Metric to use for model selection\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict : Best model information\n",
        "        \"\"\"\n",
        "        # Your code here - select best model\n",
        "        return {\n",
        "            'model_name': None,\n",
        "            'best_params': {},\n",
        "            'best_score': 0.0,\n",
        "            'model': None\n",
        "        }\n",
        "\n",
        "# Test model selection\n",
        "print(\"üéØ Testing Cross-Validation and Model Selection\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Prepare data for model selection\n",
        "print(\"üìä Preparing data...\")\n",
        "# Use news dataset for this example\n",
        "df_processed = preprocessor.preprocess_dataframe(news_df, 'text')\n",
        "X_text = df_processed['text_processed']\n",
        "y = df_processed['category']\n",
        "\n",
        "# Initialize model selector\n",
        "model_selector = ModelSelector(cv_folds=5)\n",
        "\n",
        "# Create pipelines\n",
        "print(\"üîß Creating pipelines...\")\n",
        "model_selector.create_pipelines()\n",
        "\n",
        "# Perform cross-validation\n",
        "print(\"üîÑ Performing cross-validation...\")\n",
        "model_selector.perform_cross_validation(X_text, y)\n",
        "\n",
        "# Hyperparameter tuning\n",
        "print(\"‚öôÔ∏è Hyperparameter tuning...\")\n",
        "model_selector.hyperparameter_tuning(X_text, y)\n",
        "\n",
        "# Create evaluation report\n",
        "print(\"üìã Creating evaluation report...\")\n",
        "model_selector.create_evaluation_report()\n",
        "\n",
        "# Visualize results\n",
        "print(\"üìà Creating visualizations...\")\n",
        "model_selector.visualize_cv_results()\n",
        "\n",
        "# Get best model\n",
        "best_model_info = model_selector.get_best_model()\n",
        "print(f\"\\nüèÜ Best Model: {best_model_info['model_name']}\")\n",
        "print(f\"   Best Score: {best_model_info['best_score']:.3f}\")\n",
        "print(f\"   Best Parameters: {best_model_info['best_params']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QETx4n2y9qDj",
        "outputId": "3c86abcc-2b8e-494b-ad7d-f70b48ffab7d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ Testing Cross-Validation and Model Selection\n",
            "==================================================\n",
            "üìä Preparing data...\n",
            "üîß Creating pipelines...\n",
            "üîÑ Performing cross-validation...\n",
            "‚öôÔ∏è Hyperparameter tuning...\n",
            "üìã Creating evaluation report...\n",
            "üìà Creating visualizations...\n",
            "\n",
            "üèÜ Best Model: None\n",
            "   Best Score: 0.000\n",
            "   Best Parameters: {}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üß™ COMPREHENSIVE TESTING OF TEXT CLASSIFICATION EXERCISES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Test 1: Basic Pipeline\n",
        "print(\"\\n1Ô∏è‚É£ Testing Basic Classification Pipeline\")\n",
        "print(\"-\" * 40)\n",
        "try:\n",
        "    results_basic = basic_classification_pipeline(news_df, 'text', 'category')\n",
        "    if results_basic['accuracy'] > 0:\n",
        "        print(f\"   ‚úÖ Basic pipeline working - Accuracy: {results_basic['accuracy']:.3f}\")\n",
        "    else:\n",
        "        print(\"   ‚ùå Basic pipeline needs implementation\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Error in basic pipeline: {e}\")\n",
        "\n",
        "# Test 2: Model Comparison\n",
        "print(\"\\n2Ô∏è‚É£ Testing Model Comparison Framework\")\n",
        "print(\"-\" * 40)\n",
        "try:\n",
        "    comparator_test = ModelComparator()\n",
        "    comparator_test.prepare_data(news_df, 'text', 'category')\n",
        "    print(\"   ‚úÖ Model comparator initialized successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Error in model comparator: {e}\")\n",
        "\n",
        "# Test 3: Feature Engineering\n",
        "print(\"\\n3Ô∏è‚É£ Testing Feature Engineering\")\n",
        "print(\"-\" * 40)\n",
        "try:\n",
        "    feature_eng_test = FeatureEngineering()\n",
        "    vectorizers = feature_eng_test.create_vectorizers()\n",
        "    print(f\"   ‚úÖ Feature engineering initialized - {len(vectorizers)} vectorizers created\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Error in feature engineering: {e}\")\n",
        "\n",
        "# Test 4: Sentiment Classifier\n",
        "print(\"\\n4Ô∏è‚É£ Testing Sentiment Classifier\")\n",
        "print(\"-\" * 40)\n",
        "try:\n",
        "    sentiment_test = SentimentClassifier()\n",
        "    sentiment_test.prepare_sentiment_data(sentiment_df, 'text', 'sentiment')\n",
        "    print(\"   ‚úÖ Sentiment classifier initialized successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Error in sentiment classifier: {e}\")\n",
        "\n",
        "# Test 5: Model Selection\n",
        "print(\"\\n5Ô∏è‚É£ Testing Model Selection\")\n",
        "print(\"-\" * 40)\n",
        "try:\n",
        "    selector_test = ModelSelector()\n",
        "    selector_test.create_pipelines()\n",
        "    print(\"   ‚úÖ Model selector initialized successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ùå Error in model selector: {e}\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\nüìä TESTING SUMMARY\")\n",
        "print(\"=\" * 30)\n",
        "print(\"\\nüìù Next Steps:\")\n",
        "print(\"   1. Implement the TODO sections in each exercise\")\n",
        "print(\"   2. Test with your own datasets\")\n",
        "print(\"   3. Experiment with different hyperparameters\")\n",
        "print(\"   4. Try advanced preprocessing techniques\")\n",
        "print(\"   5. Compare results with transformer models\")\n",
        "\n",
        "print(\"\\nüéì Learning Outcomes Achieved:\")\n",
        "print(\"   ‚úÖ Text preprocessing for classification\")\n",
        "print(\"   ‚úÖ Multiple classification algorithms\")\n",
        "print(\"   ‚úÖ Model evaluation and comparison\")\n",
        "print(\"   ‚úÖ Feature engineering techniques\")\n",
        "print(\"   ‚úÖ Cross-validation and model selection\")\n",
        "\n",
        "print(\"\\nüöÄ Ready for Session 7: Sentiment Analysis Deep Dive!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXaVpKgG9vzk",
        "outputId": "e5be1db8-613e-49db-9cdb-8cedccfbdd44"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ COMPREHENSIVE TESTING OF TEXT CLASSIFICATION EXERCISES\n",
            "======================================================================\n",
            "\n",
            "1Ô∏è‚É£ Testing Basic Classification Pipeline\n",
            "----------------------------------------\n",
            "üîß Step 1: Preprocessing text data...\n",
            "üìä Step 2: Creating TF-IDF vectors...\n",
            "üîÄ Step 3: Splitting data...\n",
            "ü§ñ Step 4: Training classifier...\n",
            "üéØ Step 5: Making predictions...\n",
            "üìà Step 6: Evaluating model...\n",
            "   ‚ùå Basic pipeline needs implementation\n",
            "\n",
            "2Ô∏è‚É£ Testing Model Comparison Framework\n",
            "----------------------------------------\n",
            "   ‚úÖ Model comparator initialized successfully\n",
            "\n",
            "3Ô∏è‚É£ Testing Feature Engineering\n",
            "----------------------------------------\n",
            "   ‚úÖ Feature engineering initialized - 6 vectorizers created\n",
            "\n",
            "4Ô∏è‚É£ Testing Sentiment Classifier\n",
            "----------------------------------------\n",
            "   ‚úÖ Sentiment classifier initialized successfully\n",
            "\n",
            "5Ô∏è‚É£ Testing Model Selection\n",
            "----------------------------------------\n",
            "   ‚úÖ Model selector initialized successfully\n",
            "\n",
            "üìä TESTING SUMMARY\n",
            "==============================\n",
            "\n",
            "üìù Next Steps:\n",
            "   1. Implement the TODO sections in each exercise\n",
            "   2. Test with your own datasets\n",
            "   3. Experiment with different hyperparameters\n",
            "   4. Try advanced preprocessing techniques\n",
            "   5. Compare results with transformer models\n",
            "\n",
            "üéì Learning Outcomes Achieved:\n",
            "   ‚úÖ Text preprocessing for classification\n",
            "   ‚úÖ Multiple classification algorithms\n",
            "   ‚úÖ Model evaluation and comparison\n",
            "   ‚úÖ Feature engineering techniques\n",
            "   ‚úÖ Cross-validation and model selection\n",
            "\n",
            "üöÄ Ready for Session 7: Sentiment Analysis Deep Dive!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kb5_j67E9zJ8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}