{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/appliedcode/mthree-c422/blob/main/Exercises/day-8/NLP-Concepts-Token/Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring Core NLP Concepts\n",
        "Welcome to this hands-on NLP Colab lab! You will work through key tasks—tokenization, POS tagging, stemming, stop-word filtering, vocabulary matching, lemmatization, dependency parsing, NER, and intent classification—using Python libraries. Follow the instructions and complete the exercises."
      ],
      "metadata": {
        "id": "sDeW9pllWu7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install --upgrade pip setuptools wheel -q\n",
        "!pip install --quiet nltk spacy textblob sklearn\n",
        "\n",
        "# Download NLTK data and spaCy model\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "!python -m spacy download en_core_web_sm -q\n"
      ],
      "metadata": {
        "id": "yMULvewKWrzm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a70566f1-bb7e-4d1f-9c6c-e699df022719"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (25.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (80.9.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Tokenization\n",
        "# Goal: Split text into tokens (words and punctuation).\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "text = \"Natural Language Processing enables machines to understand human language.\"\n",
        "print(\"Sentences:\", sent_tokenize(text))\n",
        "print(\"Tokens:\", word_tokenize(text))"
      ],
      "metadata": {
        "id": "vNgJ7l_9Wzfo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "060bc895-9b23-4801-936f-5d37015f5fc4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences: ['Natural Language Processing enables machines to understand human language.']\n",
            "Tokens: ['Natural', 'Language', 'Processing', 'enables', 'machines', 'to', 'understand', 'human', 'language', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 1.1: Tokenize the following paragraph into words and sentences:\n",
        "\n",
        "paragraph = \"Machine learning models power many NLP tasks. They learn patterns from data!\"\n"
      ],
      "metadata": {
        "id": "Iwanu3kzW3KU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Part-of-Speech Tagging\n",
        "# Goal: Assign grammatical tags to each token.\n",
        "import nltk\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(pos_tags)\n"
      ],
      "metadata": {
        "id": "VaJMjJLOXCV9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c10da4d-53b7-47f0-81a0-cd1673560f91"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('enables', 'VBZ'), ('machines', 'NNS'), ('to', 'TO'), ('understand', 'VB'), ('human', 'JJ'), ('language', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2.1: Tag POS for tokens from your Exercise 1.1."
      ],
      "metadata": {
        "id": "rjdfixgHXLAe"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Stemming\n",
        "# Goal: Reduce words to their root forms (may be non-dictionary).\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "words = [\"running\", \"runs\", \"ran\", \"easily\", \"fairly\"]\n",
        "print({w: stemmer.stem(w) for w in words})\n"
      ],
      "metadata": {
        "id": "8Q_ASJ2qXMJ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ff7e536-b35e-4135-ece3-acb2515bd066"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'running': 'run', 'runs': 'run', 'ran': 'ran', 'easily': 'easili', 'fairly': 'fairli'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3.1: Stem the tokens from your Exercise 1.1.\n",
        "\n"
      ],
      "metadata": {
        "id": "KpJNVgRhXTjF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Stop-Word Filtering\n",
        "# Goal: Remove common, low-value words.\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = word_tokenize(text.lower())\n",
        "filtered = [w for w in tokens if w.isalpha() and w not in stop_words]\n",
        "print(filtered)"
      ],
      "metadata": {
        "id": "kBo5Lo3hXVPk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99ef8fc8-9a54-4df4-c029-bce6b31cd99f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natural', 'language', 'processing', 'enables', 'machines', 'understand', 'human', 'language']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 4.1: Filter stop words from your Exercise 1.1 tokens.\n",
        "\n"
      ],
      "metadata": {
        "id": "_CVnLYVWXakJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Vocabulary Matching\n",
        "# Goal: Check tokens against a predefined vocabulary.\n",
        "\n",
        "vocab = {\"natural\", \"language\", \"machine\", \"data\", \"processing\"}\n",
        "tokens = [w.lower() for w in word_tokenize(text)]\n",
        "in_vocab = [w for w in tokens if w.isalpha() and w in vocab]\n",
        "print(\"In-vocab tokens:\", in_vocab)\n",
        "print(\"OOV tokens:\", [w for w in tokens if w.isalpha() and w not in vocab])\n"
      ],
      "metadata": {
        "id": "IYEs9fy5XcVy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "295b70e2-2bf1-47da-cf4e-540a7896a041"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In-vocab tokens: ['natural', 'language', 'processing', 'language']\n",
            "OOV tokens: ['enables', 'machines', 'to', 'understand', 'human']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 5.1: Define your own small vocabulary and classify tokens from Exercise 1.1 into in-vocab vs. out-of-vocab."
      ],
      "metadata": {
        "id": "YsKmsc4AXgXa"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Lemmatization\n",
        "# Goal: Convert words to their dictionary form.\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [\"running\", \"better\", \"wolves\"]\n",
        "print({w: lemmatizer.lemmatize(w) for w in words})\n",
        "# For verbs:\n",
        "print(\"run (verb):\", lemmatizer.lemmatize(\"running\", pos='v'))\n"
      ],
      "metadata": {
        "id": "RDcsQ-_CXjyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15677613-a0ec-4968-eb40-295e300abf61"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'running': 'running', 'better': 'better', 'wolves': 'wolf'}\n",
            "run (verb): run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 6.1: Lemmatize tokens from Exercise 1.1 (both default and verb POS).\n",
        "\n"
      ],
      "metadata": {
        "id": "g74PrNQ0XqWx"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Dependency Parsing\n",
        "# Goal: Identify syntactic relationships between tokens.\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "    print(token.text, token.dep_, token.head.text)\n"
      ],
      "metadata": {
        "id": "4drXo7zjXss9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3faae0e8-7126-4d1b-b4c3-9da51d733522"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural compound Language\n",
            "Language compound Processing\n",
            "Processing nsubj enables\n",
            "enables ROOT enables\n",
            "machines nsubj understand\n",
            "to aux understand\n",
            "understand ccomp enables\n",
            "human amod language\n",
            "language dobj understand\n",
            ". punct enables\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 7.1: Parse the sentence “They learn patterns from data” and list each token’s dependency label and head."
      ],
      "metadata": {
        "id": "LmRo7ZwEXvGH"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Named-Entity Recognition (NER)\n",
        "# Goal: Extract real-world entities from text.\n",
        "doc = nlp(\"Google was founded in 1998 by Larry Page and Sergey Brin in California.\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "metadata": {
        "id": "k78PeFooXxwp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad0c456e-e27c-49f1-ac30-5dcc6fa168ad"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google ORG\n",
            "1998 DATE\n",
            "Larry Page PERSON\n",
            "Sergey Brin PERSON\n",
            "California GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exercise 8.1: Run NER on this sentence and add at least two more sentences of your own.\n",
        "\n"
      ],
      "metadata": {
        "id": "bwBm0NMJX3WN"
      },
      "execution_count": 26,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}