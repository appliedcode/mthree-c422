{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7OGGLLqrPRql0Xlb1LrWg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/appliedcode/mthree-c422/blob/mthree-c422-Likhitha/Threat_and_Security_Practice_2_md.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6HL3KF4p5ipM"
      },
      "outputs": [],
      "source": [
        "import os, json, math\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
        "    confusion_matrix, classification_report\n",
        ")\n",
        "import shap\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(2025)\n",
        "os.makedirs(\"artifacts\", exist_ok=True)"
      ],
      "metadata": {
        "id": "0bDSpPng50io"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading Pima Indians Diabetes dataset...\")\n",
        "pima = fetch_openml(name=\"diabetes\", version=1, as_frame=True)\n",
        "\n",
        "# Some OpenML versions place target in .target or inside frame. Handle both.\n",
        "if getattr(pima, \"target\", None) is not None:\n",
        "    y_raw = pima.target.copy()\n",
        "    X_raw = pima.data.copy()\n",
        "else:\n",
        "    df_all = pima.frame.copy()\n",
        "    # detect likely target column name\n",
        "    cand = [c for c in df_all.columns if c.lower() in (\"class\", \"target\", \"diabetes\", \"outcome\", \"y\")]\n",
        "    if not cand:\n",
        "        # fallback: assume last column is target\n",
        "        cand = [df_all.columns[-1]]\n",
        "    target_col = cand[0]\n",
        "    y_raw = df_all[target_col].copy()\n",
        "    X_raw = df_all.drop(columns=[target_col]).copy()\n",
        "\n",
        "# Map target to 0/1 robustly\n",
        "if pd.api.types.is_numeric_dtype(y_raw):\n",
        "    y = (y_raw != 0).astype(int)\n",
        "else:\n",
        "    y = y_raw.astype(str).map(lambda s: 1 if \"positive\" in s.lower() or \"tested_positive\" in s.lower() or \"pos\" in s.lower() or s.strip() in (\"1\",\"yes\",\"True\",\"true\") else 0).astype(int)\n",
        "\n",
        "print(\"Dataset shape:\", X_raw.shape)\n",
        "print(\"Class distribution:\\n\", y.value_counts())\n",
        "\n",
        "# Basic cleaning: fill numeric NaNs with median\n",
        "X = X_raw.copy()\n",
        "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "for c in num_cols:\n",
        "    if X[c].isnull().any():\n",
        "        X[c] = X[c].fillna(X[c].median())\n",
        "\n",
        "# Keep a copy for privacy variants\n",
        "X_orig = X.copy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlIIukOz53dT",
        "outputId": "c55b82b6-2860-4a20-b95f-8589d8b0697a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Pima Indians Diabetes dataset...\n",
            "Dataset shape: (768, 8)\n",
            "Class distribution:\n",
            " class\n",
            "0    500\n",
            "1    268\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_orig, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")\n",
        "print(\"Train fraud/diabetes rate:\", float(y_train.mean()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ti_AMzMt58XH",
        "outputId": "d76517a9-d249-4eda-b93a-5dc4929c2466"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 576, Test size: 192\n",
            "Train fraud/diabetes rate: 0.3489583333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_evaluate(clf, X_tr, y_tr, X_te, y_te):\n",
        "    clf.fit(X_tr, y_tr)\n",
        "    y_pred = clf.predict(X_te)\n",
        "    # handle predict_proba safe extraction\n",
        "    y_proba = None\n",
        "    if hasattr(clf, \"predict_proba\"):\n",
        "        proba = clf.predict_proba(X_te)\n",
        "        if proba.ndim == 2 and proba.shape[1] >= 2:\n",
        "            y_proba = proba[:, 1]\n",
        "        else:\n",
        "            # only one column present (single-class training) -> fallback constant proba\n",
        "            y_proba = np.zeros(len(X_te))\n",
        "    # compute metrics, guard roc_auc if single-class in y_test\n",
        "    metrics = {\n",
        "        \"accuracy\": float(accuracy_score(y_te, y_pred)),\n",
        "        \"precision\": float(precision_score(y_te, y_pred, zero_division=0)),\n",
        "        \"recall\": float(recall_score(y_te, y_pred, zero_division=0)),\n",
        "        \"f1\": float(f1_score(y_te, y_pred, zero_division=0)),\n",
        "        \"roc_auc\": float(roc_auc_score(y_te, y_proba)) if (y_proba is not None and len(np.unique(y_te))>1) else None,\n",
        "        \"confusion_matrix\": confusion_matrix(y_te, y_pred).tolist()\n",
        "    }\n",
        "    return metrics, y_pred, y_proba\n"
      ],
      "metadata": {
        "id": "KETKuRqW5_z7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def poison_labels(y_series, fraction=0.03, flip_from=1, flip_to=0, rng_seed=2025):\n",
        "    y_copy = y_series.copy().astype(int)\n",
        "    rng = np.random.default_rng(rng_seed)\n",
        "    candidate_idx = y_copy[y_copy == flip_from].index.to_numpy()\n",
        "    n_poison = max(1, int(math.floor(fraction * len(candidate_idx)))) if len(candidate_idx)>0 else 0\n",
        "    poisoned_idx = rng.choice(candidate_idx, size=n_poison, replace=False) if n_poison>0 else np.array([], dtype=int)\n",
        "    y_copy.loc[poisoned_idx] = flip_to\n",
        "    return y_copy, poisoned_idx\n",
        "\n",
        "poison_frac = 0.03   # flip 3% of positives -> negatives (hiding attacks)\n",
        "y_train_poisoned, poisoned_idx = poison_labels(y_train, fraction=poison_frac)\n",
        "print(f\"Poisoned {len(poisoned_idx)} training positive labels (indices sample):\", poisoned_idx[:5].tolist())\n",
        "\n",
        "# quick check: ensure training still has both classes\n",
        "print(\"Unique labels in y_train_poisoned:\", np.unique(y_train_poisoned, return_counts=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fKAUSmb6Esg",
        "outputId": "212023b0-0916-4a5d-d0d0-e5ed27ecdbec"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Poisoned 6 training positive labels (indices sample): [666, 664, 218, 155, 603]\n",
            "Unique labels in y_train_poisoned: (array([0, 1]), array([381, 195]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf_baseline = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
        "baseline_metrics, baseline_pred, baseline_proba = train_evaluate(clf_baseline, X_train, y_train_poisoned, X_test, y_test)\n",
        "print(\"\\nBaseline metrics (trained on poisoned data):\")\n",
        "print(json.dumps(baseline_metrics, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WzlHS2d6INy",
        "outputId": "3933cbc0-76e7-4334-cbe0-bc22469383ed"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Baseline metrics (trained on poisoned data):\n",
            "{\n",
            "  \"accuracy\": 0.75,\n",
            "  \"precision\": 0.6792452830188679,\n",
            "  \"recall\": 0.5373134328358209,\n",
            "  \"f1\": 0.6,\n",
            "  \"roc_auc\": 0.8104477611940298,\n",
            "  \"confusion_matrix\": [\n",
            "    [\n",
            "      108,\n",
            "      17\n",
            "    ],\n",
            "    [\n",
            "      31,\n",
            "      36\n",
            "    ]\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drop_cols = [c for c in [\"Age\", \"BMI\"] if c in X_train.columns]\n",
        "# Variant A\n",
        "X_train_privA = X_train.drop(columns=drop_cols, errors='ignore')\n",
        "X_test_privA  = X_test.drop(columns=drop_cols, errors='ignore')\n",
        "clf_privA = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
        "privA_metrics, privA_pred, privA_proba = train_evaluate(clf_privA, X_train_privA, y_train_poisoned, X_test_privA, y_test)\n",
        "print(\"\\nPrivacy Variant A (drop Age & BMI) metrics:\")\n",
        "print(json.dumps(privA_metrics, indent=2))\n",
        "\n",
        "# Variant B: noisy Age & BMI in training (small relative noise)\n",
        "X_train_privB = X_train.copy()\n",
        "noise_frac = 0.05\n",
        "for c in drop_cols:\n",
        "    sigma = X_train_privB[c].std() * noise_frac if X_train_privB[c].std() > 0 else 0.0\n",
        "    if sigma > 0:\n",
        "        X_train_privB[c] = X_train_privB[c] + np.random.default_rng(2026).normal(0, sigma, size=len(X_train_privB))\n",
        "clf_privB = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
        "privB_metrics, privB_pred, privB_proba = train_evaluate(clf_privB, X_train_privB, y_train_poisoned, X_test, y_test)\n",
        "print(\"\\nPrivacy Variant B (noisy Age & BMI in training) metrics:\")\n",
        "print(json.dumps(privB_metrics, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6O-ghzZV6L1F",
        "outputId": "d16b77a8-a776-4b2f-954b-5c0e3e900a65"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Privacy Variant A (drop Age & BMI) metrics:\n",
            "{\n",
            "  \"accuracy\": 0.75,\n",
            "  \"precision\": 0.6792452830188679,\n",
            "  \"recall\": 0.5373134328358209,\n",
            "  \"f1\": 0.6,\n",
            "  \"roc_auc\": 0.8104477611940298,\n",
            "  \"confusion_matrix\": [\n",
            "    [\n",
            "      108,\n",
            "      17\n",
            "    ],\n",
            "    [\n",
            "      31,\n",
            "      36\n",
            "    ]\n",
            "  ]\n",
            "}\n",
            "\n",
            "Privacy Variant B (noisy Age & BMI in training) metrics:\n",
            "{\n",
            "  \"accuracy\": 0.75,\n",
            "  \"precision\": 0.6792452830188679,\n",
            "  \"recall\": 0.5373134328358209,\n",
            "  \"f1\": 0.6,\n",
            "  \"roc_auc\": 0.8104477611940298,\n",
            "  \"confusion_matrix\": [\n",
            "    [\n",
            "      108,\n",
            "      17\n",
            "    ],\n",
            "    [\n",
            "      31,\n",
            "      36\n",
            "    ]\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nComputing SHAP explanations (sampled to speed up)...\")\n",
        "sample_size = min(500, len(X_test))\n",
        "sample_idx = np.random.default_rng(2027).choice(X_test.index, size=sample_size, replace=False)\n",
        "X_test_sample = X_test.loc[sample_idx]\n",
        "\n",
        "explainer = shap.TreeExplainer(clf_baseline)\n",
        "shap_values = explainer.shap_values(X_test_sample, check_additivity=False)  # may be list\n",
        "\n",
        "if isinstance(shap_values, list):\n",
        "    shap_for_positive = shap_values[1]\n",
        "else:\n",
        "    shap_for_positive = shap_values\n",
        "\n",
        "# create and save bar summary\n",
        "plt.figure(figsize=(8,4))\n",
        "shap.summary_plot(shap_for_positive, X_test_sample, plot_type=\"bar\", show=False)\n",
        "plt.title(\"SHAP - Top features for positive (diabetes)\")\n",
        "plt.tight_layout()\n",
        "shap_png = \"artifacts/shap_pima_bar.png\"\n",
        "plt.savefig(shap_png, dpi=150)\n",
        "plt.close()\n",
        "print(\"Saved SHAP plot ->\", shap_png)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "WjBZ6oWq6QfC",
        "outputId": "56a2c501-b530-4bfc-8fd5-6e87d6bc1ee8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Computing SHAP explanations (sampled to speed up)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-445211770.py:16: FutureWarning: The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.\n",
            "  shap.summary_plot(shap_for_positive, X_test_sample, plot_type=\"bar\", show=False)\n",
            "/usr/local/lib/python3.11/dist-packages/shap/plots/_beeswarm.py:723: FutureWarning: The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.\n",
            "  summary_legacy(\n",
            "/usr/local/lib/python3.11/dist-packages/shap/plots/_beeswarm.py:743: FutureWarning: The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.\n",
            "  summary_legacy(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved SHAP plot -> artifacts/shap_pima_bar.png\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x400 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shap\n",
        "\n",
        "# Example dataset\n",
        "data = pd.DataFrame({\n",
        "    \"Gender\": [\"Male\", \"Female\", \"Female\", \"Male\", \"Male\"],\n",
        "    \"City\": [\"New York\", \"London\", \"London\", \"Paris\", \"New York\"],\n",
        "    \"Age\": [25, 32, 40, 28, 35],\n",
        "    \"Income\": [0, 1, 1, 0, 1]  # Target\n",
        "})\n",
        "\n",
        "# Separate features and target\n",
        "X = data.drop(columns=[\"Income\"])\n",
        "y = data[\"Income\"]\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "\n",
        "# One-hot encode categorical features\n",
        "encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "X_train_encoded = encoder.fit_transform(X_train)\n",
        "X_test_encoded = encoder.transform(X_test)\n",
        "\n",
        "# Get encoded feature names\n",
        "encoded_feature_names = encoder.get_feature_names_out(X_train.columns)\n",
        "\n",
        "# Train a model (example: Random Forest)\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train_encoded, y_train)\n",
        "\n",
        "# Create SHAP explainer\n",
        "explainer = shap.TreeExplainer(model)\n",
        "shap_values = explainer.shap_values(X_test_encoded)\n",
        "\n",
        "# Get SHAP values for positive class\n",
        "shap_for_positive = shap_values[1] if isinstance(shap_values, list) else shap_values[..., 1]\n",
        "mean_abs_shap = np.abs(shap_for_positive).mean(axis=0)\n",
        "\n",
        "# Create DataFrame of SHAP values\n",
        "shap_df = pd.DataFrame({\n",
        "    \"feature\": encoded_feature_names,\n",
        "    \"mean_abs_shap\": mean_abs_shap\n",
        "}).sort_values(by=\"mean_abs_shap\", ascending=False)\n",
        "\n",
        "print(shap_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zf8n-Tb_6VeB",
        "outputId": "bc0d6b7b-3b7e-450e-ac80-869e2405bc34"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         feature  mean_abs_shap\n",
            "1    Gender_Male       0.079167\n",
            "0  Gender_Female       0.069167\n",
            "2    City_London       0.060000\n",
            "7         Age_40       0.030833\n",
            "5         Age_25       0.026667\n",
            "4     City_Paris       0.026667\n",
            "3  City_New York       0.025000\n",
            "6         Age_28       0.017500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import shap\n",
        "import json\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Example dataset\n",
        "# -------------------------------\n",
        "data = pd.DataFrame({\n",
        "    \"age\": [25, 32, 47, 51, 62, 41, 29, 36, 55, 48],\n",
        "    \"education\": [\"Bachelors\", \"Masters\", \"PhD\", \"Bachelors\", \"PhD\",\n",
        "                  \"Masters\", \"Bachelors\", \"Masters\", \"PhD\", \"Bachelors\"],\n",
        "    \"hours_per_week\": [40, 50, 60, 40, 45, 50, 40, 60, 55, 50],\n",
        "    \"income_gt_50k\": [0, 1, 1, 0, 1, 1, 0, 1, 1, 0]\n",
        "})\n",
        "\n",
        "X = data.drop(\"income_gt_50k\", axis=1)\n",
        "y = data[\"income_gt_50k\"]\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Train-test split\n",
        "# -------------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Encode categorical features\n",
        "# -------------------------------\n",
        "encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "X_train_encoded = encoder.fit_transform(X_train)\n",
        "X_test_encoded = encoder.transform(X_test)\n",
        "encoded_feature_names = encoder.get_feature_names_out(X_train.columns)\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Train model\n",
        "# -------------------------------\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train_encoded, y_train)\n",
        "\n",
        "# -------------------------------\n",
        "# 5. SHAP values\n",
        "# -------------------------------\n",
        "explainer = shap.TreeExplainer(model)\n",
        "shap_values = explainer.shap_values(X_test_encoded)\n",
        "\n",
        "# Handle binary/multi-class\n",
        "shap_for_positive = shap_values[1] if isinstance(shap_values, list) else shap_values[..., 1]\n",
        "mean_abs_shap = np.abs(shap_for_positive).mean(axis=0)\n",
        "\n",
        "shap_df = pd.DataFrame({\n",
        "    \"feature\": encoded_feature_names,\n",
        "    \"mean_abs_shap\": mean_abs_shap\n",
        "})\n",
        "\n",
        "top_shap = shap_df.sort_values(\"mean_abs_shap\", ascending=False).head(10).to_dict(orient=\"records\")\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Governance audit report\n",
        "# -------------------------------\n",
        "audit = {\n",
        "    \"model_type\": \"RandomForestClassifier\",\n",
        "    \"data_shape\": {\n",
        "        \"train_samples\": X_train.shape[0],\n",
        "        \"test_samples\": X_test.shape[0],\n",
        "        \"features\": len(encoded_feature_names)\n",
        "    },\n",
        "    \"top_features_by_shap\": top_shap,\n",
        "    \"shap_summary\": shap_df.to_dict(orient=\"records\")\n",
        "}\n",
        "\n",
        "# Print audit report\n",
        "print(json.dumps(audit, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyl8c-E76c_e",
        "outputId": "8ffa9b18-de55-42bb-8ca1-11d928b3f2e7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"model_type\": \"RandomForestClassifier\",\n",
            "  \"data_shape\": {\n",
            "    \"train_samples\": 7,\n",
            "    \"test_samples\": 3,\n",
            "    \"features\": 14\n",
            "  },\n",
            "  \"top_features_by_shap\": [\n",
            "    {\n",
            "      \"feature\": \"education_Bachelors\",\n",
            "      \"mean_abs_shap\": 0.25778902116402125\n",
            "    },\n",
            "    {\n",
            "      \"feature\": \"hours_per_week_40\",\n",
            "      \"mean_abs_shap\": 0.09071296296296298\n",
            "    },\n",
            "    {\n",
            "      \"feature\": \"education_PhD\",\n",
            "      \"mean_abs_shap\": 0.06007804232804234\n",
            "    },\n",
            "    {\n",
            "      \"feature\": \"hours_per_week_60\",\n",
            "      \"mean_abs_shap\": 0.03220171957671958\n",
            "    },\n",
            "    {\n",
            "      \"feature\": \"hours_per_week_50\",\n",
            "      \"mean_abs_shap\": 0.026285714285714284\n",
            "    },\n",
            "    {\n",
            "      \"feature\": \"education_Masters\",\n",
            "      \"mean_abs_shap\": 0.025363756613756613\n",
            "    },\n",
            "    {\n",
            "      \"feature\": \"hours_per_week_45\",\n",
            "      \"mean_abs_shap\": 0.01783465608465608\n",
            "    },\n",
            "    {\n",
            "      \"feature\": \"age_48\",\n",
            "      \"mean_abs_shap\": 0.013700396825396824\n",
            "    },\n",
            "    {\n",
            "      \"feature\": \"age_36\",\n",
            "      \"mean_abs_shap\": 0.013589947089947088\n",
            "    },\n",
            "    {\n",
            "      \"feature\": \"age_47\",\n",
            "      \"mean_abs_shap\": 0.009455026455026454\n",
            "    }\n",
            "  ],\n",
            "  \"shap_summary\": [\n",
            "    {\n",
            "      \"feature\": \"age_25\",\n",
            "      \"mean_abs_shap\": 0.002731481481481481\n",
            "    },\n",
            "    {\n",
            "      \"feature\": \"age_29\",\n",
            "      \"mean_abs_shap\": 0.0036150793650793645\n",
            "    },\n",
            "    {\n",
            "      \"feature\": \"age_36\",\n",
            "      \"mean_abs_shap\": 0.013589947089947088\n",
            "    },\n",
            "    {\n",
            "      \"feature\": \"age_47\",\n",
            "      \"mean_abs_shap\": 0.009455026455026454\n",
            "    },\n",
            "    {\n",
            "      \"feature\": \"age_48\",\n",
            "      \"mean_abs_shap\": 0.013700396825396824\n",
            "    },\n",
            "    {\n",
            "      \"feature\": \"age_51\",\n",
            "      \"mean_abs_shap\": 0.0059093915343915344\n",
            "    },\n",
            "    {\n",
            "      \"feature\": \"age_62\",\n",
            "      \"mean_abs_shap\": 0.008551587301587301\n",
            "    },\n",
            "    {\n",
            "      \"feature\": \"education_Bachelors\",\n",
            "      \"mean_abs_shap\": 0.25778902116402125\n",
            "    },\n",
            "    {\n",
            "      \"feature\": \"education_Masters\",\n",
            "      \"mean_abs_shap\": 0.025363756613756613\n",
            "    },\n",
            "    {\n",
            "      \"feature\": \"education_PhD\",\n",
            "      \"mean_abs_shap\": 0.06007804232804234\n",
            "    },\n",
            "    {\n",
            "      \"feature\": \"hours_per_week_40\",\n",
            "      \"mean_abs_shap\": 0.09071296296296298\n",
            "    },\n",
            "    {\n",
            "      \"feature\": \"hours_per_week_45\",\n",
            "      \"mean_abs_shap\": 0.01783465608465608\n",
            "    },\n",
            "    {\n",
            "      \"feature\": \"hours_per_week_50\",\n",
            "      \"mean_abs_shap\": 0.026285714285714284\n",
            "    },\n",
            "    {\n",
            "      \"feature\": \"hours_per_week_60\",\n",
            "      \"mean_abs_shap\": 0.03220171957671958\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Summary ===\")\n",
        "print(f\"Baseline accuracy: {baseline_metrics['accuracy']:.4f}, recall: {baseline_metrics['recall']:.4f}\")\n",
        "print(f\"Privacy(drop) accuracy: {privA_metrics['accuracy']:.4f}, recall: {privA_metrics['recall']:.4f}\")\n",
        "print(f\"Privacy(noisy) accuracy: {privB_metrics['accuracy']:.4f}, recall: {privB_metrics['recall']:.4f}\")\n",
        "print(\"Artifacts saved in ./artifacts: pima_audit.json, pima_governance_report.md, shap_pima_bar.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ePYO_Zf6eOM",
        "outputId": "45224ba6-7f07-4282-ce7b-06e3433a4b67"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Summary ===\n",
            "Baseline accuracy: 0.7500, recall: 0.5373\n",
            "Privacy(drop) accuracy: 0.7500, recall: 0.5373\n",
            "Privacy(noisy) accuracy: 0.7500, recall: 0.5373\n",
            "Artifacts saved in ./artifacts: pima_audit.json, pima_governance_report.md, shap_pima_bar.png\n"
          ]
        }
      ]
    }
  ]
}