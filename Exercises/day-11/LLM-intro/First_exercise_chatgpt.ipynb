{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/appliedcode/mthree-c422/blob/c422_Mounika/Exercises/day-11/LLM-intro/First_exercise_chatgpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab Exercise: Introduction to Prompt Engineering and Large Language Models (LLMs)\n",
        "\n",
        "## Objectives\n",
        "\n",
        "- Understand *what prompt engineering is* and its importance in AI\n",
        "- Learn about *Large Language Models (LLMs)* and their key capabilities\n",
        "- Explore prompt engineering as a *new programming paradigm*\n",
        "- Survey *major LLM types* from OpenAI, Anthropic, and Google\n",
        "- Discuss *real-world applications* of prompt engineering\n",
        "- Gain practical experience crafting and testing prompts with OpenAI's GPT models in Colab\n",
        "\n",
        "***\n",
        "\n",
        "## 1. What is Prompt Engineering and Why It Matters\n",
        "\n",
        "Read this definition:\n",
        "*Prompt Engineering* is the practice of designing and refining the instructions or inputs (\"prompts\") given to Large Language Models (LLMs) to elicit desired and accurate outputs. Unlike traditional programming, here you \"program\" by language interaction.\n",
        "\n",
        "**Why it matters:**\n",
        "\n",
        "- Maximizes LLM output quality without retraining\n",
        "- Enables custom task control via instructions, examples, personas\n",
        "- Reduces hallucinations and irrelevant output\n",
        "- Vital for practical AI deployments\n",
        "\n",
        "***\n",
        "\n",
        "## 2. Overview of Large Language Models (LLMs)\n",
        "\n",
        "- LLMs are pretrained AI models with billions+ parameters trained on massive text corpora.\n",
        "- They understand and can generate human-like text in multiple formats and languages.\n",
        "- Examples:\n",
        "    - **OpenAI:** GPT-3, GPT-4, ChatGPT\n",
        "    - **Anthropic:** Claude family\n",
        "    - **Google:** PaLM, Gemini\n",
        "\n",
        "*Capabilities:* text generation, summarization, translation, question answering, code writing, conversational AI.\n",
        "\n",
        "***\n",
        "\n",
        "## 3. Prompt Engineering as a New Programming Paradigm\n",
        "\n",
        "- Move from traditional code and logic to instructing LLMs with natural language prompts\n",
        "- Incorporates clear instructions, few-shot examples, role assignments (personas)\n",
        "- Iteratively refine prompt design based on responses\n",
        "- Allows rapid prototyping across diverse tasks\n",
        "\n",
        "***\n",
        "\n",
        "## 4. Types of Popular LLMs\n",
        "\n",
        "| Provider | Model Examples | Special Features |\n",
        "| :-- | :-- | :-- |\n",
        "| OpenAI | GPT-3, GPT-4, ChatGPT | Large-scale APIs, conversational AI |\n",
        "| Anthropic | Claude | Safety-focused, steerable chatbots |\n",
        "| Google (DeepMind) | PaLM, Gemini | Multi-modal, grounded reasoning |\n",
        "\n",
        "Good to know: Different models have different token limits, pricing, and availability.\n",
        "\n",
        "***\n",
        "\n",
        "## 5. Real-World Applications of Prompt Engineering\n",
        "\n",
        "- Customer support chatbots\n",
        "- Content creation (blogs, ads, scripts)\n",
        "- Code generation and debugging\n",
        "- Legal and medical document analysis\n",
        "- Personalized tutoring and coaching\n",
        "- Data extraction and summarization\n",
        "\n",
        "***\n"
      ],
      "metadata": {
        "id": "hFnkDgKDTUgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# To set your OpenAI API key securely in Colab Secrets:\n",
        "# 1. Click on the \"üîë\" icon in the left sidebar.\n",
        "# 2. Click \"Add new secret\".\n",
        "# 3. For \"Name\", enter OPENAI_API_KEY\n",
        "# 4. For \"Value\", enter your OpenAI API key (It starts with sk-...).\n",
        "# 5. Ensure the \"Notebook access\" toggle is turned ON for this notebook.\n",
        "\n",
        "# Retrieve key in your notebook\n",
        "openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "if openai_api_key and openai_api_key.startswith(\"sk-\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "    # Verify the environment variable is set\n",
        "    if os.getenv(\"OPENAI_API_KEY\") is not None:\n",
        "        print(\"‚úÖ OpenAI API key loaded successfully into environment variables.\")\n",
        "        # Print a masked version of the key for verification\n",
        "        print(f\"üîë Retrieved key (masked): {openai_api_key[:4]}...{openai_api_key[-4:]}\")\n",
        "    else:\n",
        "        print(\"‚ùå Failed to set OpenAI API key in environment variables.\")\n",
        "else:\n",
        "    print(\"‚ùå OpenAI API key not found or is invalid. Please set it using Colab Secrets as described above.\")\n",
        "    print(\"Ensure the secret is named OPENAI_API_KEY and starts with 'sk-'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-MrRDkKZxI6",
        "outputId": "22ff2503-6302-43e8-b0a4-f0834fc58ccd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ OpenAI API key loaded successfully into environment variables.\n",
            "üîë Retrieved key (masked): sk-p...a3gA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet openai -q"
      ],
      "metadata": {
        "id": "eyeKWLK4bM2Q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create client\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "if api_key is not None:\n",
        "    client = OpenAI(api_key=api_key)\n",
        "    print(\"‚úÖ OpenAI client initialized successfully.\")\n",
        "else:\n",
        "    print(\"‚ùå Failed to initialize OpenAI client: API key not found in environment variables.\")"
      ],
      "metadata": {
        "id": "lNqOYjPrbNo7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98d4db37-523c-4e1d-f9f8-e1f5842cf47e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ OpenAI client initialized successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Explain what prompt engineering is in one paragraph.\"\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",  # Free-tier friendly\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful teacher.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ],\n",
        "    max_tokens=100\n",
        ")\n",
        "\n",
        "print(\"\\nüí¨ Model Output:\\n\")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8U4WXgvtbWV-",
        "outputId": "e0008d7f-64fb-442d-a52c-01ba0d169e70"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üí¨ Model Output:\n",
            "\n",
            "Prompt engineering is the process of crafting and refining input prompts given to artificial intelligence language models to elicit desired responses or behavior. It involves understanding how the model interprets different phrases and structures, allowing users to optimize their queries to achieve more accurate, relevant, and contextually appropriate outputs. By strategically modifying the wording, tone, and specificity of prompts, practitioners can enhance the effectiveness of AI communications in various applications, such as content generation, chatbot interactions, and data analysis. This practice is crucial for maximizing\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}