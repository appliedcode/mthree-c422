{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7011d66a",
      "metadata": {
        "id": "7011d66a"
      },
      "source": [
        "# Lab Exercise: Comparison of Self-Attention, Multi-Head Attention, and Feedforward Neural Networks\n",
        "\n",
        "## Objective:\n",
        "\n",
        "Understand and compare the function, behavior, and outputs of self-attention, multi-head attention, and feedforward layers in transformer models, using a simple example sentence.\n",
        "\n",
        "## Part 1: Conceptual Understanding (Pre-Lab)\n",
        "\n",
        "- Review what each component does:\n",
        "    - **Self-Attention:** Learn how each word in a sentence attends to (relates to) every other word.\n",
        "    - **Multi-Head Attention:** See how multiple attention mechanisms operate simultaneously, each capturing different types of relationships.\n",
        "    - **Feedforward Neural Networks:** Observe how each token embedding is independently processed post-attention for refined transformation.\n",
        "- Use the sentence:\n",
        "*\"The cat sat on the mat.\"*\n",
        "Keep this in mind while running the code and analyzing outputs.\n",
        "\n",
        "\n",
        "## Part 2: Implement and Visualize Self-Attention\n",
        "\n",
        "1. **Build a self-attention layer** that calculates attention scores between all words.\n",
        "2. **Input:** Represent words as vectors (use random or simplified embeddings for demonstration).\n",
        "3. **Output:** Attention matrix showing how each word attends to others.\n",
        "4. **Visualize:** Plot the attention weights matrix as a heatmap.\n",
        "\n",
        "## Part 3: Implement Multi-Head Attention\n",
        "\n",
        "1. **Build multiple self-attention heads** (e.g., 4 heads), each with different parameters.\n",
        "2. **Combine their outputs** by concatenation and a projection layer.\n",
        "3. **Visualize:** Plot each attention head’s matrix separately and the combined output.\n",
        "\n",
        "## Part 4: Build Feedforward Neural Network Layer\n",
        "\n",
        "1. Apply a simple feedforward neural network to each word vector after attention.\n",
        "2. Use at least two dense layers with activation functions (e.g., ReLU).\n",
        "3. Observe how the vectors transform compared to the attention output.\n",
        "\n",
        "## Part 5: Analyze and Compare Outputs\n",
        "\n",
        "1. Compare the attention weights of self-attention vs. multi-head attention.\n",
        "2. Analyze how feedforward layers refine or transform the attention’s output.\n",
        "3. Discuss differences in what each component contributes to the model’s understanding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4182f4da",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "4182f4da"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Sample sentence tokens\n",
        "tokens = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
        "vocab_size = len(tokens)\n",
        "embedding_dim = 8  # small for demo\n",
        "\n",
        "# Random embeddings for each word token (normally learned)\n",
        "torch.manual_seed(0)\n",
        "embeddings = torch.rand(vocab_size, embedding_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d8abbf4",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "1d8abbf4"
      },
      "outputs": [],
      "source": [
        "# --------- Part 2: Self-Attention ---------\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "        self.scale = embed_dim ** 0.5\n",
        "\n",
        "    def forward(self, x):\n",
        "        Q = self.query(x)\n",
        "        K = self.key(x)\n",
        "        V = self.value(x)\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
        "        weights = torch.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(weights, V)\n",
        "        return weights, output\n",
        "\n",
        "self_attention = SelfAttention(embedding_dim)\n",
        "attn_weights, attn_output = self_attention(embeddings)\n",
        "\n",
        "# Visualize self-attention weights\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(attn_weights.detach().numpy(), xticklabels=tokens, yticklabels=tokens, cmap='viridis')\n",
        "plt.title(\"Self-Attention Weights\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11734d69",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "11734d69"
      },
      "outputs": [],
      "source": [
        "# --------- Part 3: Multi-Head Attention ---------\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0] if x.ndim == 3 else 1\n",
        "        Q = self.query(x).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = self.key(x).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.value(x).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        weights = torch.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(weights, V)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.head_dim)\n",
        "\n",
        "        output = self.out_proj(output)\n",
        "        return weights, output\n",
        "\n",
        "multihead_attention = MultiHeadAttention(embedding_dim, num_heads=4)\n",
        "# Add batch dimension for multihead (batch=1)\n",
        "multihead_weights, multihead_output = multihead_attention(embeddings.unsqueeze(0))\n",
        "\n",
        "# Visualize each head's attention matrix individually\n",
        "for i in range(multihead_attention.num_heads):\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(multihead_weights[0, i].detach().numpy(), xticklabels=tokens, yticklabels=tokens, cmap='viridis')\n",
        "    plt.title(f\"Multi-Head Attention Weights - Head {i+1}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7616941",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "a7616941"
      },
      "outputs": [],
      "source": [
        "# --------- Part 4: Feedforward Layer ---------\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))\n",
        "\n",
        "feed_forward = FeedForward(embedding_dim, hidden_dim=16)\n",
        "ff_output = feed_forward(attn_output)\n",
        "\n",
        "print(\"Shape of attention output:\", attn_output.shape)\n",
        "print(\"Shape of feedforward output:\", ff_output.shape)\n",
        "\n",
        "# Optional: Print vectors for first token as example\n",
        "print(\"\\nSample token 'The' embedding vectors:\")\n",
        "print(\"Attention output vector:\", attn_output[0].detach().numpy())\n",
        "print(\"Feedforward output vector:\", ff_output[0].detach().numpy())\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20956eae",
      "metadata": {
        "id": "20956eae"
      },
      "source": [
        "## Tasks for You:\n",
        "\n",
        "- Run the code and observe the **attention weight heatmaps** for self-attention and different heads of multi-head attention.\n",
        "- Notice how different heads pay attention to different word relationships.\n",
        "- Compare the input embeddings, attention output embeddings, and feedforward output embeddings.\n",
        "- Write a short summary on how the feedforward network transforms attention outputs for each token.\n",
        "- Experiment by changing embedding sizes, number of heads, and hidden layer sizes.\n",
        "- Reflect on why multi-head attention can capture more diverse aspects than single self-attention.\n",
        "- Bonus: Implement positional encoding and add it to embeddings before attention.\n",
        "\n",
        "\n",
        "## Summary\n",
        "\n",
        "This lab will give you hands-on experience distinguishing and visualizing how:\n",
        "\n",
        "- **Self-attention** builds relationships between words\n",
        "- **Multi-head attention** extends this by capturing multiple types of relationships simultaneously\n",
        "- **Feedforward networks** independently refine each token’s embedding after attention steps\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}