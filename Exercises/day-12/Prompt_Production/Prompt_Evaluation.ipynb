{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/appliedcode/mthree-c422/blob/main/Exercises/day-12/Prompt_Production/Prompt_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ## Lab Exercises: Integrating Prompts \\& Measuring Prompt Performance\n",
        "\n",
        "\n",
        "> Add blockquote\n",
        "\n",
        "\n",
        "***\n",
        "\n",
        "### Setup:\n",
        "\n",
        "- Use OpenAI API or any language model API accessible in your Colab.\n",
        "- If API keys are needed, ensure they are safely added as environment variables or input by the user."
      ],
      "metadata": {
        "id": "eq4qKCdSNSko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Set your OpenAI API key securely in Colab Secrets (once)\n",
        "# userdata.set(\"OPENAI_API_KEY\", \"your-api-key-here\")\n",
        "\n",
        "# Retrieve key in your notebook\n",
        "openai_api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "if openai_api_key:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "    print(\"✅ OpenAI API key loaded safely\")\n",
        "else:\n",
        "    print(\"❌ OpenAI API key not found. Please set it using Colab Secrets.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YMRqyRCNGmo",
        "outputId": "ff0d7246-fab6-4966-9510-e01bd254173e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ OpenAI API key loaded safely\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install --quiet openai -q\n",
        "# Create client\n",
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])"
      ],
      "metadata": {
        "id": "9qrFrzlcPI1n"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper Function to Send Prompts\n",
        "def generate_response(prompt, model=\"gpt-4o-mini\", temperature=0.7):\n",
        "    \"\"\"\n",
        "    Send a prompt to the OpenAI model and return the response text.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=temperature\n",
        "        )\n",
        "        return completion.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n"
      ],
      "metadata": {
        "id": "AieFziFDTcuy"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "\n",
        "# Exercise 1: Integrating Prompts in Applications and Pipelines\n",
        "\n",
        "**Objective:** Learn how to integrate prompts into application workflows with versioning, automated testing, and CI/CD principles.\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "1. **Simulate prompt versioning:**\n",
        "    - Create two versions of a prompt for a customer support chatbot (e.g., one generic, one with explicit instructions).\n",
        "    - Store them as Python strings and switch between them in your code."
      ],
      "metadata": {
        "id": "u9GORkRefi-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_v1 = \"Answer customer queries about returns.\"\n",
        "prompt_v2 = \"You are a helpful assistant answering customer return questions clearly and politely.\"\n",
        "\n",
        "# Use either version\n",
        "prompt = prompt_v2\n",
        "\n",
        "response = generate_response(prompt + \"\\nCustomer: How do I return a product?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "dpcx6QvsfkxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Build a simple automated prompt test:**\n",
        "    - Write a small function that runs a set of input prompts through your prompt versions.\n",
        "    - Checks if responses include key expected phrases (e.g., \"return policy\", \"contact support\").\n",
        "    - Output pass/fail for each test."
      ],
      "metadata": {
        "id": "8Fsjj2aofmQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_inputs = [\n",
        "    \"How do I return my purchase?\",\n",
        "    \"What is the refund policy?\"\n",
        "]\n",
        "expected_keywords = [\"return policy\", \"contact support\"]\n",
        "\n",
        "def test_prompt(prompt):\n",
        "    print(f\"Testing prompt:\\n{prompt}\\n\")\n",
        "    for input_text in test_inputs:\n",
        "        response = generate_response(prompt + \"\\nCustomer: \" + input_text)\n",
        "        print(f\"Input: {input_text}\\nResponse: {response}\\n\")\n",
        "        passed = all(keyword in response.lower() for keyword in expected_keywords)\n",
        "        print(\"Pass\" if passed else \"Fail\", \"\\n\")\n",
        "\n",
        "# Test both versions\n",
        "test_prompt(prompt_v1)\n",
        "test_prompt(prompt_v2)"
      ],
      "metadata": {
        "id": "AU2L4Da7fn13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Discuss integration with CI/CD:**\n",
        "    - Outline how this kind of automated prompt testing can be triggered on every prompt update in a CI/CD pipeline to ensure prompt quality before deployment.\n",
        "    - Optionally, simulate prompt update by changing the prompt string and re-running tests.\n",
        "\n",
        "***\n",
        "\n",
        "# Exercise 2: Prompt Performance Measurement and Evaluation\n",
        "\n",
        "**Objective:** Measure prompt quality using key metrics and understand how to evaluate and improve prompts systematically.\n",
        "\n",
        "**Tasks:**\n",
        "\n",
        "1. **Define and implement simple metrics:**\n",
        "    - For each prompt response, measure:\n",
        "        - **Relevance**: Does the response include certain keywords or meet a relevance criterion?\n",
        "        - **Length**: Count the number of words or tokens.\n",
        "        - **Consistency**: Run the same prompt multiple times and compare responses for similarity (string match or cosine similarity using embeddings if available).\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "6OGS4DosfpWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import difflib\n",
        "\n",
        "prompt = \"Explain the benefits of renewable energy.\"\n",
        "\n",
        "# Run same prompt 3 times\n",
        "responses = [generate_response(prompt) for _ in range(3)]\n",
        "\n",
        "print(\"Responses:\")\n",
        "for i, resp in enumerate(responses):\n",
        "    print(f\"Run {i+1}: {resp}\\n\")\n",
        "\n",
        "# Simple consistency check: ratio of similarity between runs 1 and 2\n",
        "similarity = difflib.SequenceMatcher(None, responses[^0], responses[^1]).ratio()\n",
        "print(f\"Similarity between first two runs: {similarity:.2f}\")"
      ],
      "metadata": {
        "id": "GDs4-cpifvsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Automated evaluation function:**\n",
        "    - Build a function that takes a prompt and expected keywords (or a reference text) and computes pass/fail or similarity score."
      ],
      "metadata": {
        "id": "0V25d5oGfxeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_prompt(prompt, expected_keywords):\n",
        "    response = generate_response(prompt)\n",
        "    print(f\"Response:\\n{response}\\n\")\n",
        "    relevance = all(keyword.lower() in response.lower() for keyword in expected_keywords)\n",
        "    length = len(response.split())\n",
        "    print(f\"Relevance: {'Passed' if relevance else 'Failed'}\")\n",
        "    print(f\"Response length (words): {length}\\n\")\n",
        "\n",
        "evaluate_prompt(\"Explain the importance of water conservation.\", [\"water\", \"conservation\", \"importance\"])"
      ],
      "metadata": {
        "id": "smMm5hVAfzTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Incorporate user feedback (simulated):**\n",
        "    - Ask students to simulate or collect simple user feedback scores (e.g., 1 to 5 rating).\n",
        "    - Show how this qualitative data complements automated metrics.\n",
        "4. **Discuss trade-offs:**\n",
        "    - Latency vs. response quality\n",
        "    - Length vs. clarity\n",
        "    - Consistency vs. creativity\n",
        "\n",
        "***\n",
        "\n",
        "## Summary for Students:\n",
        "\n",
        "- Practice managing prompt versions and testing integrations as mini automation.\n",
        "- Use keyword checks and repeated runs for simple prompt effectiveness measurement.\n",
        "- Understand that prompt evaluation involves both quantitative metrics and qualitative feedback.\n",
        "- Consider embedding prompt evaluation into automated pipelines for continuous prompt improvement.\n",
        "\n",
        "***"
      ],
      "metadata": {
        "id": "3sP66Oy8f1qX"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}