{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwsewmcfHD3t20yCmLsZs5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/appliedcode/mthree-c422/blob/main/Exercises/day-4/Embedded_methods/Lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Je1rSo0GgiJ0"
      },
      "outputs": [],
      "source": [
        "# Lab Exercises: Embedded Feature‐Selection Methods\n",
        "# Use the Breast Cancer dataset (sklearn.datasets.load_breast_cancer) for all exercises. Split once into training and test sets:\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True, as_frame=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 4: Tree-Based Importance\n",
        "- Fit RandomForestClassifier(n_estimators=100, random_state=0).\n",
        "\n",
        "- Use SelectFromModel to select the top 5 features by impurity importance.\n",
        "\n",
        "- Retrain and evaluate."
      ],
      "metadata": {
        "id": "rbDBckzBgvE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "sfm_rf = SelectFromModel(rf, prefit=True, max_features=5, threshold=-np.inf)\n",
        "feat_rf = X_train.columns[sfm_rf.get_support()]\n",
        "\n",
        "model = LogisticRegression(max_iter=5000).fit(X_train[feat_rf], y_train)\n",
        "print(\"RF features:\", list(feat_rf))\n",
        "print(\"Accuracy (RF):\", accuracy_score(y_test, model.predict(X_test[feat_rf])))"
      ],
      "metadata": {
        "id": "mJ9vGlC-gqTu",
        "outputId": "f4724728-7c5e-470a-9b3e-cd762f586dc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RF features: ['mean concavity', 'mean concave points', 'worst radius', 'worst perimeter', 'worst concave points']\n",
            "Accuracy (RF): 0.9532163742690059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 5: Gradient-Boosting (XGBoost) Importance\n",
        "- Fit XGBClassifier(n_estimators=100, use_label_encoder=False, - eval_metric='logloss', random_state=0).\n",
        "\n",
        "- Use SelectFromModel to pick the top 5 features.\n",
        "\n",
        "- Retrain and evaluate."
      ],
      "metadata": {
        "id": "LrQIwB2Kg4BT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "xgb_clf = xgb.XGBClassifier(\n",
        "    n_estimators=100, eval_metric='logloss', random_state=0\n",
        ")\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "sfm_xgb = SelectFromModel(xgb_clf, prefit=True, max_features=5, threshold=-np.inf)\n",
        "feat_xgb = X_train.columns[sfm_xgb.get_support()]\n",
        "\n",
        "model = LogisticRegression(max_iter=5000).fit(X_train[feat_xgb], y_train)\n",
        "print(\"XGB features:\", list(feat_xgb))\n",
        "print(\"Accuracy (XGB):\", accuracy_score(y_test, model.predict(X_test[feat_xgb])))\n"
      ],
      "metadata": {
        "id": "UtHlhNfgg0_N",
        "outputId": "2bc34547-5e9b-4653-825b-e588d3747329",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGB features: ['mean concave points', 'worst radius', 'worst perimeter', 'worst area', 'worst concave points']\n",
            "Accuracy (XGB): 0.9590643274853801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 6: Stability Selection (Randomized Lasso)\n",
        "- Fit RandomizedLasso(alpha=0.025, random_state=0).\n",
        "\n",
        "- Select the top 5 features with highest selection frequency.\n",
        "\n",
        "- Retrain and evaluate."
      ],
      "metadata": {
        "id": "q9Ihd18WhEoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Parameters for stability selection\n",
        "n_bootstraps    = 100\n",
        "sample_fraction = 0.75\n",
        "C               = 1.0\n",
        "\n",
        "# Increase iterations and loosen tolerance\n",
        "max_iter = 20000\n",
        "tol      = 1e-3\n",
        "\n",
        "n_features       = X_train.shape[1]\n",
        "selection_counts = np.zeros(n_features, dtype=int)\n",
        "\n",
        "for i in range(n_bootstraps):\n",
        "    # a) bootstrap sample\n",
        "    idx  = resample(\n",
        "        np.arange(X_train.shape[0]),\n",
        "        replace=True,\n",
        "        n_samples=int(sample_fraction * X_train.shape[0]),\n",
        "        random_state=i\n",
        "    )\n",
        "    X_bs = X_train.values[idx]\n",
        "    y_bs = y_train.values[idx]\n",
        "\n",
        "    # b) fit L1‐penalized logistic with more iterations and looser tol\n",
        "    lr = LogisticRegression(\n",
        "        penalty='l1',\n",
        "        solver='saga',\n",
        "        C=C,\n",
        "        max_iter=max_iter,\n",
        "        tol=tol,\n",
        "        random_state=0\n",
        "    )\n",
        "    lr.fit(X_bs, y_bs)\n",
        "\n",
        "    # c) tally nonzero coefficients\n",
        "    nonzero = np.abs(lr.coef_)[0] > 1e-8\n",
        "    selection_counts += nonzero.astype(int)\n",
        "\n",
        "# Compute frequencies and pick top 5 stable features\n",
        "selection_freq = selection_counts / n_bootstraps\n",
        "top5_idx       = np.argsort(-selection_freq)[:5]\n",
        "feat_rl        = X_train.columns[top5_idx]\n",
        "\n",
        "# Retrain on the stable features\n",
        "final_model = LogisticRegression(max_iter=5000).fit(X_train[feat_rl], y_train)\n",
        "accuracy    = accuracy_score(y_test, final_model.predict(X_test[feat_rl]))\n",
        "\n",
        "print(\"Stability features:\", list(feat_rl))\n",
        "print(\"Accuracy (Stability):\", accuracy)"
      ],
      "metadata": {
        "id": "2CfDVtaZg-2R",
        "outputId": "02e9eb60-7475-4c7f-95b2-284505ab357e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stability features: ['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean concavity']\n",
            "Accuracy (Stability): 0.935672514619883\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 7: Embedded Specialized Model (Decision Tree)\n",
        "- Fit DecisionTreeClassifier(max_depth=3, random_state=0).\n",
        "\n",
        "- Use its feature_importances_ to select top 5.\n",
        "\n",
        "- Retrain logistic model and evaluate."
      ],
      "metadata": {
        "id": "Yfcf07osiPGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "dt = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
        "dt.fit(X_train, y_train)\n",
        "\n",
        "import numpy as np\n",
        "idx_dt = np.argsort(dt.feature_importances_)[-5:]\n",
        "feat_dt = X_train.columns[idx_dt]\n",
        "\n",
        "model = LogisticRegression(max_iter=5000).fit(X_train[feat_dt], y_train)\n",
        "print(\"DT features:\", list(feat_dt))\n",
        "print(\"Accuracy (DT):\", accuracy_score(y_test, model.predict(X_test[feat_dt])))\n"
      ],
      "metadata": {
        "id": "9vTePYw4iWql",
        "outputId": "e6a6d6dd-e665-4785-a09d-bef07604e0f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DT features: ['worst perimeter', 'worst area', 'worst radius', 'worst texture', 'mean concave points']\n",
            "Accuracy (DT): 0.9707602339181286\n"
          ]
        }
      ]
    }
  ]
}