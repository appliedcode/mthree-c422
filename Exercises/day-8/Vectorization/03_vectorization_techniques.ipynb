{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "808233dc",
   "metadata": {},
   "source": [
    "# Notebook 03: Vectorization Techniques\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this session, you will:\n",
    "1. **Understand text vectorization** - Convert text to numerical representations\n",
    "2. **Master Bag-of-Words (BoW)** - Build document-term matrices\n",
    "3. **Implement TF-IDF** - Weight terms by importance\n",
    "4. **Extract n-grams** - Capture phrase-level patterns\n",
    "5. **Visualize text features** - Analyze word frequencies and importance\n",
    "6. **Compare vectorization methods** - Choose the right approach for your task\n",
    "\n",
    "## Why Vectorize Text?\n",
    "Machine learning algorithms work with numbers, not text. We need to convert text into numerical vectors while preserving semantic meaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fb75b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "\n",
    "# Sample documents for vectorization\n",
    "documents = [\n",
    "    \"I love machine learning and natural language processing\",\n",
    "    \"Machine learning is a subset of artificial intelligence\", \n",
    "    \"Natural language processing helps computers understand human language\",\n",
    "    \"Deep learning is a powerful machine learning technique\",\n",
    "    \"Python is great for machine learning and data science\",\n",
    "    \"Text mining and natural language processing are related fields\",\n",
    "    \"Artificial intelligence will transform many industries\",\n",
    "    \"Data science combines statistics, programming, and domain knowledge\"\n",
    "]\n",
    "\n",
    "print(\"Sample Documents for Vectorization:\")\n",
    "print(\"=\" * 50)\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    print(f\"{i}. {doc}\")\n",
    "\n",
    "# Create a simple preprocessing function\n",
    "def simple_preprocess(text):\n",
    "    \"\"\"Basic text preprocessing for vectorization\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and extra spaces\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Preprocess documents\n",
    "processed_docs = [simple_preprocess(doc) for doc in documents]\n",
    "print(f\"\\nPreprocessed Documents:\")\n",
    "for i, doc in enumerate(processed_docs, 1):\n",
    "    print(f\"{i}. {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b33acf3",
   "metadata": {},
   "source": [
    "## Section 1: Bag-of-Words (BoW) Representation\n",
    "\n",
    "### Concept\n",
    "Bag-of-Words represents each document as a vector of word counts, ignoring grammar and word order but keeping track of frequency.\n",
    "\n",
    "**Example**: \"I love NLP\" â†’ [1, 1, 1, 0, 0, ...] for vocabulary [I, love, NLP, machine, learning, ...]\n",
    "\n",
    "### Advantages:\n",
    "- Simple to understand and implement\n",
    "- Works well for many text classification tasks\n",
    "- Captures word importance through frequency\n",
    "\n",
    "### Disadvantages:\n",
    "- Loses word order and grammar\n",
    "- Sparse vectors (mostly zeros)\n",
    "- No semantic understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d128ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-Words with scikit-learn CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer(\n",
    "    lowercase=True,          # Convert to lowercase\n",
    "    stop_words='english',    # Remove common English stop words\n",
    "    max_features=100,        # Limit vocabulary size\n",
    "    ngram_range=(1, 1)       # Use single words (unigrams)\n",
    ")\n",
    "\n",
    "# Fit and transform documents\n",
    "bow_matrix = vectorizer.fit_transform(processed_docs)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(\"BAG-OF-WORDS ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Vocabulary size: {len(feature_names)}\")\n",
    "print(f\"Matrix shape: {bow_matrix.shape}\")\n",
    "print(f\"Matrix density: {bow_matrix.nnz / (bow_matrix.shape[0] * bow_matrix.shape[1]):.3f}\")\n",
    "\n",
    "# Convert to dense array for visualization\n",
    "bow_dense = bow_matrix.toarray()\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "bow_df = pd.DataFrame(bow_dense, columns=feature_names)\n",
    "bow_df.index = [f\"Doc {i+1}\" for i in range(len(documents))]\n",
    "\n",
    "print(f\"\\nFirst 10 features:\")\n",
    "print(bow_df.iloc[:, :10])\n",
    "\n",
    "# Analyze vocabulary\n",
    "print(f\"\\nVocabulary (first 20 words):\")\n",
    "print(list(feature_names[:20]))\n",
    "\n",
    "# Word frequency analysis\n",
    "word_freq = bow_matrix.sum(axis=0).A1  # Sum across documents\n",
    "word_freq_df = pd.DataFrame({\n",
    "    'word': feature_names,\n",
    "    'frequency': word_freq\n",
    "}).sort_values('frequency', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 15 most frequent words:\")\n",
    "print(word_freq_df.head(15))\n",
    "\n",
    "# Visualize word frequencies\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_words = word_freq_df.head(15)\n",
    "plt.bar(top_words['word'], top_words['frequency'])\n",
    "plt.title('Top 15 Most Frequent Words (BoW)')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Document similarity using BoW\n",
    "similarity_matrix = cosine_similarity(bow_matrix)\n",
    "similarity_df = pd.DataFrame(similarity_matrix, \n",
    "                           index=[f\"Doc {i+1}\" for i in range(len(documents))],\n",
    "                           columns=[f\"Doc {i+1}\" for i in range(len(documents))])\n",
    "\n",
    "print(f\"\\nDocument Similarity Matrix (Cosine Similarity):\")\n",
    "print(similarity_df.round(3))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
