{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlaHgny1N0UiXUtaP3yHUE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/appliedcode/mthree-c422/blob/mthree-c422-Likhitha/Production_Ready.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HNNYhLdSa0_n"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import hashlib\n",
        "import datetime\n",
        "import csv\n",
        "import os\n",
        "import warnings\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = 1000\n",
        "genders = [\"Male\", \"Female\", \"Other\"]\n",
        "regions = [\"Urban\", \"Rural\"]\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    \"Patient_ID\": range(1, n+1),\n",
        "    \"Age\": np.random.randint(18, 90, n),\n",
        "    \"Gender\": np.random.choice(genders, n, p=[0.48, 0.48, 0.04]),\n",
        "    \"Region\": np.random.choice(regions, n, p=[0.65, 0.35]),\n",
        "    \"BMI\": np.round(np.random.normal(27, 5, n), 1),\n",
        "    \"Blood_Pressure\": np.random.randint(80, 180, n),\n",
        "    \"Cholesterol\": np.random.randint(150, 300, n),\n",
        "})\n",
        "\n",
        "# baseline label logic\n",
        "data[\"Heart_Disease\"] = np.where(\n",
        "    (data[\"Cholesterol\"] > 220) & (data[\"Blood_Pressure\"] > 140),\n",
        "    np.random.choice([1, 0], n, p=[0.7, 0.3]),\n",
        "    np.random.choice([1, 0], n, p=[0.3, 0.7])\n",
        ")\n",
        "\n",
        "# introduce small bias: slightly higher positive rate for urban patients\n",
        "mask_urban = data[\"Region\"] == \"Urban\"\n",
        "rand_for_all = np.random.choice([1, 0], n, p=[0.55, 0.45])\n",
        "data.loc[mask_urban, \"Heart_Disease\"] = rand_for_all[mask_urban]\n",
        "\n",
        "print(\"Sample rows:\")\n",
        "print(data.head(), \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYq2usEzbKL3",
        "outputId": "245a0369-34f8-4c4e-e684-ff863fe4229a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample rows:\n",
            "   Patient_ID  Age  Gender Region   BMI  Blood_Pressure  Cholesterol  \\\n",
            "0           1   69  Female  Rural  19.9             167          216   \n",
            "1           2   32    Male  Urban  28.1              93          220   \n",
            "2           3   89  Female  Urban  22.7             113          295   \n",
            "3           4   78    Male  Urban  21.8             137          255   \n",
            "4           5   38    Male  Urban  24.8             155          156   \n",
            "\n",
            "   Heart_Disease  \n",
            "0              0  \n",
            "1              1  \n",
            "2              0  \n",
            "3              1  \n",
            "4              0   \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pseudonymize_ids(series):\n",
        "    return series.astype(str).apply(lambda x: hashlib.sha256(x.encode()).hexdigest())\n",
        "\n",
        "def prepare_features(df, drop_sensitive=False):\n",
        "    df_proc = df.copy()\n",
        "    # optionally drop sensitive columns\n",
        "    sensitive_cols = [\"Gender\", \"Region\"]\n",
        "    if drop_sensitive:\n",
        "        df_proc = df_proc.drop(columns=sensitive_cols)\n",
        "    X = pd.get_dummies(df_proc.drop(columns=[\"Patient_ID\",\"Heart_Disease\"]), drop_first=True)\n",
        "    return X\n",
        "\n",
        "def train_random_forest(X_train, y_train, n_estimators=100, random_state=42, sample_weight=None):\n",
        "    clf = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state)\n",
        "    clf.fit(X_train, y_train, sample_weight)\n",
        "    return clf\n",
        "\n",
        "def evaluate_by_group(df_features, y_true, y_pred, original_df, group_col):\n",
        "    # original_df must align with df_features index\n",
        "    df_eval = original_df.loc[y_true.index].copy()\n",
        "    df_eval = df_eval.assign(y_true=y_true.values, y_pred=y_pred)\n",
        "    rates = df_eval.groupby(group_col)[\"y_pred\"].mean()\n",
        "    return rates, df_eval\n",
        "\n",
        "def fairness_check(df_eval, sensitive_attr, threshold=0.1):\n",
        "    rates = df_eval.groupby(sensitive_attr)[\"y_pred\"].mean()\n",
        "    disparity = float(rates.max() - rates.min())\n",
        "    print(f\"{sensitive_attr} disparity = {disparity:.4f} (threshold {threshold})\")\n",
        "    return disparity <= threshold, disparity, rates"
      ],
      "metadata": {
        "id": "-4PskE-8bOsM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = prepare_features(data, drop_sensitive=False)\n",
        "y = data[\"Heart_Disease\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, stratify=y)\n",
        "print(\"Train/test sizes:\", X_train.shape, X_test.shape)\n",
        "\n",
        "model = train_random_forest(X_train, y_train, n_estimators=100)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"\\nBaseline accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, digits=3))\n",
        "\n",
        "# Map group columns for evaluation (we need original data rows corresponding to test index)\n",
        "# Construct df_test_original aligned with X_test.index\n",
        "df_test_original = data.loc[X_test.index]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNDyGbq4bTCF",
        "outputId": "1788b21f-1f96-4ba2-8e7d-280d5bed91ea"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/test sizes: (700, 7) (300, 7)\n",
            "\n",
            "Baseline accuracy: 0.5466666666666666\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.548     0.563     0.556       151\n",
            "           1      0.545     0.530     0.537       149\n",
            "\n",
            "    accuracy                          0.547       300\n",
            "   macro avg      0.547     0.547     0.546       300\n",
            "weighted avg      0.547     0.547     0.547       300\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n-- Bias detection on baseline model --\")\n",
        "rates_gender, df_eval_gender = evaluate_by_group(X_test, y_test, y_pred, data, \"Gender\")\n",
        "print(\"Positive prediction rate by Gender:\\n\", rates_gender, \"\\n\")\n",
        "ok_gender, disp_gender, rates_gender = fairness_check(df_eval_gender, \"Gender\", threshold=0.1)\n",
        "\n",
        "rates_region, df_eval_region = evaluate_by_group(X_test, y_test, y_pred, data, \"Region\")\n",
        "print(\"Positive prediction rate by Region:\\n\", rates_region, \"\\n\")\n",
        "ok_region, disp_region, rates_region = fairness_check(df_eval_region, \"Region\", threshold=0.1)\n",
        "\n",
        "if not ok_gender or not ok_region:\n",
        "    print(\"\\n❌ Fairness check failed — disparity above threshold. Mitigation required before deployment.\")\n",
        "else:\n",
        "    print(\"\\n✅ Fairness check passed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qPf4Q6BbVyW",
        "outputId": "3125cc22-5602-438f-a7da-d3053de32e5e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-- Bias detection on baseline model --\n",
            "Positive prediction rate by Gender:\n",
            " Gender\n",
            "Female    0.490196\n",
            "Male      0.455224\n",
            "Other     0.692308\n",
            "Name: y_pred, dtype: float64 \n",
            "\n",
            "Gender disparity = 0.2371 (threshold 0.1)\n",
            "Positive prediction rate by Region:\n",
            " Region\n",
            "Rural    0.203252\n",
            "Urban    0.677966\n",
            "Name: y_pred, dtype: float64 \n",
            "\n",
            "Region disparity = 0.4747 (threshold 0.1)\n",
            "\n",
            "❌ Fairness check failed — disparity above threshold. Mitigation required before deployment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n-- Feature importance (top 10) --\")\n",
        "feat_imp = pd.Series(model.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
        "print(feat_imp.head(10).to_string())\n",
        "\n",
        "# Flag if socio-demographic features are large contributors\n",
        "sensitive_influence = 0.0\n",
        "for col in X_train.columns:\n",
        "    if col.startswith(\"Gender_\") or col.startswith(\"Region_\"):\n",
        "        sensitive_influence += feat_imp.get(col, 0.0)\n",
        "print(f\"\\nTotal importance attributed to Gender/Region columns: {sensitive_influence:.4f}\")\n",
        "if sensitive_influence > 0.05:\n",
        "    print(\"Note: Sensitive attributes contribute non-trivially; consider mitigation (remove/reweight/regularize).\")\n",
        "else:\n",
        "    print(\"Sensitive attributes contribution low.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btAPhCDrbgqC",
        "outputId": "52cee09e-1de9-4253-9e33-bebd1578cae8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-- Feature importance (top 10) --\n",
            "BMI               0.231879\n",
            "Blood_Pressure    0.229189\n",
            "Cholesterol       0.225865\n",
            "Age               0.216455\n",
            "Region_Urban      0.059267\n",
            "Gender_Male       0.027622\n",
            "Gender_Other      0.009722\n",
            "\n",
            "Total importance attributed to Gender/Region columns: 0.0966\n",
            "Note: Sensitive attributes contribute non-trivially; consider mitigation (remove/reweight/regularize).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n-- Mitigation A: Retrain without sensitive features (Gender, Region) --\")\n",
        "X_nosens = prepare_features(data, drop_sensitive=True)\n",
        "Xn_train, Xn_test, yn_train, yn_test = train_test_split(X_nosens, y, test_size=0.30, random_state=42, stratify=y)\n",
        "\n",
        "model_nosens = train_random_forest(Xn_train, yn_train)\n",
        "yn_pred = model_nosens.predict(Xn_test)\n",
        "print(\"Accuracy (no sensitive features):\", accuracy_score(yn_test, yn_pred))\n",
        "\n",
        "# Evaluate disparity\n",
        "df_eval_nosens = data.loc[Xn_test.index].assign(y_true=yn_test.values, y_pred=yn_pred)\n",
        "ok_nosens, disp_nosens, rates_nosens = fairness_check(df_eval_nosens, \"Gender\", threshold=0.1)\n",
        "print(\"Rates without sensitive features (by Gender):\\n\", rates_nosens, \"\\n\")\n",
        "if ok_nosens:\n",
        "    print(\"Removing sensitive features reduced disparity to acceptable level.\")\n",
        "else:\n",
        "    print(\"Disparity still beyond threshold. Try alternative mitigation.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfUs3crbblyW",
        "outputId": "d3a22baf-b7ef-4773-a6f7-79b43e4412ab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-- Mitigation A: Retrain without sensitive features (Gender, Region) --\n",
            "Accuracy (no sensitive features): 0.5033333333333333\n",
            "Gender disparity = 0.0483 (threshold 0.1)\n",
            "Rates without sensitive features (by Gender):\n",
            " Gender\n",
            "Female    0.509804\n",
            "Male      0.492537\n",
            "Other     0.461538\n",
            "Name: y_pred, dtype: float64 \n",
            "\n",
            "Removing sensitive features reduced disparity to acceptable level.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n-- Mitigation B: Re-weighting training samples by group (Gender) --\")\n",
        "# compute group-wise label distribution in training set\n",
        "train_idx = X_train.index\n",
        "train_original = data.loc[train_idx]\n",
        "group_pos_rate = train_original.groupby(\"Gender\")[\"Heart_Disease\"].mean()\n",
        "group_counts = train_original[\"Gender\"].value_counts()\n",
        "\n",
        "# create weights to upweight underrepresented group's positives/negatives — simple heuristic\n",
        "weights = np.ones(len(train_original))\n",
        "gender_map = {\"Male\":0, \"Female\":1, \"Other\":2}  # not used directly but nice to have\n",
        "\n",
        "# target: equalize overall positive rates across genders by scaling each group's samples\n",
        "target_rate = train_original[\"Heart_Disease\"].mean()\n",
        "for g in train_original[\"Gender\"].unique():\n",
        "    g_mask = (train_original[\"Gender\"] == g).values\n",
        "    # scale weight for this group's samples\n",
        "    # if group's positive rate < target, upweight group's positive samples\n",
        "    grp_pos_rate = train_original.loc[train_original[\"Gender\"]==g, \"Heart_Disease\"].mean()\n",
        "    # avoid division by zero\n",
        "    scale = target_rate / (grp_pos_rate + 1e-6)\n",
        "    # cap scale to avoid exploding weights\n",
        "    scale = min(scale, 3.0)\n",
        "    weights[g_mask] = weights[g_mask] * scale\n",
        "\n",
        "# Fit model with sample weights\n",
        "model_rw = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model_rw.fit(X_train, y_train, sample_weight=weights)\n",
        "y_rw_pred = model_rw.predict(X_test)\n",
        "print(\"Accuracy (re-weighted):\", accuracy_score(y_test, y_rw_pred))\n",
        "# Evaluate disparity after reweighting\n",
        "rates_gender_rw, df_eval_gender_rw = evaluate_by_group(X_test, y_test, y_rw_pred, data, \"Gender\")\n",
        "ok_rw, disp_rw, rates_gender_rw = fairness_check(df_eval_gender_rw, \"Gender\", threshold=0.1)\n",
        "print(\"Rates after re-weighting (by Gender):\\n\", rates_gender_rw, \"\\n\")\n",
        "if ok_rw:\n",
        "    print(\"Re-weighting reduced disparity.\")\n",
        "else:\n",
        "    print(\"Re-weighting did not reduce disparity sufficiently. Consider more advanced fairness algorithms (e.g., adversarial debiasing, constraints).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LM-mFBghbrKF",
        "outputId": "ebab4ae3-7b37-4cff-e468-3c39f0377611"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-- Mitigation B: Re-weighting training samples by group (Gender) --\n",
            "Accuracy (re-weighted): 0.5233333333333333\n",
            "Gender disparity = 0.1751 (threshold 0.1)\n",
            "Rates after re-weighting (by Gender):\n",
            " Gender\n",
            "Female    0.503268\n",
            "Male      0.440299\n",
            "Other     0.615385\n",
            "Name: y_pred, dtype: float64 \n",
            "\n",
            "Re-weighting did not reduce disparity sufficiently. Consider more advanced fairness algorithms (e.g., adversarial debiasing, constraints).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n-- Deployment: Logging & Pseudonymization --\")\n",
        "# Prepare logging CSV\n",
        "LOGFILE = \"prediction_log.csv\"\n",
        "if os.path.exists(LOGFILE):\n",
        "    os.remove(LOGFILE)\n",
        "with open(LOGFILE, \"w\", newline='') as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=[\"timestamp\",\"patient_pseudo_id\",\"input_features\",\"prediction\",\"model_version\"])\n",
        "    writer.writeheader()\n",
        "\n",
        "# Pseudonymize Patient_IDs in the main dataset for deployment\n",
        "data_deploy = data.copy()\n",
        "data_deploy[\"Patient_PseudoID\"] = pseudonymize_ids(data_deploy[\"Patient_ID\"])\n",
        "\n",
        "# Logging function\n",
        "def predict_and_log_deploy(model, X_row, patient_id_pseudo, model_version=\"v1\"):\n",
        "    pred = int(model.predict(X_row)[0])\n",
        "    entry = {\n",
        "        \"timestamp\": datetime.datetime.now().isoformat(),\n",
        "        \"patient_pseudo_id\": patient_id_pseudo,\n",
        "        \"input_features\": X_row.to_dict(orient='records')[0],\n",
        "        \"prediction\": pred,\n",
        "        \"model_version\": model_version\n",
        "    }\n",
        "    # append to CSV\n",
        "    with open(LOGFILE, \"a\", newline='') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=entry.keys())\n",
        "        writer.writerow(entry)\n",
        "    return pred\n",
        "\n",
        "# simulate a deployment prediction on first test sample\n",
        "sample_idx = X_test.index[0]\n",
        "X_row = X_test.loc[[sample_idx]]\n",
        "pseudo = data_deploy.loc[sample_idx, \"Patient_PseudoID\"]\n",
        "pred_sim = predict_and_log_deploy(model, X_row, pseudo, model_version=\"baseline-v1\")\n",
        "print(\"Simulated deploy prediction logged for pseudo id:\", pseudo, \"prediction:\", pred_sim)\n",
        "print(\"Check\", LOGFILE, \"for logs.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpmvitbubxgW",
        "outputId": "c662291a-4942-4f9f-f6ec-e7c68bde0e1d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-- Deployment: Logging & Pseudonymization --\n",
            "Simulated deploy prediction logged for pseudo id: 8a8b2d66735ed03d0841027e42d38806eedd8e5bd5da54270f958a55d509091f prediction: 1\n",
            "Check prediction_log.csv for logs.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-- Monitoring: Simulate post-deployment monitoring --\")\n",
        "# for simplicity, compute latest batch predictions (use y_pred from baseline) and group rates\n",
        "latest_preds = y_pred  # from baseline earlier\n",
        "# compute group positive rates used earlier: rates_gender\n",
        "dp_diff = disp_gender\n",
        "if dp_diff > 0.1:\n",
        "    print(\"⚠️ Alert: Fairness threshold exceeded post-deployment. Disparity:\", disp_gender)\n",
        "else:\n",
        "    print(\"✅ Fairness levels acceptable post-deployment. Disparity:\", disp_gender)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xti5Otn2b2FC",
        "outputId": "603425bd-5e2c-4130-8e02-a2e32634f346"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Monitoring: Simulate post-deployment monitoring --\n",
            "⚠️ Alert: Fairness threshold exceeded post-deployment. Disparity: 0.23708381171067738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n-- Privacy note --\")\n",
        "print(\"Patient IDs pseudonymized; mapping stored separately if needed under strict access controls.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tux0u_S3b5L3",
        "outputId": "f189f639-1bfe-4414-dc4f-3d30034f40e0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-- Privacy note --\n",
            "Patient IDs pseudonymized; mapping stored separately if needed under strict access controls.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-- Inclusivity feedback loop: appending targeted examples for 'Other' gender in Rural region --\")\n",
        "feedback = pd.DataFrame({\n",
        "    \"Patient_ID\": [1001, 1002],\n",
        "    \"Age\": [55, 60],\n",
        "    \"Gender\": [\"Other\", \"Other\"],\n",
        "    \"Region\": [\"Rural\", \"Rural\"],\n",
        "    \"BMI\": [26.5, 29.2],\n",
        "    \"Blood_Pressure\": [150, 142],\n",
        "    \"Cholesterol\": [230, 245],\n",
        "    \"Heart_Disease\": [1, 1]\n",
        "})\n",
        "data_updated = pd.concat([data, feedback], ignore_index=True)\n",
        "print(\"Updated dataset size:\", data_updated.shape)\n",
        "# prepare and retrain quickly on small combined data (demonstration only)\n",
        "X_up = prepare_features(data_updated, drop_sensitive=False)\n",
        "y_up = data_updated[\"Heart_Disease\"]\n",
        "Xu_train, Xu_test, yu_train, yu_test = train_test_split(X_up, y_up, test_size=0.3, random_state=42, stratify=y_up)\n",
        "model_updated = train_random_forest(Xu_train, yu_train)\n",
        "print(\"Retrained model on dataset with feedback — ready to re-evaluate fairness in next CI run.\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqfXpfKzb8OD",
        "outputId": "596ce2c5-2628-4540-a7ed-10e7638d4ec1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Inclusivity feedback loop: appending targeted examples for 'Other' gender in Rural region --\n",
            "Updated dataset size: (1002, 8)\n",
            "Retrained model on dataset with feedback — ready to re-evaluate fairness in next CI run.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== FINAL SUMMARY ===\")\n",
        "print(\"- Baseline accuracy and fairness measured.\")\n",
        "print(\"- Fairness gate implemented and triggered for Gender/Region in baseline.\")\n",
        "print(\"- Feature importance shown; sensitive features had small but non-zero contribution.\")\n",
        "print(\"- Mitigations tried: remove sensitive features and re-weighting; neither is guaranteed—use advanced fairness methods if required.\")\n",
        "print(\"- Deployment logging, pseudonymization, monitoring, and inclusivity feedback loop demonstrated.\")\n",
        "print(\"\\nSuggested next steps:\")\n",
        "print(\"1) Integrate automated fairness checks into CI/CD to block models failing thresholds.\")\n",
        "print(\"2) Evaluate advanced fairness algorithms (in-processing or post-processing) and threshold calibration.\")\n",
        "print(\"3) Use explainability (SHAP/LIME) to provide per-prediction explanations in deployment.\")\n",
        "print(\"4) Protect logs with encryption and access control; keep pseudonym mapping in secure vault.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8duNtpm3cCKW",
        "outputId": "960a9f4a-1225-44c2-9f26-c683adac1a98"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== FINAL SUMMARY ===\n",
            "- Baseline accuracy and fairness measured.\n",
            "- Fairness gate implemented and triggered for Gender/Region in baseline.\n",
            "- Feature importance shown; sensitive features had small but non-zero contribution.\n",
            "- Mitigations tried: remove sensitive features and re-weighting; neither is guaranteed—use advanced fairness methods if required.\n",
            "- Deployment logging, pseudonymization, monitoring, and inclusivity feedback loop demonstrated.\n",
            "\n",
            "Suggested next steps:\n",
            "1) Integrate automated fairness checks into CI/CD to block models failing thresholds.\n",
            "2) Evaluate advanced fairness algorithms (in-processing or post-processing) and threshold calibration.\n",
            "3) Use explainability (SHAP/LIME) to provide per-prediction explanations in deployment.\n",
            "4) Protect logs with encryption and access control; keep pseudonym mapping in secure vault.\n"
          ]
        }
      ]
    }
  ]
}