{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdz9wLQLBfAHGDTgEot/i9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/appliedcode/mthree-c422/blob/mthree-c422-dipti/Exercises/day-14/Threat_and_Security/Threat_and_Security_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Problem Statement** – AI Security, Threat Simulation, and Privacy-Preserving AI in Credit Card Fraud Detection\n",
        "\n",
        "You are working in the **security analytics division** of a global financial services company.\n",
        "Your team’s goal is to build, evaluate, and audit a **credit card fraud detection model** that not only detects fraudulent transactions but also complies with **AI security and privacy governance best practices**.\n",
        "\n",
        "Your responsibilities include:\n",
        "\n",
        "1. **Dataset Handling \\& Preprocessing**\n",
        "    - Load and analyze the **Credit Card Fraud dataset** containing anonymized transaction features.\n",
        "    - Prepare the data for binary classification (`fraud` vs `non-fraud`).\n",
        "2. **Threat Simulation**\n",
        "    - Simulate a **data poisoning attack** where a small percentage of fraud labels are intentionally flipped, testing the system’s resilience to adversarial manipulation.\n",
        "3. **Baseline Model Training**\n",
        "    - Train a machine learning classifier to detect fraud and evaluate its performance on the poisoned dataset.\n",
        "4. **Privacy Preservation**\n",
        "    - Apply a **privacy-preserving transformation** (e.g., partial feature masking or removal of certain sensitive transaction indicators) and retrain the model.\n",
        "    - Compare performance trade-offs between the original and privacy-preserved versions.\n",
        "5. **Auditing \\& Transparency**\n",
        "    - Use **SHAP** to explain the model’s decisions and uncover key features influencing fraud classification.\n",
        "    - Create an **audit log** recording performance metrics, poisoning parameters, and privacy measures taken.\n",
        "6. **Governance Report**\n",
        "    - Generate a report summarizing the threat simulation, model results, privacy steps, and interpretability findings, suitable for an internal security and compliance review.\n",
        "\n",
        "**Business Context:**\n",
        "Financial fraud models are prime targets for **adversarial attacks** and can inadvertently leak sensitive transaction patterns. Regulatory frameworks like **GDPR**, **PCI DSS**, and emerging **AI Act** mandate proper bias control, privacy safeguards, and detailed audit trails for AI systems used in financial services.\n",
        "\n",
        "***\n",
        "\n",
        "### **Dataset Collection Code**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Credit Card Fraud Detection dataset from Kaggle public source\n",
        "# If running in Colab, you must upload 'creditcard.csv' or fetch from Kaggle using API\n",
        "# For example:\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "\n",
        "df = pd.read_csv('creditcard.csv')\n",
        "\n",
        "# Features (X) and Target (y)\n",
        "X = df.drop(columns=['Class'])\n",
        "y = df['Class']  # 1 = Fraud, 0 = Non-Fraud\n",
        "\n",
        "print(\"Dataset shape:\", X.shape)\n",
        "print(\"Fraud distribution:\\n\", y.value_counts())\n",
        "\n",
        "# Preview the first few rows\n",
        "df.head()\n",
        "```\n",
        "\n",
        "\n",
        "***\n"
      ],
      "metadata": {
        "id": "AhgjQrIA-c4p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXNl5LGA-cG_",
        "outputId": "03904ac3-88ac-40fc-b8b0-90ce108198a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (265359, 30)\n",
            "Fraud class distribution:\n",
            " Class\n",
            "0.0    264879\n",
            "1.0       480\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Simulating label poisoning (2% of fraud labels flipped to class 0)...\n",
            "\n",
            "Baseline Model - Classification Report (With Poisoned Data):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9995    0.9999    0.9997     79467\n",
            "         1.0     0.9286    0.7376    0.8221       141\n",
            "\n",
            "    accuracy                         0.9994     79608\n",
            "   macro avg     0.9641    0.8687    0.9109     79608\n",
            "weighted avg     0.9994    0.9994    0.9994     79608\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Credit Card Fraud Detection - AI Security, Privacy, and Auditing\"\"\"\n",
        "\n",
        "# Install necessary packages\n",
        "!pip install shap scikit-learn pandas matplotlib -q\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import shap\n",
        "import datetime\n",
        "\n",
        "# -------------------------\n",
        "# 1. LOAD DATASET\n",
        "# -------------------------\n",
        "# NOTE: Ensure 'creditcard.csv' is uploaded to Colab or available in working directory\n",
        "# Dataset: https://www.kaggle.com/mlg-ulb/creditcardfraud\n",
        "\n",
        "df = pd.read_csv('creditcard.csv')\n",
        "\n",
        "# Drop rows with missing values in the 'Class' column\n",
        "df.dropna(subset=['Class'], inplace=True)\n",
        "\n",
        "\n",
        "X = df.drop(columns=['Class'])\n",
        "y = df['Class']  # Target: 1=Fraud, 0=Non-Fraud\n",
        "\n",
        "\n",
        "print(\"Dataset shape:\", X.shape)\n",
        "print(\"Fraud class distribution:\\n\", y.value_counts())\n",
        "\n",
        "# -------------------------\n",
        "# 2. SIMULATE DATA POISONING ATTACK\n",
        "# -------------------------\n",
        "def poison_labels(y, fraction=0.02, target_label=0):\n",
        "    \"\"\"\n",
        "    Flip labels for a fraction of fraud cases to normal (or vice versa) to simulate poisoning.\n",
        "    \"\"\"\n",
        "    y_poisoned = y.copy()\n",
        "    n_poison = int(fraction * len(y))\n",
        "    # Ensure indices are within bounds and unique\n",
        "    all_indices = y[y == 1 - target_label].index\n",
        "    if len(all_indices) < n_poison:\n",
        "        print(f\"Warning: Not enough samples of target label {1-target_label} to poison {fraction*100}%\")\n",
        "        n_poison = len(all_indices)\n",
        "    indices_to_poison = np.random.choice(all_indices, n_poison, replace=False)\n",
        "    y_poisoned.loc[indices_to_poison] = target_label\n",
        "    return y_poisoned\n",
        "\n",
        "\n",
        "print(\"\\nSimulating label poisoning (2% of fraud labels flipped to class 0)...\")\n",
        "# Identify fraud indices before poisoning\n",
        "fraud_indices = y[y == 1].index\n",
        "y_poisoned = y.copy()\n",
        "n_poison_fraud = int(0.02 * len(fraud_indices))\n",
        "indices_to_poison = np.random.choice(fraud_indices, n_poison_fraud, replace=False)\n",
        "y_poisoned.loc[indices_to_poison] = 0\n",
        "\n",
        "# -------------------------\n",
        "# 3. TRAIN-TEST SPLIT\n",
        "# -------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_poisoned, test_size=0.3, random_state=42, stratify=y_poisoned\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# 4. BASELINE MODEL TRAINING\n",
        "# -------------------------\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "print(\"\\nBaseline Model - Classification Report (With Poisoned Data):\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "# -------------------------\n",
        "# 5. PRIVACY PRESERVATION - FEATURE MASKING\n",
        "# -------------------------\n",
        "# Example: Drop \"Amount\" and \"Time\" columns as a simple anonymization step\n",
        "X_privacy = X.drop(columns=['Amount', 'Time'])\n",
        "X_train_priv, X_test_priv, y_train_priv, y_test_priv = train_test_split(\n",
        "    X_privacy, y_poisoned, test_size=0.3, random_state=42, stratify=y_poisoned\n",
        ")\n",
        "\n",
        "clf_priv = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf_priv.fit(X_train_priv, y_train_priv)\n",
        "y_pred_priv = clf_priv.predict(X_test_priv)\n",
        "\n",
        "print(\"\\nPrivacy-Preserved Model - Classification Report:\")\n",
        "print(classification_report(y_test_priv, y_pred_priv, digits=4))\n",
        "\n",
        "# -------------------------\n",
        "# 6. AUDIT LOGGING\n",
        "# -------------------------\n",
        "audit_log = {\n",
        "    \"timestamp\": datetime.datetime.now().isoformat(),\n",
        "    \"poisoning_fraction\": 0.02,\n",
        "    \"baseline_metrics\": classification_report(y_test, y_pred, output_dict=True),\n",
        "    \"privacy_preserved_metrics\": classification_report(y_test_priv, y_pred_priv, output_dict=True),\n",
        "    \"confusion_matrix_baseline\": confusion_matrix(y_test, y_pred).tolist(),\n",
        "    \"confusion_matrix_privacy\": confusion_matrix(y_test_priv, y_pred_priv).tolist()\n",
        "}\n",
        "\n",
        "print(\"\\n--- Security Audit Log ---\")\n",
        "print(audit_log)\n",
        "\n",
        "# -------------------------\n",
        "# 7. MODEL EXPLAINABILITY (SHAP)\n",
        "# -------------------------\n",
        "print(\"\\nGenerating SHAP explanation for baseline model...\")\n",
        "explainer = shap.TreeExplainer(clf)\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "\n",
        "# Print debugging information about shap_values\n",
        "print(f\"Type of shap_values: {type(shap_values)}\")\n",
        "if isinstance(shap_values, list):\n",
        "    print(f\"Length of shap_values list: {len(shap_values)}\")\n",
        "    for i, arr in enumerate(shap_values):\n",
        "        print(f\"Shape of shap_values[{i}]: {arr.shape}\")\n",
        "        if isinstance(arr, np.ndarray):\n",
        "            print(f\"First 5 elements of shap_values[{i}]: {arr.flatten()[:5]}\")\n",
        "else:\n",
        "    print(f\"Shape of shap_values: {shap_values.shape}\")\n",
        "    if isinstance(shap_values, np.ndarray):\n",
        "        print(f\"First 5 elements of shap_values: {shap_values.flatten()[:5]}\")\n",
        "\n",
        "\n",
        "# Summary plot for class 1 (fraud)\n",
        "# Ensure shap_values[1] has the correct shape (samples, features)\n",
        "if isinstance(shap_values, list) and len(shap_values) > 1 and isinstance(shap_values[1], np.ndarray) and shap_values[1].shape == X_test.shape:\n",
        "     shap.summary_plot(shap_values[1], X_test, plot_type=\"bar\")\n",
        "elif isinstance(shap_values, np.ndarray) and shap_values.shape == X_test.shape:\n",
        "     # This case is less likely for binary classification TreeExplainer output\n",
        "     print(\"Attempting to plot shap_values directly as its shape matches X_test.\")\n",
        "     shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
        "else:\n",
        "    print(\"\\nCould not generate SHAP summary plot due to unexpected shap_values structure or shape.\")\n",
        "    print(f\"Expected shape for plotting: {X_test.shape}\")\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# 8. COMPLIANCE / GOVERNANCE REPORT\n",
        "# -------------------------\n",
        "compliance_report = f\"\"\"\n",
        "CREDIT CARD FRAUD DETECTION - AI SECURITY & PRIVACY AUDIT\n",
        "--------------------------------------------------------\n",
        "Timestamp: {audit_log['timestamp']}\n",
        "\n",
        "Threat Simulation:\n",
        "- Simulated {audit_log['poisoning_fraction']*100:.1f}% label poisoning to test fraud model robustness.\n",
        "\n",
        "Model Performance:\n",
        "- Baseline Accuracy: {audit_log['baseline_metrics']['accuracy']:.4f}\n",
        "- Privacy-Preserved Accuracy: {audit_log['privacy_preserved_metrics']['accuracy']:.4f}\n",
        "\n",
        "Security Governance:\n",
        "- Evaluated model under data poisoning threat scenario.\n",
        "- Applied simple feature masking ('Amount', 'Time') to enhance privacy.\n",
        "\n",
        "Transparency:\n",
        "- SHAP feature importance analysis used to interpret influential features for fraud classification.\n",
        "\n",
        "Audit Readiness:\n",
        "- Metrics, confusion matrices, and parameters logged for review.\n",
        "- Supports compliance with GDPR, PCI DSS, and AI governance best practices.\n",
        "\"\"\"\n",
        "\n",
        "print(compliance_report)"
      ]
    }
  ]
}