{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/appliedcode/mthree-c422/blob/mthree-c422-Avantika/Multiple_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Practical Data Ingestion from Multiple Sources in Colab\n",
        "This lab is inspired by the image and will guide you through ingesting data from multiple sources **(CSV, JSON, REST API)**, cleaning and transforming the data, and producing a unified clean dataset using Python and pandas in Google Colab.\n",
        "\n",
        "Objectives\n",
        "- Ingest data from CSV, JSON, and REST API sources\n",
        "\n",
        "- Use a central “ingestion layer” (pandas) for data import\n",
        "\n",
        "- Apply cleaning and transformation steps modularly\n",
        "\n",
        "- Consolidate results into a single, unified output\n",
        "\n"
      ],
      "metadata": {
        "id": "RpsOG3v9XMxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Set Up Environment\n",
        "!pip install pandas requests -q"
      ],
      "metadata": {
        "id": "dDQaHeWjk73I"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Ingest Data from Multiple Sources\n",
        "# a. CSV File\n",
        "import pandas as pd\n",
        "\n",
        "csv_url = \"https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv\"\n",
        "df_csv = pd.read_csv(csv_url)\n",
        "print(\"CSV Data Sample:\")\n",
        "print(df_csv.head())\n",
        "\n",
        "# b. JSON File\n",
        "import json\n",
        "\n",
        "json_url = \"https://jsonplaceholder.typicode.com/users\"\n",
        "df_json = pd.read_json(json_url)\n",
        "print(\"\\nJSON Data Sample:\")\n",
        "print(df_json.head())\n",
        "\n",
        "#c. REST API\n",
        "import requests\n",
        "\n",
        "api_url = \"https://randomuser.me/api/?results=5\"\n",
        "response = requests.get(api_url)\n",
        "data = response.json()\n",
        "df_api = pd.json_normalize(data['results'])\n",
        "print(\"\\nREST API Data Sample:\")\n",
        "print(df_api.head())\n"
      ],
      "metadata": {
        "id": "1GR4c7Y4k_ny",
        "outputId": "c7cd5cb7-cab3-4f46-ad01-08162b74240e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV Data Sample:\n",
            "   sepal_length  sepal_width  petal_length  petal_width species\n",
            "0           5.1          3.5           1.4          0.2  setosa\n",
            "1           4.9          3.0           1.4          0.2  setosa\n",
            "2           4.7          3.2           1.3          0.2  setosa\n",
            "3           4.6          3.1           1.5          0.2  setosa\n",
            "4           5.0          3.6           1.4          0.2  setosa\n",
            "\n",
            "JSON Data Sample:\n",
            "   id              name   username                      email  \\\n",
            "0   1     Leanne Graham       Bret          Sincere@april.biz   \n",
            "1   2      Ervin Howell  Antonette          Shanna@melissa.tv   \n",
            "2   3  Clementine Bauch   Samantha         Nathan@yesenia.net   \n",
            "3   4  Patricia Lebsack   Karianne  Julianne.OConner@kory.org   \n",
            "4   5  Chelsey Dietrich     Kamren   Lucio_Hettinger@annie.ca   \n",
            "\n",
            "                                             address                  phone  \\\n",
            "0  {'street': 'Kulas Light', 'suite': 'Apt. 556',...  1-770-736-8031 x56442   \n",
            "1  {'street': 'Victor Plains', 'suite': 'Suite 87...    010-692-6593 x09125   \n",
            "2  {'street': 'Douglas Extension', 'suite': 'Suit...         1-463-123-4447   \n",
            "3  {'street': 'Hoeger Mall', 'suite': 'Apt. 692',...      493-170-9623 x156   \n",
            "4  {'street': 'Skiles Walks', 'suite': 'Suite 351...          (254)954-1289   \n",
            "\n",
            "         website                                            company  \n",
            "0  hildegard.org  {'name': 'Romaguera-Crona', 'catchPhrase': 'Mu...  \n",
            "1  anastasia.net  {'name': 'Deckow-Crist', 'catchPhrase': 'Proac...  \n",
            "2    ramiro.info  {'name': 'Romaguera-Jacobson', 'catchPhrase': ...  \n",
            "3       kale.biz  {'name': 'Robel-Corkery', 'catchPhrase': 'Mult...  \n",
            "4   demarco.info  {'name': 'Keebler LLC', 'catchPhrase': 'User-c...  \n",
            "\n",
            "REST API Data Sample:\n",
            "   gender                       email           phone            cell nat  \\\n",
            "0  female   suzy.caldwell@example.com    041-081-9818    081-694-5682  IE   \n",
            "1    male   daniel.howell@example.com    08-0462-7116    0404-424-194  AU   \n",
            "2    male  vitorino.ramos@example.com  (61) 8977-0778  (19) 2921-6992  BR   \n",
            "3    male  karl-ernst.rau@example.com    0959-7829636    0170-6002263  DE   \n",
            "4  female     meral.tasli@example.com  (171)-992-6041  (241)-392-6308  TR   \n",
            "\n",
            "  name.title  name.first name.last  location.street.number  \\\n",
            "0        Mrs        Suzy  Caldwell                    5891   \n",
            "1         Mr      Daniel    Howell                    3016   \n",
            "2         Mr    Vitorino     Ramos                    3758   \n",
            "3         Mr  Karl-Ernst       Rau                    5421   \n",
            "4         Ms       Meral     Taşlı                    8142   \n",
            "\n",
            "  location.street.name  ...  \\\n",
            "0        Pearse Street  ...   \n",
            "1   Stevens Creek Blvd  ...   \n",
            "2       Rua São Paulo   ...   \n",
            "3             Kirchweg  ...   \n",
            "4             Maçka Cd  ...   \n",
            "\n",
            "                                        login.sha256  \\\n",
            "0  26a7c28ab3bc69f9d73cae60f57259433f160ed5bce76f...   \n",
            "1  fe8f435e1b7a684f95332c9104688ba64f4d132a6f7040...   \n",
            "2  3d81152e06ea7d0ec7a92b65549670f601f64edc6a4e61...   \n",
            "3  c8267b22380dc1dfcad3266e9f9ca3f524febb1050113a...   \n",
            "4  f4065dbafcd73ea05b3838455e0a88854498549d66dd72...   \n",
            "\n",
            "                   dob.date dob.age           registered.date registered.age  \\\n",
            "0  1999-06-03T18:00:42.376Z      26  2015-06-01T18:43:41.502Z             10   \n",
            "1  1948-06-25T19:54:04.968Z      77  2020-03-01T08:06:29.647Z              5   \n",
            "2  1984-01-26T07:50:32.557Z      41  2017-07-04T17:37:02.149Z              8   \n",
            "3  1953-11-10T08:35:29.748Z      71  2002-05-13T13:20:39.847Z             23   \n",
            "4  1977-12-31T03:29:19.435Z      47  2003-05-24T21:13:17.222Z             22   \n",
            "\n",
            "  id.name         id.value                                     picture.large  \\\n",
            "0     PPS         0707479T  https://randomuser.me/api/portraits/women/13.jpg   \n",
            "1     TFN        007988441    https://randomuser.me/api/portraits/men/22.jpg   \n",
            "2     CPF   259.254.153-68     https://randomuser.me/api/portraits/men/5.jpg   \n",
            "3    SVNR  20 101153 R 150    https://randomuser.me/api/portraits/men/87.jpg   \n",
            "4                     None   https://randomuser.me/api/portraits/women/6.jpg   \n",
            "\n",
            "                                      picture.medium  \\\n",
            "0  https://randomuser.me/api/portraits/med/women/...   \n",
            "1  https://randomuser.me/api/portraits/med/men/22...   \n",
            "2  https://randomuser.me/api/portraits/med/men/5.jpg   \n",
            "3  https://randomuser.me/api/portraits/med/men/87...   \n",
            "4  https://randomuser.me/api/portraits/med/women/...   \n",
            "\n",
            "                                   picture.thumbnail  \n",
            "0  https://randomuser.me/api/portraits/thumb/wome...  \n",
            "1  https://randomuser.me/api/portraits/thumb/men/...  \n",
            "2  https://randomuser.me/api/portraits/thumb/men/...  \n",
            "3  https://randomuser.me/api/portraits/thumb/men/...  \n",
            "4  https://randomuser.me/api/portraits/thumb/wome...  \n",
            "\n",
            "[5 rows x 34 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Modular Cleaning/Transformation\n",
        "# Example: Clean and select specific columns from each\n",
        "\n",
        "# For CSV (Iris), let's only keep numeric columns and rename\n",
        "df_csv_clean = df_csv.rename(columns={'species':'source'}).dropna()\n",
        "\n",
        "# For JSON (User info), select name and email\n",
        "df_json_clean = df_json[['name', 'email']].copy()\n",
        "df_json_clean['source'] = 'json'\n",
        "\n",
        "# For API data (Random users), grab first/last name, email\n",
        "df_api_clean = pd.DataFrame()\n",
        "df_api_clean['name'] = df_api['name.first'] + \" \" + df_api['name.last']\n",
        "df_api_clean['email'] = df_api['email']\n",
        "df_api_clean['source'] = 'api'\n"
      ],
      "metadata": {
        "id": "MBtYPeyxlIti"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Prepare each cleaned DataFrame with identical columns\n",
        "\n",
        "common_cols = ['name', 'email', 'source',\n",
        "               'sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
        "\n",
        "# CSV (Iris) — rename species to name, add missing columns\n",
        "df_csv_clean = df_csv.rename(columns={'species': 'name'})\n",
        "df_csv_clean['email'] = None\n",
        "df_csv_clean['source'] = 'csv'\n",
        "for col in ['sepal_length','sepal_width','petal_length','petal_width']:\n",
        "    # numeric columns already exist\n",
        "    pass\n",
        "\n",
        "# JSON (Users) — add placeholder iris columns\n",
        "df_json_clean = df_json[['name','email']].copy()\n",
        "df_json_clean['source'] = 'json'\n",
        "for col in ['sepal_length','sepal_width','petal_length','petal_width']:\n",
        "    df_json_clean[col] = None\n",
        "\n",
        "# API (Random Users) — add placeholder iris columns\n",
        "df_api_clean = pd.DataFrame({\n",
        "    'name': df_api['name.first'] + ' ' + df_api['name.last'],\n",
        "    'email': df_api['email'],\n",
        "    'source': 'api'\n",
        "})\n",
        "for col in ['sepal_length','sepal_width','petal_length','petal_width']:\n",
        "    df_api_clean[col] = None\n",
        "\n",
        "# Step 5: Concatenate into unified DataFrame\n",
        "unified_df = pd.concat([\n",
        "    df_csv_clean[common_cols],\n",
        "    df_json_clean[common_cols],\n",
        "    df_api_clean[common_cols]\n",
        "], ignore_index=True)\n",
        "\n",
        "print(\"\\nUnified Clean Dataset Sample:\")\n",
        "print(unified_df.head(10))\n"
      ],
      "metadata": {
        "id": "pfcxyfiklLtx",
        "outputId": "49210922-e5ba-492d-dc33-a2e96b027658",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Unified Clean Dataset Sample:\n",
            "     name email source  sepal_length  sepal_width  petal_length  petal_width\n",
            "0  setosa  None    csv           5.1          3.5           1.4          0.2\n",
            "1  setosa  None    csv           4.9          3.0           1.4          0.2\n",
            "2  setosa  None    csv           4.7          3.2           1.3          0.2\n",
            "3  setosa  None    csv           4.6          3.1           1.5          0.2\n",
            "4  setosa  None    csv           5.0          3.6           1.4          0.2\n",
            "5  setosa  None    csv           5.4          3.9           1.7          0.4\n",
            "6  setosa  None    csv           4.6          3.4           1.4          0.3\n",
            "7  setosa  None    csv           5.0          3.4           1.5          0.2\n",
            "8  setosa  None    csv           4.4          2.9           1.4          0.2\n",
            "9  setosa  None    csv           4.9          3.1           1.5          0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-573786710.py:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  unified_df = pd.concat([\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Reflection\n",
        "- Identify which steps required the most standardization.\n",
        "The most standardization was needed in Step 4: Prepare each cleaned DataFrame with the same columns. This was due to the fact that the data from each source (CSV, JSON, API) had different column names, structures, and data types. For the data to exist together in one DataFrame, we needed to confirm they all contained the same set of columns with matching data types, even if that meant adding placeholder columns of None values for data that did not exist in that source.\n",
        "\n",
        "- What common problems occur when merging data of different shapes and sources?\n",
        "\n",
        "Inconsistent column names: Each source may use different names for the same type of data (e.g., \"species\" vs. \"name\").\n",
        "- Data types are different: A column can be a string in one source and a number in another source.\n",
        "- Missing columns: Some sources may not contain all of the columns necessary for the dataset\n",
        "Differences in structures of data: JSON and API data may be nested and will need to flatten the data to merge it together\n",
        "Issues cleaning data: Each source may have individual  issues with data quality (missing values, incorrect formats, etc.) that need to be addressed before merging the data together.\n",
        "\n",
        "- Why is a central ingestion and transformation layer important for reliability and scalability?\n",
        "\n",
        "**Reliability**: A central layer allows data from all sources to be processed in the same way based on the same rules and logic, thus reducing the chances of errors and inconsistencies in the resulting dataset.\n",
        "\n",
        "\n",
        "**Scalability**: When you add more data sources or increase the amount of data being processed, having a modular and central layer enables you to integrate the new data and scale without having to define the logic for each source. It also encourages code reusability and maintainability.\n",
        "\n",
        "\n",
        "**Maintainability**: Changes to existing data sources or cleansing requirements can occur in one spot within the central layer, allowing you to change the data sources with minimal effort and worry that your changes do not affect other parts of the entire process.\n",
        "\n",
        "\n",
        "**Reproducibility**: A clean and clearly-defined ingestion and transform layer ensures that all the steps taken to process the data are transparently displayed and reproducible when necessary, which is important for debugging and auditing."
      ],
      "metadata": {
        "id": "bot06-lplNtZ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}