{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "aceadca5",
      "metadata": {
        "id": "aceadca5"
      },
      "source": [
        "# ðŸ“Š Session 6: Text Classification\n",
        "\n",
        "**Learning Objectives:**\n",
        "- Understand different text classification algorithms (MultinomialNB, SVM, Logistic Regression)\n",
        "- Implement train/test split and cross-validation\n",
        "- Evaluate models using confusion matrix, F1-score, and other metrics\n",
        "- Practice multi-class text classification\n",
        "- Compare model performance and select the best approach\n",
        "\n",
        "**Dataset:** We'll work with news articles, product reviews, and custom datasets\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8271d38b",
      "metadata": {
        "id": "8271d38b"
      },
      "source": [
        "## ðŸ› ï¸ Setup and Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8da6b89",
      "metadata": {
        "id": "b8da6b89"
      },
      "source": [
        "ðŸŽ¬ Movie Reviews Sentiment Classification: Complete Solution\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                             classification_report, confusion_matrix, roc_curve, auc)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# 1. Create Sample Movie Reviews Dataset\n",
        "def create_movie_review_dataset():\n",
        "    # Positive reviews\n",
        "    positive_reviews = [\n",
        "        \"I absolutely loved this film! The story was compelling and the acting superb.\",\n",
        "        \"An amazing cinematic experience with brilliant performances.\",\n",
        "        \"A heartwarming story that touched me deeply.\",\n",
        "        \"Fantastic direction and stunning visuals. Highly recommended!\",\n",
        "        \"Top-notch screenplay and incredible soundtrack.\"\n",
        "    ]\n",
        "    # Negative reviews\n",
        "    negative_reviews = [\n",
        "        \"The movie was boring and slow-paced; I wouldnâ€™t recommend it.\",\n",
        "        \"Terrible acting and a predictable script ruined it for me.\",\n",
        "        \"Waste of time. The plot made no sense at all.\",\n",
        "        \"Poor editing and bad special effects spoiled the experience.\",\n",
        "        \"One of the worst movies I have ever watched.\"\n",
        "    ]\n",
        "    # Neutral reviews\n",
        "    neutral_reviews = [\n",
        "        \"It was an okay movie â€” nothing special but watchable.\",\n",
        "        \"The storyline was average and the characters were decent.\",\n",
        "        \"Some parts were entertaining, others quite dull.\",\n",
        "        \"Not great, not terrible, just an average film experience.\",\n",
        "        \"A middling movie, not memorable but not horrible either.\"\n",
        "    ]\n",
        "    texts = positive_reviews + negative_reviews + neutral_reviews\n",
        "    labels = (['Positive'] * len(positive_reviews) +\n",
        "              ['Negative'] * len(negative_reviews) +\n",
        "              ['Neutral'] * len(neutral_reviews))\n",
        "    return pd.DataFrame({'text': texts, 'sentiment': labels})\n",
        "\n",
        "df = create_movie_review_dataset()\n",
        "\n",
        "# 2. Text Preprocessing Pipeline\n",
        "class TextPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'http\\S+', '', text)\n",
        "        text = re.sub(r'\\S+@\\S+', '', text)\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "        text = text.strip()\n",
        "        return text\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        text = self.clean_text(text)\n",
        "        tokens = word_tokenize(text)\n",
        "        tokens = [t for t in tokens if t not in self.stop_words and len(t) > 2]\n",
        "        tokens = [self.lemmatizer.lemmatize(t) for t in tokens]\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "preprocessor = TextPreprocessor()\n",
        "df['processed_text'] = df['text'].apply(preprocessor.preprocess)\n",
        "\n",
        "# 3. Basic Text Classification Pipeline (Naive Bayes)\n",
        "\n",
        "# Vectorize text\n",
        "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1,2))\n",
        "X = vectorizer.fit_transform(df['processed_text'])\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(df['sentiment'])\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Train Naive Bayes\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = nb_model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"=== Naive Bayes Classification Report ===\")\n",
        "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize Confusion Matrix\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - Naive Bayes')\n",
        "plt.show()\n",
        "\n",
        "# 4. Model Comparison Framework with Cross-Validation\n",
        "\n",
        "models = {\n",
        "    'Naive Bayes': MultinomialNB(),\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
        "    'SVM (Linear)': SVC(kernel='linear', probability=True),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42),\n",
        "    'K-Nearest Neighbors': KNeighborsClassifier()\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining and cross-validating {name} ...\")\n",
        "    pipeline = Pipeline([\n",
        "        ('vect', TfidfVectorizer(max_features=1000, ngram_range=(1,2))),\n",
        "        ('clf', model)\n",
        "    ])\n",
        "    scores = cross_val_score(pipeline, df['processed_text'], y, cv=cv, scoring='f1_macro')\n",
        "    print(f\"F1 Macro CV Mean: {scores.mean():.3f} | Std: {scores.std():.3f}\")\n",
        "    results[name] = scores.mean()\n",
        "\n",
        "# Visualize Model Comparison\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(x=list(results.keys()), y=list(results.values()))\n",
        "plt.ylabel('Mean F1 Macro Score (5-fold CV)')\n",
        "plt.title('Model Comparison')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# 5. Advanced Feature Engineering - Comparing Vectorizers\n",
        "\n",
        "vectorizers = {\n",
        "    'CountVectorizer (unigrams)': CountVectorizer(max_features=1000, ngram_range=(1,1)),\n",
        "    'TF-IDF (unigrams)': TfidfVectorizer(max_features=1000, ngram_range=(1,1)),\n",
        "    'TF-IDF (bigrams)': TfidfVectorizer(max_features=1000, ngram_range=(2,2)),\n",
        "    'TF-IDF (unigrams + bigrams)': TfidfVectorizer(max_features=1000, ngram_range=(1,2)),\n",
        "    'TF-IDF (char 3-5 grams)': TfidfVectorizer(max_features=1000, analyzer='char', ngram_range=(3,5))\n",
        "}\n",
        "\n",
        "feature_results = {}\n",
        "model_fe = LogisticRegression(max_iter=1000)\n",
        "for name, vect in vectorizers.items():\n",
        "    X_vect = vect.fit_transform(df['processed_text'])\n",
        "    scores = cross_val_score(model_fe, X_vect, y, cv=cv, scoring='f1_macro')\n",
        "    print(f\"{name}: F1 Macro CV Mean={scores.mean():.3f}\")\n",
        "    feature_results[name] = scores.mean()\n",
        "\n",
        "# Plot Feature Engineering Results\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=list(feature_results.keys()), y=list(feature_results.values()))\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('Mean F1 Macro (Cross-Validation)')\n",
        "plt.title('Feature Extraction Method Comparison')\n",
        "plt.show()\n",
        "\n",
        "# 6. Sentiment Classification with Prediction Confidence (Logistic Regression)\n",
        "\n",
        "pipeline_lr = Pipeline([\n",
        "    ('vect', TfidfVectorizer(max_features=1000, ngram_range=(1,2))),\n",
        "    ('clf', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "pipeline_lr.fit(df['processed_text'], y)\n",
        "y_proba = pipeline_lr.predict_proba(df['processed_text'])\n",
        "max_confidence = np.max(y_proba, axis=1)\n",
        "\n",
        "# Confidence Distribution Plot\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.hist(max_confidence, bins=20, alpha=0.7)\n",
        "plt.xlabel('Prediction Confidence')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Prediction Confidence Distribution (Logistic Regression)')\n",
        "plt.show()\n",
        "\n",
        "# Predict with confidence function\n",
        "def predict_with_confidence(text, pipeline, label_encoder):\n",
        "    text_proc = preprocessor.preprocess(text)\n",
        "    probs = pipeline.predict_proba([text_proc])[0]\n",
        "    pred_idx = np.argmax(probs)\n",
        "    pred_label = label_encoder.inverse_transform([pred_idx])[0]\n",
        "    confidence = probs[pred_idx]\n",
        "    return pred_label, confidence\n",
        "\n",
        "# Test on new reviews\n",
        "test_texts = [\n",
        "    \"What a fantastic movie! Loved every minute of it.\",\n",
        "    \"It was dull and uninteresting. Not worth watching.\",\n",
        "    \"Some scenes were good, but overall just okay.\"\n",
        "]\n",
        "\n",
        "for txt in test_texts:\n",
        "    pred, conf = predict_with_confidence(txt, pipeline_lr, le)\n",
        "    print(f\"Review: {txt}\\nPredicted Sentiment: {pred}, Confidence: {conf:.3f}\\n\")\n",
        "\n",
        "# 7. Hyperparameter Tuning Example with GridSearchCV (Logistic Regression)\n",
        "\n",
        "param_grid = {\n",
        "    'clf__C': [0.01, 0.1, 1, 10],\n",
        "    'clf__penalty': ['l2'],\n",
        "    'clf__solver': ['lbfgs']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipeline_lr, param_grid, cv=cv, scoring='f1_macro', n_jobs=-1)\n",
        "grid_search.fit(df['processed_text'], y)\n",
        "\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(f\"Best cross-validated F1 Macro: {grid_search.best_score_:.3f}\")\n",
        "\n",
        "# Use best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Done! You now have a full working pipeline for movie reviews sentiment classification.\n"
      ],
      "metadata": {
        "id": "s2_1WQ91oFd6"
      },
      "id": "s2_1WQ91oFd6",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}