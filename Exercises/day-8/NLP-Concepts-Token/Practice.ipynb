{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/appliedcode/mthree-c422/blob/main/Exercises/day-8/NLP-Concepts-Token/Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring Core NLP Concepts\n",
        "Welcome to this hands-on NLP Colab lab! You will work through key tasks—tokenization, POS tagging, stemming, stop-word filtering, vocabulary matching, lemmatization, dependency parsing, NER, and intent classification—using Python libraries. Follow the instructions and complete the exercises."
      ],
      "metadata": {
        "id": "sDeW9pllWu7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install --quiet nltk spacy textblob sklearn\n",
        "\n",
        "# Download NLTK data and spaCy model\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "!python -m spacy download en_core_web_sm -q\n"
      ],
      "metadata": {
        "id": "yMULvewKWrzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Tokenization\n",
        "# Goal: Split text into tokens (words and punctuation).\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "text = \"Natural Language Processing enables machines to understand human language.\"\n",
        "print(\"Sentences:\", sent_tokenize(text))\n",
        "print(\"Tokens:\", word_tokenize(text))"
      ],
      "metadata": {
        "id": "vNgJ7l_9Wzfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 1.1: Tokenize the following paragraph into words and sentences:\n",
        "\n",
        "paragraph = \"Machine learning models power many NLP tasks. They learn patterns from data!\"\n"
      ],
      "metadata": {
        "id": "Iwanu3kzW3KU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Part-of-Speech Tagging\n",
        "# Goal: Assign grammatical tags to each token.\n",
        "import nltk\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(pos_tags)\n"
      ],
      "metadata": {
        "id": "VaJMjJLOXCV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2.1: Tag POS for tokens from your Exercise 1.1."
      ],
      "metadata": {
        "id": "rjdfixgHXLAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Stemming\n",
        "# Goal: Reduce words to their root forms (may be non-dictionary).\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "words = [\"running\", \"runs\", \"ran\", \"easily\", \"fairly\"]\n",
        "print({w: stemmer.stem(w) for w in words})\n"
      ],
      "metadata": {
        "id": "8Q_ASJ2qXMJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3.1: Stem the tokens from your Exercise 1.1.\n",
        "\n"
      ],
      "metadata": {
        "id": "KpJNVgRhXTjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Stop-Word Filtering\n",
        "# Goal: Remove common, low-value words.\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = word_tokenize(text.lower())\n",
        "filtered = [w for w in tokens if w.isalpha() and w not in stop_words]\n",
        "print(filtered)"
      ],
      "metadata": {
        "id": "kBo5Lo3hXVPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 4.1: Filter stop words from your Exercise 1.1 tokens.\n",
        "\n"
      ],
      "metadata": {
        "id": "_CVnLYVWXakJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Vocabulary Matching\n",
        "# Goal: Check tokens against a predefined vocabulary.\n",
        "\n",
        "vocab = {\"natural\", \"language\", \"machine\", \"data\", \"processing\"}\n",
        "tokens = [w.lower() for w in word_tokenize(text)]\n",
        "in_vocab = [w for w in tokens if w.isalpha() and w in vocab]\n",
        "print(\"In-vocab tokens:\", in_vocab)\n",
        "print(\"OOV tokens:\", [w for w in tokens if w.isalpha() and w not in vocab])\n"
      ],
      "metadata": {
        "id": "IYEs9fy5XcVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 5.1: Define your own small vocabulary and classify tokens from Exercise 1.1 into in-vocab vs. out-of-vocab."
      ],
      "metadata": {
        "id": "YsKmsc4AXgXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Lemmatization\n",
        "# Goal: Convert words to their dictionary form.\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [\"running\", \"better\", \"wolves\"]\n",
        "print({w: lemmatizer.lemmatize(w) for w in words})\n",
        "# For verbs:\n",
        "print(\"run (verb):\", lemmatizer.lemmatize(\"running\", pos='v'))\n"
      ],
      "metadata": {
        "id": "RDcsQ-_CXjyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 6.1: Lemmatize tokens from Exercise 1.1 (both default and verb POS).\n",
        "\n"
      ],
      "metadata": {
        "id": "g74PrNQ0XqWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Dependency Parsing\n",
        "# Goal: Identify syntactic relationships between tokens.\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "    print(token.text, token.dep_, token.head.text)\n"
      ],
      "metadata": {
        "id": "4drXo7zjXss9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 7.1: Parse the sentence “They learn patterns from data” and list each token’s dependency label and head."
      ],
      "metadata": {
        "id": "LmRo7ZwEXvGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Named-Entity Recognition (NER)\n",
        "# Goal: Extract real-world entities from text.\n",
        "doc = nlp(\"Google was founded in 1998 by Larry Page and Sergey Brin in California.\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "metadata": {
        "id": "k78PeFooXxwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exercise 8.1: Run NER on this sentence and add at least two more sentences of your own.\n",
        "\n"
      ],
      "metadata": {
        "id": "bwBm0NMJX3WN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}