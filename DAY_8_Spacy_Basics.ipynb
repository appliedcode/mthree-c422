{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnGG3hoZP05x1HnfHWJBzf"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwNDS23fVapk",
        "outputId": "1a7c7ed1-1f76-4063-8f42-92b6f4c628f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla PROPN nsubj\n",
            "is AUX aux\n",
            "looking VERB ROOT\n",
            "at ADP prep\n",
            "buying VERB pcomp\n",
            "U.S. PROPN dobj\n",
            "startup VERB advcl\n",
            "for ADP prep\n",
            "$ SYM quantmod\n",
            "6 NUM compound\n",
            "million NUM pobj\n"
          ]
        }
      ],
      "source": [
        "# Import spaCy and load the language library\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create a Doc object\n",
        "doc = nlp(u'Tesla is looking at buying U.S. startup for $6 million')\n",
        "\n",
        "# Print each token separately\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_, token.dep_)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipeline\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isfF4fkiWU29",
        "outputId": "5200f655-997c-4bc7-cdb5-789a5714906a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x7e0d6e29ae10>),\n",
              " ('tagger', <spacy.pipeline.tagger.Tagger at 0x7e0d6e29ba10>),\n",
              " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7e0e4c7cb3e0>),\n",
              " ('attribute_ruler',\n",
              "  <spacy.pipeline.attributeruler.AttributeRuler at 0x7e0d6dfee050>),\n",
              " ('lemmatizer',\n",
              "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x7e0d6e02d3d0>),\n",
              " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x7e0e4c746c70>)]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2gYLIxIWbBi",
        "outputId": "d4b92af7-98f6-464d-e121-d01978e455d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc2 = nlp(u\"Tesla isn't   looking into startups anymore.\")\n",
        "\n",
        "for token in doc2:\n",
        "    print(token.text, token.pos_, token.dep_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nx0HIxmCWc8K",
        "outputId": "f06da1f6-1f21-4f5e-8f12-834cf7af38d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla PROPN nsubj\n",
            "is AUX aux\n",
            "n't PART neg\n",
            "   SPACE dep\n",
            "looking VERB ROOT\n",
            "into ADP prep\n",
            "startups NOUN pobj\n",
            "anymore ADV advmod\n",
            ". PUNCT punct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPvNRN6PXy3h",
        "outputId": "98250814-31e6-446b-93b1-89111b87e528"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tesla isn't   looking into startups anymore."
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc2[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q59vWGTPX5ex",
        "outputId": "c124c7a5-7bcb-4cbb-bba6-6d83bfe130f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tesla"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(doc2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUmQMMJXX7da",
        "outputId": "d1655238-6b07-49df-adac-54599e659963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.doc.Doc"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc2[0].pos_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "5DKypxzJX_03",
        "outputId": "1695b480-25fc-49ca-ef7e-eefed0505fd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'PROPN'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc2[0].dep_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "zoEEHahqYMT_",
        "outputId": "6da7bbef-526e-4813-d47e-dbe840b1c415"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'nsubj'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.explain('PROPN')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ZVxF9lYCYOJx",
        "outputId": "c01ededa-3e3a-4e0e-8dae-3b2881f91621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'proper noun'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.explain('nsubj')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "3o4_0XBRYSOR",
        "outputId": "d88a9590-d4f2-4047-a492-d63266361fd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'nominal subject'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmas (the base form of the word):\n",
        "print(doc2[4].text)\n",
        "print(doc2[4].lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruC2aRHIYWIw",
        "outputId": "6cdfe174-1273-45b9-fb67-0ac458d34d98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "looking\n",
            "look\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple Parts-of-Speech & Detailed Tags:\n",
        "print(doc2[4].pos_)\n",
        "print(doc2[4].tag_ + ' / ' + spacy.explain(doc2[4].tag_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9SRSmwrYa7A",
        "outputId": "0b7c7dc5-8e54-467a-dc9f-e3967c69c707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VERB\n",
            "VBG / verb, gerund or present participle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Shapes:\n",
        "print(doc2[0].text+': '+doc2[0].shape_)\n",
        "print(doc[5].text+' : '+doc[5].shape_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOzkVTgmYftD",
        "outputId": "fddde061-e10a-4fc1-b563-2b3020403eb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla: Xxxxx\n",
            "U.S. : X.X.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Boolean Values:\n",
        "print(doc2[0].is_alpha)\n",
        "print(doc2[0].is_stop)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9GzzWxRYlEd",
        "outputId": "2ee086d7-1443-4a63-c484-8dc41becfc28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc3 = nlp(u'Although commmonly attributed to John Lennon from his song \"Beautiful Boy\", \\\n",
        "the phrase \"Life is what happens to us while we are making other plans\" was written by \\\n",
        "cartoonist Allen Saunders and published in Reader\\'s Digest in 1957, when Lennon was 17.')\n",
        "life_quote = doc3[16:30]\n",
        "print(life_quote)\n",
        "type(life_quote)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6aQAPRxYpyh",
        "outputId": "ce69f533-4205-4a1d-9261-15d49cc006e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Life is what happens to us while we are making other plans\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.span.Span"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc4 = nlp(u'This is the first sentence. This is another sentence. This is the last sentence.')\n",
        "for sent in doc4.sents:\n",
        "    print(sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yMjnctJZAEn",
        "outputId": "0415e070-226e-4469-eb19-4b8d28e7d7d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the first sentence.\n",
            "This is another sentence.\n",
            "This is the last sentence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc4[6].is_sent_start"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONRBa3O6ZCV8",
        "outputId": "513e1e3c-e0ca-4128-a545-dff068903290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP"
      ],
      "metadata": {
        "id": "5dI99aNZZZbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages (run this cell if packages are not already installed)\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package):\n",
        "    \"\"\"Install a package using pip\"\"\"\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "        print(f\"✓ Successfully installed {package}\")\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(f\"✗ Failed to install {package}\")\n",
        "\n",
        "# Core NLP packages\n",
        "packages = [\n",
        "    \"nltk==3.8.1\",\n",
        "    \"spacy==3.7.2\",\n",
        "    \"scikit-learn==1.3.2\",\n",
        "    \"transformers==4.35.2\",\n",
        "    \"datasets==2.14.6\",\n",
        "    \"torch==2.1.1\",\n",
        "    \"pandas==2.0.3\",\n",
        "    \"matplotlib==3.7.2\",\n",
        "    \"seaborn==0.12.2\",\n",
        "    \"wordcloud==1.9.2\",\n",
        "    \"textblob==0.17.1\"\n",
        "]\n",
        "\n",
        "# Uncomment the next lines to install packages\n",
        "# for package in packages:\n",
        "#     install_package(package)\n",
        "\n",
        "print(\"Package installation section complete!\")\n",
        "print(\"Note: Uncomment the installation loop above if you need to install packages.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7X3ltFMZbdr",
        "outputId": "4f53bd5d-1595-4222-9ebf-5ff412d18819"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Package installation section complete!\n",
            "Note: Uncomment the installation loop above if you need to install packages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify imports and download NLTK data\n",
        "import nltk\n",
        "import spacy\n",
        "import sklearn\n",
        "import transformers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✓ All core packages imported successfully!\")\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk_downloads = [\n",
        "    'punkt',\n",
        "    'stopwords',\n",
        "    'vader_lexicon',\n",
        "    'wordnet',\n",
        "    'averaged_perceptron_tagger',\n",
        "    'omw-1.4'\n",
        "]\n",
        "\n",
        "print(\"\\nDownloading NLTK data...\")\n",
        "for data in nltk_downloads:\n",
        "    try:\n",
        "        nltk.download(data, quiet=True)\n",
        "        print(f\"✓ Downloaded {data}\")\n",
        "    except:\n",
        "        print(f\"✗ Failed to download {data}\")\n",
        "\n",
        "print(\"\\n🎉 Environment setup complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8gDKLZ3Zn68",
        "outputId": "2601e86b-810e-4d66-b879-d7d2b9cd644c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ All core packages imported successfully!\n",
            "\n",
            "Downloading NLTK data...\n",
            "✓ Downloaded punkt\n",
            "✓ Downloaded stopwords\n",
            "✓ Downloaded vader_lexicon\n",
            "✓ Downloaded wordnet\n",
            "✓ Downloaded averaged_perceptron_tagger\n",
            "✓ Downloaded omw-1.4\n",
            "\n",
            "🎉 Environment setup complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's start with a simple example to demonstrate basic NLP concepts\n",
        "\n",
        "# Sample text data\n",
        "sample_texts = [\n",
        "    \"I love this product! It's amazing and works perfectly.\",\n",
        "    \"This is the worst purchase I've ever made. Terrible quality!\",\n",
        "    \"The product is okay, nothing special but gets the job done.\",\n",
        "    \"Absolutely fantastic! Would recommend to everyone.\",\n",
        "    \"Not bad, could be better but decent for the price.\"\n",
        "]\n",
        "\n",
        "print(\"Sample Texts:\")\n",
        "print(\"=\" * 50)\n",
        "for i, text in enumerate(sample_texts, 1):\n",
        "    print(f\"{i}. {text}\")\n",
        "\n",
        "# Basic text analysis\n",
        "print(f\"\\nBasic Statistics:\")\n",
        "print(f\"Number of texts: {len(sample_texts)}\")\n",
        "print(f\"Total characters: {sum(len(text) for text in sample_texts)}\")\n",
        "print(f\"Average text length: {sum(len(text) for text in sample_texts) / len(sample_texts):.1f} characters\")\n",
        "\n",
        "# Word frequency analysis\n",
        "all_words = []\n",
        "for text in sample_texts:\n",
        "    # Simple tokenization (split by spaces and remove punctuation)\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    all_words.extend(words)\n",
        "\n",
        "word_freq = Counter(all_words)\n",
        "print(f\"\\nTop 10 Most Common Words:\")\n",
        "for word, count in word_freq.most_common(10):\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "# Basic sentiment indicators\n",
        "positive_words = ['love', 'amazing', 'perfectly', 'fantastic', 'recommend']\n",
        "negative_words = ['worst', 'terrible', 'bad']\n",
        "\n",
        "print(f\"\\nBasic Sentiment Analysis:\")\n",
        "for i, text in enumerate(sample_texts, 1):\n",
        "    text_lower = text.lower()\n",
        "    pos_count = sum(1 for word in positive_words if word in text_lower)\n",
        "    neg_count = sum(1 for word in negative_words if word in text_lower)\n",
        "\n",
        "    if pos_count > neg_count:\n",
        "        sentiment = \"Positive\"\n",
        "    elif neg_count > pos_count:\n",
        "        sentiment = \"Negative\"\n",
        "    else:\n",
        "        sentiment = \"Neutral\"\n",
        "\n",
        "    print(f\"Text {i}: {sentiment} (pos: {pos_count}, neg: {neg_count})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5klUbtrZ6LZ",
        "outputId": "bc07d5fc-9597-4a91-9a85-fd45540d67eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Texts:\n",
            "==================================================\n",
            "1. I love this product! It's amazing and works perfectly.\n",
            "2. This is the worst purchase I've ever made. Terrible quality!\n",
            "3. The product is okay, nothing special but gets the job done.\n",
            "4. Absolutely fantastic! Would recommend to everyone.\n",
            "5. Not bad, could be better but decent for the price.\n",
            "\n",
            "Basic Statistics:\n",
            "Number of texts: 5\n",
            "Total characters: 273\n",
            "Average text length: 54.6 characters\n",
            "\n",
            "Top 10 Most Common Words:\n",
            "the: 4\n",
            "i: 2\n",
            "this: 2\n",
            "product: 2\n",
            "is: 2\n",
            "but: 2\n",
            "love: 1\n",
            "it: 1\n",
            "s: 1\n",
            "amazing: 1\n",
            "\n",
            "Basic Sentiment Analysis:\n",
            "Text 1: Positive (pos: 3, neg: 0)\n",
            "Text 2: Negative (pos: 0, neg: 2)\n",
            "Text 3: Neutral (pos: 0, neg: 0)\n",
            "Text 4: Positive (pos: 2, neg: 0)\n",
            "Text 5: Negative (pos: 0, neg: 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sample datasets for the course\n",
        "import os\n",
        "\n",
        "# Create data directory\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# 1. Sample SMS Spam Dataset\n",
        "sms_data = [\n",
        "    (\"ham\", \"Hey, are we still on for lunch today?\"),\n",
        "    (\"spam\", \"URGENT! You've won $1000! Click here now!\"),\n",
        "    (\"ham\", \"Can you pick up milk on your way home?\"),\n",
        "    (\"spam\", \"FREE iPhone! Limited time offer! Call now!\"),\n",
        "    (\"ham\", \"Meeting moved to 3pm tomorrow\"),\n",
        "    (\"spam\", \"Congratulations! You've been selected for a special offer!\"),\n",
        "    (\"ham\", \"Thanks for the birthday wishes!\"),\n",
        "    (\"spam\", \"SALE ALERT: 90% off everything! Don't miss out!\"),\n",
        "    (\"ham\", \"Running late, be there in 10 minutes\"),\n",
        "    (\"spam\", \"You owe $500 in taxes. Pay immediately or face legal action!\")\n",
        "]\n",
        "\n",
        "sms_df = pd.DataFrame(sms_data, columns=['label', 'message'])\n",
        "sms_df.to_csv('data/sms_spam_sample.csv', index=False)\n",
        "\n",
        "print(\"SMS Spam Dataset:\")\n",
        "print(sms_df)\n",
        "print(f\"\\nDataset shape: {sms_df.shape}\")\n",
        "print(f\"Label distribution:\\n{sms_df['label'].value_counts()}\")\n",
        "\n",
        "# 2. Sample Movie Reviews Dataset\n",
        "movie_reviews = [\n",
        "    (\"positive\", \"This movie was absolutely fantastic! Great acting and storyline.\"),\n",
        "    (\"negative\", \"Boring and predictable. Waste of time and money.\"),\n",
        "    (\"positive\", \"Brilliant cinematography and outstanding performances.\"),\n",
        "    (\"negative\", \"Poor script and terrible direction. Very disappointed.\"),\n",
        "    (\"positive\", \"Highly recommend! One of the best films this year.\"),\n",
        "    (\"negative\", \"Confusing plot and weak character development.\"),\n",
        "    (\"positive\", \"Amazing visual effects and compelling story.\"),\n",
        "    (\"negative\", \"Overrated and underwhelming. Expected much more.\"),\n",
        "    (\"positive\", \"Perfect blend of comedy and drama. Loved every minute!\"),\n",
        "    (\"negative\", \"Slow paced and lacks substance. Not worth watching.\")\n",
        "]\n",
        "\n",
        "reviews_df = pd.DataFrame(movie_reviews, columns=['sentiment', 'review'])\n",
        "reviews_df.to_csv('data/movie_reviews_sample.csv', index=False)\n",
        "\n",
        "print(f\"\\nMovie Reviews Dataset:\")\n",
        "print(reviews_df)\n",
        "print(f\"\\nDataset shape: {reviews_df.shape}\")\n",
        "print(f\"Sentiment distribution:\\n{reviews_df['sentiment'].value_counts()}\")\n",
        "\n",
        "# 3. Sample News Headlines Dataset\n",
        "news_headlines = [\n",
        "    \"Apple announces new iPhone with revolutionary camera technology\",\n",
        "    \"Stock market reaches all-time high amid economic recovery\",\n",
        "    \"Scientists discover new species in Amazon rainforest\",\n",
        "    \"Local restaurant wins prestigious culinary award\",\n",
        "    \"Tech company Microsoft invests in renewable energy projects\",\n",
        "    \"Weather forecast predicts heavy rainfall this weekend\",\n",
        "    \"University researchers develop breakthrough medical treatment\",\n",
        "    \"Amazon expands delivery services to rural areas\",\n",
        "    \"New study reveals benefits of regular exercise\",\n",
        "    \"Government announces new environmental protection policies\"\n",
        "]\n",
        "\n",
        "news_df = pd.DataFrame({'headline': news_headlines})\n",
        "news_df.to_csv('data/news_headlines_sample.csv', index=False)\n",
        "\n",
        "print(f\"\\nNews Headlines Dataset:\")\n",
        "print(news_df)\n",
        "print(f\"\\nDataset shape: {news_df.shape}\")\n",
        "\n",
        "print(f\"\\n✅ Sample datasets created and saved to 'data/' directory!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maIGGU4JaaNW",
        "outputId": "92d62e53-5fa1-4f99-eb23-14df0cca46ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SMS Spam Dataset:\n",
            "  label                                            message\n",
            "0   ham              Hey, are we still on for lunch today?\n",
            "1  spam          URGENT! You've won $1000! Click here now!\n",
            "2   ham             Can you pick up milk on your way home?\n",
            "3  spam         FREE iPhone! Limited time offer! Call now!\n",
            "4   ham                      Meeting moved to 3pm tomorrow\n",
            "5  spam  Congratulations! You've been selected for a sp...\n",
            "6   ham                    Thanks for the birthday wishes!\n",
            "7  spam    SALE ALERT: 90% off everything! Don't miss out!\n",
            "8   ham               Running late, be there in 10 minutes\n",
            "9  spam  You owe $500 in taxes. Pay immediately or face...\n",
            "\n",
            "Dataset shape: (10, 2)\n",
            "Label distribution:\n",
            "label\n",
            "ham     5\n",
            "spam    5\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Movie Reviews Dataset:\n",
            "  sentiment                                             review\n",
            "0  positive  This movie was absolutely fantastic! Great act...\n",
            "1  negative   Boring and predictable. Waste of time and money.\n",
            "2  positive  Brilliant cinematography and outstanding perfo...\n",
            "3  negative  Poor script and terrible direction. Very disap...\n",
            "4  positive  Highly recommend! One of the best films this y...\n",
            "5  negative     Confusing plot and weak character development.\n",
            "6  positive       Amazing visual effects and compelling story.\n",
            "7  negative   Overrated and underwhelming. Expected much more.\n",
            "8  positive  Perfect blend of comedy and drama. Loved every...\n",
            "9  negative  Slow paced and lacks substance. Not worth watc...\n",
            "\n",
            "Dataset shape: (10, 2)\n",
            "Sentiment distribution:\n",
            "sentiment\n",
            "positive    5\n",
            "negative    5\n",
            "Name: count, dtype: int64\n",
            "\n",
            "News Headlines Dataset:\n",
            "                                            headline\n",
            "0  Apple announces new iPhone with revolutionary ...\n",
            "1  Stock market reaches all-time high amid econom...\n",
            "2  Scientists discover new species in Amazon rain...\n",
            "3   Local restaurant wins prestigious culinary award\n",
            "4  Tech company Microsoft invests in renewable en...\n",
            "5  Weather forecast predicts heavy rainfall this ...\n",
            "6  University researchers develop breakthrough me...\n",
            "7    Amazon expands delivery services to rural areas\n",
            "8     New study reveals benefits of regular exercise\n",
            "9  Government announces new environmental protect...\n",
            "\n",
            "Dataset shape: (10, 1)\n",
            "\n",
            "✅ Sample datasets created and saved to 'data/' directory!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 1: Text Statistics and Basic Analysis\n",
        "#\n",
        "# Task: Write a function that takes a text string and returns:\n",
        "# 1. Number of words\n",
        "# 2. Number of sentences (assume sentences end with '.', '!', or '?')\n",
        "# 3. Average word length\n",
        "# 4. Most frequent word\n",
        "# 5. Number of unique words\n",
        "\n",
        "def analyze_text(text):\n",
        "    \"\"\"\n",
        "    Analyze basic statistics of a text string.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to analyze\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing text statistics\n",
        "    \"\"\"\n",
        "    # TODO: Implement this function\n",
        "    # Hint: Use string methods, regular expressions, and Counter\n",
        "\n",
        "    # Your code here\n",
        "    pass\n",
        "\n",
        "# Test your function with this sample text\n",
        "sample_text = \"\"\"\n",
        "Natural Language Processing is a fascinating field of artificial intelligence.\n",
        "It combines computational linguistics with machine learning and deep learning.\n",
        "NLP enables computers to understand, interpret, and generate human language in a valuable way!\n",
        "\"\"\"\n",
        "\n",
        "# Uncomment and run when you've implemented the function\n",
        "# result = analyze_text(sample_text)\n",
        "# print(\"Text Analysis Results:\")\n",
        "# for key, value in result.items():\n",
        "#     print(f\"{key}: {value}\")\n",
        "\n",
        "# Solution (run this cell to see the solution)\n",
        "def analyze_text_solution(text):\n",
        "    \"\"\"Complete solution for text analysis function\"\"\"\n",
        "    import re\n",
        "    from collections import Counter\n",
        "\n",
        "    # Clean text and split into words\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "\n",
        "    # Count sentences\n",
        "    sentences = re.findall(r'[.!?]+', text)\n",
        "\n",
        "    # Calculate statistics\n",
        "    num_words = len(words)\n",
        "    num_sentences = len(sentences)\n",
        "    avg_word_length = sum(len(word) for word in words) / num_words if num_words > 0 else 0\n",
        "\n",
        "    # Find most frequent word\n",
        "    word_freq = Counter(words)\n",
        "    most_frequent = word_freq.most_common(1)[0] if words else (\"\", 0)\n",
        "\n",
        "    # Count unique words\n",
        "    unique_words = len(set(words))\n",
        "\n",
        "    return {\n",
        "        \"word_count\": num_words,\n",
        "        \"sentence_count\": num_sentences,\n",
        "        \"average_word_length\": round(avg_word_length, 2),\n",
        "        \"most_frequent_word\": f\"{most_frequent[0]} ({most_frequent[1]} times)\",\n",
        "        \"unique_words\": unique_words\n",
        "    }\n",
        "\n",
        "# Test the solution\n",
        "result = analyze_text_solution(sample_text)\n",
        "print(\"📊 Text Analysis Results:\")\n",
        "print(\"=\" * 30)\n",
        "for key, value in result.items():\n",
        "    print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
        "\n",
        "# Challenge: Try your function on the movie reviews dataset!\n",
        "print(f\"\\n🎬 Analyzing movie reviews...\")\n",
        "for i, review in enumerate(reviews_df['review'].head(3), 1):\n",
        "    print(f\"\\nReview {i}:\")\n",
        "    stats = analyze_text_solution(review)\n",
        "    print(f\"Words: {stats['word_count']}, Unique: {stats['unique_words']}, Avg length: {stats['average_word_length']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4D9ldOzatbc",
        "outputId": "c9d7fac2-c4eb-4081-ee6e-d81e00afd701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Text Analysis Results:\n",
            "==============================\n",
            "Word Count: 34\n",
            "Sentence Count: 3\n",
            "Average Word Length: 6.29\n",
            "Most Frequent Word: language (2 times)\n",
            "Unique Words: 30\n",
            "\n",
            "🎬 Analyzing movie reviews...\n",
            "\n",
            "Review 1:\n",
            "Words: 9, Unique: 9, Avg length: 6.0\n",
            "\n",
            "Review 2:\n",
            "Words: 8, Unique: 7, Avg length: 4.88\n",
            "\n",
            "Review 3:\n",
            "Words: 5, Unique: 5, Avg length: 9.8\n"
          ]
        }
      ]
    }
  ]
}