{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMf0bmmSRIx4coHWMIGTxkl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/appliedcode/mthree-c422/blob/mthree-422-salleh/Exercises/day-11/Prompt-categories/prompt_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìù Lab Student Exercises: Prompting Styles\n",
        "\n",
        "\n",
        "***\n",
        "\n",
        "## **Zero-Shot Prompting Exercises**\n",
        "\n",
        "**Objective:** Practice giving a model classification or extraction tasks **without** providing any examples.\n",
        "\n",
        "### Exercise Z1: News Headline Classification\n",
        "\n",
        "Replace the `texts` and `candidate_labels` in your zero-shot pipeline.\n",
        "\n",
        "```python\n",
        "texts = [\n",
        "    \"NASA launches new mission to search for exoplanets.\",\n",
        "    \"The city council approved new recycling rules yesterday.\",\n",
        "    \"Barcelona defeats Real Madrid in a thrilling match.\"\n",
        "]\n",
        "candidate_labels = [\"science\", \"sports\", \"politics\", \"environment\"]\n",
        "\n",
        "# Run using your zero-shot classification pipeline.\n",
        "```\n",
        "\n",
        "- **Task:** For each headline, write down which category you think it should be, then compare to the model.\n",
        "- **Reflect:** Did the model choose as you expected? Were any surprising?\n",
        "\n",
        "***\n",
        "\n",
        "### Exercise Z2: Sentiment Classification (Zero-Shot)\n",
        "\n",
        "```python\n",
        "texts = [\n",
        "    \"What a disappointing experience, nothing went right.\",\n",
        "    \"Absolutely loved the atmosphere and the food was perfect!\",\n",
        "    \"It was okay, nothing stood out as good or bad.\"\n",
        "]\n",
        "candidate_labels = [\"positive\", \"negative\", \"neutral\"]\n",
        "\n",
        "# Run using your zero-shot classification pipeline.\n",
        "```\n",
        "\n",
        "- **Task:** How does the model rate each? Are there any edge cases (\"It was okay\")?\n",
        "- **Bonus:** Try adding new candidate labels like \"mixed\" or \"not sure.\"\n",
        "\n",
        "***\n",
        "\n",
        "## **Few-Shot Prompting Exercises**\n",
        "\n",
        "**Objective:** Give a few examples in your prompt to help the model learn a pattern, then ask it to continue.\n",
        "\n",
        "### Exercise F1: Intent Classification (Few-Shot)\n",
        "\n",
        "Compose a text-generation prompt like:\n",
        "\n",
        "```python\n",
        "prompt = \"\"\"Classify each customer support inquiry as Order, Technical Issue, or Complaint.\n",
        "\n",
        "Inquiry: \"My package hasn't arrived and it's a week late.\"\n",
        "Intent: Complaint\n",
        "\n",
        "Inquiry: \"Can I change my delivery address after ordering?\"\n",
        "Intent: Order\n",
        "\n",
        "Inquiry: \"The app crashes when I try to log in.\"\n",
        "Intent: Technical Issue\n",
        "\n",
        "Inquiry: \"The product is broken, please help.\"\n",
        "Intent:\n",
        "\"\"\"\n",
        "response = generator(prompt, max_new_tokens=10, temperature=0.2)\n",
        "print(response[0]['generated_text'])\n",
        "```\n",
        "\n",
        "- **Task:** Add 1 or 2 more inquiries of your own and assess the model's outputs for those new items.\n",
        "\n",
        "***\n",
        "\n",
        "### Exercise F2: Movie Genre Classification (Few-Shot)\n",
        "\n",
        "Prepare a prompt such as:\n",
        "\n",
        "```python\n",
        "prompt = \"\"\"Classify the following movie plots as Comedy, Drama, or Action.\n",
        "\n",
        "Plot: \"A group of friends go on a hilarious road trip and get into wild situations.\"\n",
        "Genre: Comedy\n",
        "\n",
        "Plot: \"A detective teams up with a spy to stop an international heist.\"\n",
        "Genre: Action\n",
        "\n",
        "Plot: \"A family comes together during a difficult time to heal old wounds.\"\n",
        "Genre: Drama\n",
        "\n",
        "Plot: \"Two rival chefs compete to win the city's food festival.\"\n",
        "Genre:\n",
        "\"\"\"\n",
        "# Send to the text-generation pipeline\n",
        "```\n",
        "\n",
        "- **Bonus:** Try inventing your own plot and see if the model classifies it correctly!\n",
        "\n",
        "***\n",
        "\n",
        "## **Chain-of-Thought (Step-by-Step) Prompting Exercises**\n",
        "\n",
        "**Objective:** Guide the model to show its reasoning, not just answers. Use \"Let's think step by step.\"\n",
        "\n",
        "### Exercise C1: Math Word Problem (CoT)\n",
        "\n",
        "Continue this pattern with your own question.\n",
        "\n",
        "```python\n",
        "prompt = \"\"\"\n",
        "Q: There are 18 oranges. If 6 are eaten and then 4 more are bought, how many are there now?\n",
        "Let's think step by step.\n",
        "\n",
        "1. Start with 18 oranges.\n",
        "2. 6 are eaten: 18 - 6 = 12 oranges left.\n",
        "3. 4 more are bought: 12 + 4 = 16 oranges.\n",
        "\n",
        "Answer: 16\n",
        "\n",
        "Q: John has 24 candies. He gives 7 to his friend and then buys 5 more. How many candies does John have?\n",
        "Let's think step by step.\n",
        "\"\"\"\n",
        "response = generator(prompt, max_new_tokens=50, temperature=0)\n",
        "print(response[0]['generated_text'])\n",
        "```\n",
        "\n",
        "- **Task:** Write at least 1 new word problem and try to see if the model solves it step by step.\n",
        "\n",
        "***\n",
        "\n",
        "### Exercise C2: Everyday Reasoning (CoT)\n",
        "\n",
        "Try a real-life reasoning question. Prompt example:\n",
        "\n",
        "```python\n",
        "prompt = \"\"\"\n",
        "Q: Amy has 20 dollars. She spends 8 dollars on lunch and 5 dollars on a gift. How much money does she have left?\n",
        "Let's think step by step.\n",
        "\n",
        "1. Start with 20 dollars.\n",
        "2. Spend 8 dollars: 20 - 8 = 12 dollars left.\n",
        "3. Spend 5 dollars: 12 - 5 = 7 dollars left.\n",
        "\n",
        "Answer: 7\n",
        "\n",
        "Q: Ben has 15 chocolate bars. He shares 5 with his friends and then buys 3 more. How many does he have now?\n",
        "Let's think step by step.\n",
        "\"\"\"\n",
        "# Complete as above\n",
        "```\n",
        "\n",
        "- **Bonus:** Try a logic puzzle or \"If-Then\" reasoning and see if you can get stepwise explanations.\n",
        "\n",
        "***\n",
        "\n",
        "## **Reflection Questions**\n",
        "\n",
        "- Did step-by-step (CoT) prompting improve the model's explanation or accuracy?\n",
        "- In which scenario did few-shot examples matter most? Why?\n",
        "- What surprised you most about the zero-shot result?\n",
        "\n",
        "***\n",
        "\n",
        "## **Extra Challenge**\n",
        "\n",
        "- Swap out the genres, sentiment labels, or categories in any example and invent new test examples yourself. Experiment with the order of instructions and see what changes!\n",
        "\n",
        "***"
      ],
      "metadata": {
        "id": "47E21mRN8Hut"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qW4fZhL67hre"
      },
      "outputs": [],
      "source": []
    }
  ]
}