{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring Core NLP Concepts\n",
        "Welcome to this hands-on NLP Colab lab! You will work through key tasks—tokenization, POS tagging, stemming, stop-word filtering, vocabulary matching, lemmatization, dependency parsing, NER, and intent classification—using Python libraries. Follow the instructions and complete the exercises."
      ],
      "metadata": {
        "id": "sDeW9pllWu7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install --upgrade pip setuptools wheel -q\n",
        "!pip install --quiet nltk spacy textblob sklearn\n",
        "\n",
        "# Download NLTK data and spaCy model\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "!python -m spacy download en_core_web_sm -q\n"
      ],
      "metadata": {
        "id": "yMULvewKWrzm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06b6305c-6dfb-4851-dce6-e576813b385e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jedi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhcb0nFoiiCN",
        "outputId": "b8e96bb7-99a3-423c-a17c-b6bb4a58f020"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jedi\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi) (0.8.4)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi\n",
            "Successfully installed jedi-0.19.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob\n",
        "!pip install scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Tgn8RH0ixnK",
        "outputId": "85aa65d2-148a-468a-b2cb-9ad6695bc4f8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Tokenization\n",
        "# Goal: Split text into tokens (words and punctuation).\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "text = \"Natural Language Processing enables machines to understand human language.\"\n",
        "print(\"Sentences:\", sent_tokenize(text))\n",
        "print(\"Tokens:\", word_tokenize(text))"
      ],
      "metadata": {
        "id": "vNgJ7l_9Wzfo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "498c2072-7940-46ed-a8e4-c5b60cf0c5ae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences: ['Natural Language Processing enables machines to understand human language.']\n",
            "Tokens: ['Natural', 'Language', 'Processing', 'enables', 'machines', 'to', 'understand', 'human', 'language', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 1.1: Tokenize the following paragraph into words and sentences:\n",
        "\n",
        "paragraph = \"Machine learning models power many NLP tasks. They learn patterns from data!\"\n",
        "print(\"Sentences:\", sent_tokenize(paragraph))\n",
        "print(\"Tokens:\", word_tokenize(paragraph))\n"
      ],
      "metadata": {
        "id": "Iwanu3kzW3KU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1ad2b2f-320c-4d4a-c09e-5b4f4b9485b4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences: ['Machine learning models power many NLP tasks.', 'They learn patterns from data!']\n",
            "Tokens: ['Machine', 'learning', 'models', 'power', 'many', 'NLP', 'tasks', '.', 'They', 'learn', 'patterns', 'from', 'data', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Part-of-Speech Tagging\n",
        "# Goal: Assign grammatical tags to each token.\n",
        "import nltk\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(pos_tags)\n"
      ],
      "metadata": {
        "id": "VaJMjJLOXCV9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83436e99-1054-4251-9771-ed25db76ca07"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('enables', 'VBZ'), ('machines', 'NNS'), ('to', 'TO'), ('understand', 'VB'), ('human', 'JJ'), ('language', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2.1: Tag POS for tokens from your Exercise 1.1.\n",
        "tokens_1_1 = word_tokenize(paragraph)\n",
        "pos_tags_1_1 = nltk.pos_tag(tokens_1_1)\n",
        "print(pos_tags_1_1)"
      ],
      "metadata": {
        "id": "rjdfixgHXLAe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "665f4a75-0004-4a36-a069-b8e7217231b0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Machine', 'NN'), ('learning', 'NN'), ('models', 'NNS'), ('power', 'NN'), ('many', 'JJ'), ('NLP', 'NNP'), ('tasks', 'NNS'), ('.', '.'), ('They', 'PRP'), ('learn', 'VBP'), ('patterns', 'NNS'), ('from', 'IN'), ('data', 'NN'), ('!', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Stemming\n",
        "# Goal: Reduce words to their root forms (may be non-dictionary).\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "words = [\"running\", \"runs\", \"ran\", \"easily\", \"fairly\"]\n",
        "print({w: stemmer.stem(w) for w in words})\n"
      ],
      "metadata": {
        "id": "8Q_ASJ2qXMJ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "103fc595-8177-40c8-e785-1cee2b60f650"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'running': 'run', 'runs': 'run', 'ran': 'ran', 'easily': 'easili', 'fairly': 'fairli'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3.1: Stem the tokens from your Exercise 1.1.\n",
        "stemmed = {word: stemmer.stem(word) for word in tokens_1_1 if word.isalpha()}\n",
        "print(stemmed)"
      ],
      "metadata": {
        "id": "KpJNVgRhXTjF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6d12141-9781-4825-db22-dc98dfdd3d50"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Machine': 'machin', 'learning': 'learn', 'models': 'model', 'power': 'power', 'many': 'mani', 'NLP': 'nlp', 'tasks': 'task', 'They': 'they', 'learn': 'learn', 'patterns': 'pattern', 'from': 'from', 'data': 'data'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Stop-Word Filtering\n",
        "# Goal: Remove common, low-value words.\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = word_tokenize(text.lower())\n",
        "filtered = [w for w in tokens if w.isalpha() and w not in stop_words]\n",
        "print(filtered)"
      ],
      "metadata": {
        "id": "kBo5Lo3hXVPk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1e4b558-54c9-47a4-a119-62e299b61456"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natural', 'language', 'processing', 'enables', 'machines', 'understand', 'human', 'language']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 4.1: Filter stop words from your Exercise 1.1 tokens.\n",
        "tokens_lower = [w.lower() for w in tokens_1_1]\n",
        "filtered = [w for w in tokens_lower if w.isalpha() and w not in stop_words]\n",
        "print(filtered)"
      ],
      "metadata": {
        "id": "_CVnLYVWXakJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8519b423-03a2-4285-deaf-9c74dfd89944"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['machine', 'learning', 'models', 'power', 'many', 'nlp', 'tasks', 'learn', 'patterns', 'data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Vocabulary Matching\n",
        "# Goal: Check tokens against a predefined vocabulary.\n",
        "\n",
        "vocab = {\"natural\", \"language\", \"machine\", \"data\", \"processing\"}\n",
        "tokens = [w.lower() for w in word_tokenize(text)]\n",
        "in_vocab = [w for w in tokens if w.isalpha() and w in vocab]\n",
        "print(\"In-vocab tokens:\", in_vocab)\n",
        "print(\"OOV tokens:\", [w for w in tokens if w.isalpha() and w not in vocab])\n"
      ],
      "metadata": {
        "id": "IYEs9fy5XcVy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0105784-78f2-4ef2-f141-35ad635a40be"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In-vocab tokens: ['natural', 'language', 'processing', 'language']\n",
            "OOV tokens: ['enables', 'machines', 'to', 'understand', 'human']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 5.1: Define your own small vocabulary and classify tokens from Exercise 1.1 into in-vocab vs. out-of-vocab.\n",
        "vocab_custom = {'machine', 'learning', 'data', 'nlp', 'patterns'}\n",
        "in_vocab = [w for w in tokens_lower if w.isalpha() and w in vocab_custom]\n",
        "oov = [w for w in tokens_lower if w.isalpha() and w not in vocab_custom]\n",
        "print(\"In-vocab:\", in_vocab)\n",
        "print(\"OOV:\", oov)"
      ],
      "metadata": {
        "id": "YsKmsc4AXgXa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3827c1e-3970-4ac4-e41f-bddfd5da262f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In-vocab: ['machine', 'learning', 'nlp', 'patterns', 'data']\n",
            "OOV: ['models', 'power', 'many', 'tasks', 'they', 'learn', 'from']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Lemmatization\n",
        "# Goal: Convert words to their dictionary form.\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [\"running\", \"better\", \"wolves\"]\n",
        "print({w: lemmatizer.lemmatize(w) for w in words})\n",
        "# For verbs:\n",
        "print(\"run (verb):\", lemmatizer.lemmatize(\"running\", pos='v'))"
      ],
      "metadata": {
        "id": "RDcsQ-_CXjyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a4c60a2-2ad4-488e-bd6e-23db3362806d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'running': 'running', 'better': 'better', 'wolves': 'wolf'}\n",
            "run (verb): run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 6.1: Lemmatize tokens from Exercise 1.1 (both default and verb POS).\n",
        "lemmatized = {w: lemmatizer.lemmatize(w) for w in tokens_lower if w.isalpha()}\n",
        "lemmatized_verbs = {w: lemmatizer.lemmatize(w, pos='v') for w in tokens_lower if w.isalpha()}\n",
        "print(\"Default Lemmas:\", lemmatized)\n",
        "print(\"Verb Lemmas:\", lemmatized_verbs)"
      ],
      "metadata": {
        "id": "g74PrNQ0XqWx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6babc78-0bed-49d0-8594-02d0eea02149"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Default Lemmas: {'machine': 'machine', 'learning': 'learning', 'models': 'model', 'power': 'power', 'many': 'many', 'nlp': 'nlp', 'tasks': 'task', 'they': 'they', 'learn': 'learn', 'patterns': 'pattern', 'from': 'from', 'data': 'data'}\n",
            "Verb Lemmas: {'machine': 'machine', 'learning': 'learn', 'models': 'model', 'power': 'power', 'many': 'many', 'nlp': 'nlp', 'tasks': 'task', 'they': 'they', 'learn': 'learn', 'patterns': 'pattern', 'from': 'from', 'data': 'data'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Dependency Parsing\n",
        "# Goal: Identify syntactic relationships between tokens.\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "    print(token.text, token.dep_, token.head.text)\n"
      ],
      "metadata": {
        "id": "4drXo7zjXss9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efd20900-3e76-4117-9731-3c022c0fe498"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural compound Language\n",
            "Language compound Processing\n",
            "Processing nsubj enables\n",
            "enables ROOT enables\n",
            "machines nsubj understand\n",
            "to aux understand\n",
            "understand ccomp enables\n",
            "human amod language\n",
            "language dobj understand\n",
            ". punct enables\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 7.1: Parse the sentence “They learn patterns from data” and list each token’s dependency label and head.\n",
        "doc2 = nlp(\"They learn patterns from data\")\n",
        "for token in doc2:\n",
        "    print(token.text, token.dep_, token.head.text)"
      ],
      "metadata": {
        "id": "LmRo7ZwEXvGH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c90c9bc-efcd-4e8c-efd8-e7ad09e005da"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "They nsubj learn\n",
            "learn ROOT learn\n",
            "patterns dobj learn\n",
            "from prep patterns\n",
            "data pobj from\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Named-Entity Recognition (NER)\n",
        "# Goal: Extract real-world entities from text.\n",
        "doc = nlp(\"Google was founded in 1998 by Larry Page and Sergey Brin in California.\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "metadata": {
        "id": "k78PeFooXxwp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8f6d3cb-fe95-4f1f-a7eb-702d0143a9e4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google ORG\n",
            "1998 DATE\n",
            "Larry Page PERSON\n",
            "Sergey Brin PERSON\n",
            "California GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exercise 8.1: Run NER on this sentence and add at least two more sentences of your own.\n",
        "ner_text = \"\"\"Google was founded in 1998 by Larry Page and Sergey Brin in California.\n",
        "Apple Inc. is headquartered in Cupertino.\n",
        "Elon Musk leads SpaceX and Tesla.\"\"\"\n",
        "\n",
        "doc3 = nlp(ner_text)\n",
        "for ent in doc3.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "metadata": {
        "id": "bwBm0NMJX3WN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee82e7d4-2197-400b-f278-c277c770b2aa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google ORG\n",
            "1998 DATE\n",
            "Larry Page PERSON\n",
            "Sergey Brin PERSON\n",
            "California GPE\n",
            "Apple Inc. ORG\n",
            "Cupertino GPE\n",
            "Elon Musk PERSON\n",
            "SpaceX PERSON\n",
            "Tesla NORP\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}