{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "809aac6f",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on Social Media Posts (Tweets) â€” Complete Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3291ae13",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             classification_report, confusion_matrix, roc_curve, auc)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK data (run once)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# 1. Sample Dataset Creation\n",
    "# Example tweets with sentiment labels (Positive, Negative, Neutral)\n",
    "data = {\n",
    "    \"tweet\": [\n",
    "        \"Loving the new features in the latest update! #awesome ðŸ˜Š\",\n",
    "        \"Really tired of the delays and bad service. #frustrated ðŸ˜ \",\n",
    "        \"Just another day, nothing special happening.\",\n",
    "        \"The customer support was fantastic! #helpful\",\n",
    "        \"I hate when my phone battery dies so fast!!!\",\n",
    "        \"Meh, the event was okay, nothing great to share.\",\n",
    "        \"Had a wonderful day at the park, feeling great!\",\n",
    "        \"Worst experience ever. Not coming back again.\",\n",
    "        \"Itâ€™s fine I guess, could be better.\",\n",
    "        \"Amazing performance by the team! Proud fan here.\",\n",
    "    ],\n",
    "    \"sentiment\": [\n",
    "        \"Positive\",\n",
    "        \"Negative\",\n",
    "        \"Neutral\",\n",
    "        \"Positive\",\n",
    "        \"Negative\",\n",
    "        \"Neutral\",\n",
    "        \"Positive\",\n",
    "        \"Negative\",\n",
    "        \"Neutral\",\n",
    "        \"Positive\"\n",
    "    ]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 2. Preprocessing tailored for tweets\n",
    "class TweetPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = re.sub(r'http\\S+', '', text)  # remove urls\n",
    "        text = re.sub(r'@\\w+', '', text)     # remove mentions\n",
    "        text = re.sub(r'#', '', text)        # remove hashtag symbol only, keep the word\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # remove punctuation\n",
    "        text = re.sub(r'\\d+', '', text)      # remove digits\n",
    "        return text.strip()\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        text = self.clean_text(text)\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        tokens = [token for token in tokens if token not in self.stop_words and len(token) > 1]\n",
    "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "preprocessor = TweetPreprocessor()\n",
    "df['processed_tweet'] = df['tweet'].apply(preprocessor.preprocess)\n",
    "\n",
    "# 3. Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['sentiment_label'] = label_encoder.fit_transform(df['sentiment'])\n",
    "\n",
    "# 4. Train-Test Split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['processed_tweet'], df['sentiment_label'],\n",
    "    test_size=0.2, random_state=42, stratify=df['sentiment_label']\n",
    ")\n",
    "\n",
    "# 5. Model training and evaluation framework\n",
    "models = {\n",
    "    'MultinomialNB': MultinomialNB(),\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
    "    'SVM_Linear': SVC(kernel='linear', probability=True),\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'KNN': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=1000)\n",
    "\n",
    "evaluation_results = {}\n",
    "\n",
    "print(\"Model training and evaluation:\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Build pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', vectorizer),\n",
    "        ('clf', model)\n",
    "    ])\n",
    "    # Train\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    # Predict\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    report = classification_report(y_test, y_pred, target_names=label_encoder.classes_, zero_division=0)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    evaluation_results[name] = {\n",
    "        'pipeline': pipeline,\n",
    "        'accuracy': acc,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'f1_score': f1,\n",
    "        'classification_report': report,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{name} --- Accuracy: {acc:.3f}, Precision: {prec:.3f}, Recall: {rec:.3f}, F1-Score: {f1:.3f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "# 6. Visualization of Confusion Matrix for best model (example with LogisticRegression)\n",
    "best_model_name = 'LogisticRegression'\n",
    "best_result = evaluation_results[best_model_name]\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(best_result['confusion_matrix'], annot=True, fmt='d',\n",
    "            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_,\n",
    "            cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title(f'{best_model_name} Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# 7. ROC Curves and AUCs (One-vs-Rest)\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "y_test_bin = label_binarize(y_test, classes=[0,1,2])\n",
    "n_classes = y_test_bin.shape[1]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "for i, class_label in enumerate(label_encoder.classes_):\n",
    "    y_score = evaluation_results[best_model_name]['pipeline'].predict_proba(X_test)[:, i]\n",
    "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'{class_label} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0,1], [0,1], 'k--')\n",
    "plt.title(f'ROC Curves - {best_model_name}')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# 8. Prediction Confidence on New Tweets\n",
    "def predict_sentiment(text, model_pipeline, label_enc):\n",
    "    processed_text = preprocessor.preprocess(text)\n",
    "    proba = model_pipeline.predict_proba([processed_text])[0]\n",
    "    pred_idx = np.argmax(proba)\n",
    "    pred_label = label_enc.inverse_transform([pred_idx])[0]\n",
    "    confidence = proba[pred_idx]\n",
    "    return pred_label, confidence\n",
    "\n",
    "new_tweets = [\n",
    "    \"Can't wait for the concert tonight! So excited! #fun\",\n",
    "    \"So disappointed by the service call, waiting forever...\",\n",
    "    \"Nothing really special today, just chillin'.\"\n",
    "]\n",
    "\n",
    "for tweet in new_tweets:\n",
    "    sentiment, conf = predict_sentiment(tweet, evaluation_results[best_model_name]['pipeline'], label_encoder)\n",
    "    print(f\"Tweet: {tweet}\\nPredicted Sentiment: {sentiment} (Confidence: {conf:.2f})\\n\")\n",
    "\n",
    "# 9. Optional: Hyperparameter Tuning Example for Logistic Regression\n",
    "param_grid = {\n",
    "    'clf__C': [0.01, 0.1, 1, 10],\n",
    "    'clf__solver': ['lbfgs'],\n",
    "    'clf__max_iter': [1000]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    Pipeline([\n",
    "        ('vect', TfidfVectorizer(max_features=1000, ngram_range=(1,2))),\n",
    "        ('clf', LogisticRegression())\n",
    "    ]),\n",
    "    param_grid, cv=5, scoring='f1_macro', n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(df['processed_tweet'], df['sentiment_label'])\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "print(f\"Best cross-validated F1 macro: {grid_search.best_score_:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e60f33f",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "- We created a small sample dataset of tweets labeled with sentiment.\n",
    "\n",
    "- The preprocessing is customized for social media text, removing URLs, mentions, hashtags, and punctuation using TweetTokenizer and lemmatization.\n",
    "\n",
    "- We encode sentiment labels and split the data into train/test sets preserving label proportions.\n",
    "\n",
    "- Multiple classification models (Naive Bayes, Logistic Regression, SVM, Random Forest, KNN) are trained using TF-IDF vectors with unigrams and bigrams.\n",
    "\n",
    "- Each model is evaluated with accuracy, precision, recall, F1-score, and confusion matrix.\n",
    "\n",
    "- We visualize confusion matrices and ROC/AUC curves (for Logistic Regression).\n",
    "\n",
    "- A function for predicting sentiment on new tweets with confidence scores is provided.\n",
    "\n",
    "- Lastly, hyperparameter tuning via GridSearchCV is demonstrated for Logistic Regression."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
