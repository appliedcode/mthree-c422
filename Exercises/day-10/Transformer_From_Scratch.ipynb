{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuKux8/3tMFKnDNnScK6zd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/appliedcode/mthree-c422/blob/mthree-c422-dipti/Exercises/day-10/Transformer_From_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Problem Statement: Sentiment Classification on IMDb Movie Reviews Using BERT\n",
        "\n",
        "## Objective\n",
        "\n",
        "Build and fine-tune a Transformer-based model (BERT) to classify movie reviews from the IMDb dataset into positive or negative sentiment. This task involves data cleaning, tokenization, model training, evaluation, and analysis, following a similar pipeline demonstrated in the transformer tweet sentiment example.\n",
        "\n",
        "## Dataset\n",
        "\n",
        "**IMDb Movie Reviews**\n",
        "\n",
        "- Publicly accessible via the Hugging Face Datasets library (no manual download or sign-in required).\n",
        "- Loading code snippet:\n",
        "\n",
        "```python\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"imdb\")\n",
        "train = dataset[\"train\"]\n",
        "test = dataset[\"test\"]\n",
        "```\n",
        "\n",
        "- Dataset size: 25,000 training samples and 25,000 testing samples.\n",
        "- Structure: Each example contains a `text` field (the movie review) and a `label` field (0 = negative, 1 = positive).\n",
        "\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "- Clean and preprocess natural language movie reviews (remove HTML tags, special characters, unwanted whitespace).\n",
        "- Tokenize and encode text using `BertTokenizer`.\n",
        "- Fine-tune `BertForSequenceClassification` for binary sentiment classification.\n",
        "- Evaluate model with classification metrics (precision, recall, F1-score).\n",
        "- Analyze model predictions, including inspection of correctly and incorrectly classified samples.\n",
        "\n",
        "\n",
        "## Tasks\n",
        "\n",
        "1. **Data Loading \\& Exploration**\n",
        "    - Load the IMDb dataset directly using Hugging Faceâ€™s `load_dataset(\"imdb\")` function.\n",
        "    - Analyze dataset distribution and sample texts to understand the data.\n",
        "2. **Data Cleaning**\n",
        "    - Clean the review texts to remove noise such as HTML tags and punctuation.\n",
        "    - Prepare the cleaned text for tokenization.\n",
        "3. **Dataset Preparation**\n",
        "    - Implement a PyTorch Dataset class similar to `TweetDataset`, which performs tokenization, padding, and truncation using `BertTokenizer`.\n",
        "    - Ensure token sequences have a max length (e.g., 128) for efficient batching.\n",
        "4. **Model Setup and Training**\n",
        "    - Load the pretrained BERT base uncased model configured for sequence classification with two output labels.\n",
        "    - Define training parameters such as batch size, epochs, and logging setup.\n",
        "    - Use the Hugging Face `Trainer` API to train and validate the model on the IMDb data.\n",
        "5. **Evaluation and Reporting**\n",
        "    - Generate a detailed classification report with precision, recall, and F1-score.\n",
        "    - Create a DataFrame comparing review texts, actual labels, and predicted labels for sample inspection.\n",
        "\n",
        "## Deliverables\n",
        "\n",
        "- Python notebook or script containing fully documented code for the entire pipeline.\n",
        "- Classification report and insights into model performance and errors.\n",
        "- Examples of correct and incorrect predictions with analysis.\n",
        "\n",
        "\n",
        "## Getting Started Example\n",
        "\n",
        "```python\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load IMDb dataset\n",
        "dataset = load_dataset(\"imdb\")\n",
        "train = dataset[\"train\"]\n",
        "test = dataset[\"test\"]\n",
        "\n",
        "print(f\"Number of training samples: {len(train)}\")\n",
        "print(f\"Number of test samples: {len(test)}\")\n",
        "\n",
        "# Sample review and label\n",
        "print(\"Sample text:\", train[0][\"text\"][:200])\n",
        "print(\"Sample label:\", train[0][\"label\"])\n",
        "```\n",
        "\n",
        "\n",
        "***\n",
        "\n",
        "This problem statement ensures you use a reliable, easy-to-access dataset with no external sign-in or manual downloads, perfectly fitting into a Transformer fine-tuning workflow.\n"
      ],
      "metadata": {
        "id": "QabcG0bH1z9f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hmtG_7g21y9p"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    BertTokenizerFast,\n",
        "    BertForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        "    logging,\n",
        ")\n",
        "\n",
        "# Reduce transformers logging noise\n",
        "logging.set_verbosity_error()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Data Loading & Exploration\n",
        "\n",
        "def load_and_inspect():\n",
        "    dataset = load_dataset(\"imdb\")\n",
        "    train = dataset[\"train\"]\n",
        "    test = dataset[\"test\"]\n",
        "    print(f\"Train samples: {len(train)} | Test samples: {len(test)}\")\n",
        "    print(\"Sample text (first 300 chars):\\n\", train[0][\"text\"][:300])\n",
        "    print(\"Sample label:\", train[0][\"label\"])\n",
        "    # Distribution\n",
        "    print(\"Train label distribution:\\n\", train.features[\"label\"].names if hasattr(train, 'features') else None)\n",
        "    return train, test\n"
      ],
      "metadata": {
        "id": "3WP6FA0G2lLe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Basic cleaning: remove HTML tags, extra whitespace, weird control chars.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r\"<[^>]+>\", \" \", text)\n",
        "    # Replace newlines / tabs with space\n",
        "    text = re.sub(r\"[\\r\\n\\t]+\", \" \", text)\n",
        "    # Remove repeated spaces\n",
        "    text = re.sub(r\" +\", \" \", text)\n",
        "    # Strip\n",
        "    text = text.strip()\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "vritFYDk2pWc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Dataset Preparation\n",
        "\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, texts: List[str], labels: List[int], tokenizer: BertTokenizerFast, max_length: int = 128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = int(self.labels[idx])\n",
        "        # Tokenize + return torch tensors for input_ids, attention_mask\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding=False,  # padding will be handled by the data collator\n",
        "            return_tensors=None,\n",
        "        )\n",
        "        item = {k: torch.tensor(v) for k, v in encoding.items()}\n",
        "        item[\"labels\"] = torch.tensor(label, dtype=torch.long)\n",
        "        return item\n",
        "\n"
      ],
      "metadata": {
        "id": "9DQMwtSM2sqL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Model Setup and Training\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    preds = pred.predictions\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    y_pred = np.argmax(preds, axis=1)\n",
        "    y_true = pred.label_ids\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\")\n",
        "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "\n",
        "def train_and_evaluate(train_dataset, eval_dataset, tokenizer, output_dir=\"./imdb-bert-output\", num_train_epochs=2, batch_size=16):\n",
        "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "    args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    save_steps=500,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=100,\n",
        "    save_total_limit=2,\n",
        "    fp16=torch.cuda.is_available(),\n",
        ")\n",
        "\n",
        "\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    # Evaluate\n",
        "    metrics = trainer.evaluate(eval_dataset=eval_dataset)\n",
        "    print(\"Evaluation metrics:\\n\", metrics)\n",
        "\n",
        "    # Return trainer and model for further analysis\n",
        "    return trainer, trainer.model"
      ],
      "metadata": {
        "id": "0cjhQC6E2yFv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKWncaLi4I1p",
        "outputId": "426daa25-77db-43ff-de48-2a274b788383"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Full pipeline orchestration\n",
        "\n",
        "def run_pipeline(sample_limit=None, max_length=128, num_train_epochs=2, batch_size=16):\n",
        "    # Load\n",
        "    train_raw, test_raw = load_and_inspect()\n",
        "\n",
        "    # Optionally subsample for quick experiments\n",
        "    if sample_limit is not None:\n",
        "        train_raw = train_raw.select(range(min(sample_limit, len(train_raw))))\n",
        "        test_raw = test_raw.select(range(min(sample_limit, len(test_raw))))\n",
        "\n",
        "    # Clean texts\n",
        "    print(\"Cleaning texts...\")\n",
        "    train_texts = [clean_text(x) for x in train_raw[\"text\"]]\n",
        "    train_labels = train_raw[\"label\"]\n",
        "    test_texts = [clean_text(x) for x in test_raw[\"text\"]]\n",
        "    test_labels = test_raw[\"label\"]\n",
        "\n",
        "    # Tokenizer\n",
        "    tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    # Create Datasets\n",
        "    train_dataset = IMDBDataset(train_texts, train_labels, tokenizer, max_length=max_length)\n",
        "    test_dataset = IMDBDataset(test_texts, test_labels, tokenizer, max_length=max_length)\n",
        "\n",
        "    # Train\n",
        "    trainer, model = train_and_evaluate(\n",
        "        train_dataset,\n",
        "        test_dataset,\n",
        "        tokenizer,\n",
        "        output_dir=\"./imdb-bert-output\",\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "\n",
        "    # Predictions on test set\n",
        "    print(\"Generating predictions on test set...\")\n",
        "    preds_output = trainer.predict(test_dataset)\n",
        "    logits = preds_output.predictions\n",
        "    if isinstance(logits, tuple):\n",
        "        logits = logits[0]\n",
        "    y_pred = np.argmax(logits, axis=1)\n",
        "    y_true = preds_output.label_ids\n",
        "\n",
        "    # Classification report\n",
        "    report = classification_report(y_true, y_pred, target_names=[\"negative\", \"positive\"], digits=4)\n",
        "    print(\"\\nClassification Report:\\n\", report)\n",
        "\n",
        "    # Create a DataFrame for a small sample to inspect correct/incorrect predictions\n",
        "    df = pd.DataFrame({\"text\": test_texts, \"true_label\": y_true, \"pred_label\": y_pred})\n",
        "    df[\"correct\"] = df[\"true_label\"] == df[\"pred_label\"]\n",
        "\n",
        "    # Save full predictions to CSV\n",
        "    os.makedirs(\"outputs\", exist_ok=True)\n",
        "    df.to_csv(\"outputs/test_predictions.csv\", index=False)\n",
        "    print(\"Saved test predictions to outputs/test_predictions.csv\")\n",
        "\n",
        "    # Show some correctly and incorrectly classified samples\n",
        "    incorrect = df[~df[\"correct\"]].sample(n=min(10, df[~df[\"correct\"]].shape[0]), random_state=42)\n",
        "    correct = df[df[\"correct\"]].sample(n=min(10, df[df[\"correct\"]].shape[0]), random_state=42)\n",
        "\n",
        "    print(\"\\nExamples of incorrect predictions:\\n\")\n",
        "    for i, row in incorrect.iterrows():\n",
        "        print(f\"True: {row['true_label']} Pred: {row['pred_label']} Text: {row['text'][:300]}\\n---\\n\")\n",
        "\n",
        "    print(\"\\nExamples of correct predictions:\\n\")\n",
        "    for i, row in correct.iterrows():\n",
        "        print(f\"True: {row['true_label']} Pred: {row['pred_label']} Text: {row['text'][:300]}\\n---\\n\")\n",
        "\n",
        "    return trainer, model, df, report\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # To run quickly for testing, you can set sample_limit=2000\n",
        "    # For final training set sample_limit=None to use full dataset (25k)\n",
        "    trainer, model, df, report = run_pipeline(sample_limit=5000, max_length=128, num_train_epochs=2, batch_size=8)\n",
        "    print(\"Done.\\nCheck outputs/test_predictions.csv for detailed predictions and outputs/ for saved models in ./imdb-bert-output\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVvY6PEc293J",
        "outputId": "3003d2cf-6bf0-4c5c-8c11-3b5549b32a1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 25000 | Test samples: 25000\n",
            "Sample text (first 300 chars):\n",
            " I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really h\n",
            "Sample label: 0\n",
            "Train label distribution:\n",
            " ['neg', 'pos']\n",
            "Cleaning texts...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3432875986.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    }
  ]
}