{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkxpfCqIjiFe",
        "outputId": "044967fa-89a0-4875-8f61-b8a01d0822a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install --upgrade pip setuptools wheel -q\n",
        "!pip install --quiet nltk spacy textblob sklearn\n",
        "\n",
        "# Download NLTK data and spaCy model\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "!python -m spacy download en_core_web_sm -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Tokenization\n",
        "# Goal: Split text into tokens (words and punctuation).\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "text = \"Natural Language Processing enables machines to understand human language.\"\n",
        "print(\"Sentences:\", sent_tokenize(text))\n",
        "print(\"Tokens:\", word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vu0-9AWpj_ji",
        "outputId": "358cfa78-d002-49bf-921a-1fd6767ce9f1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences: ['Natural Language Processing enables machines to understand human language.']\n",
            "Tokens: ['Natural', 'Language', 'Processing', 'enables', 'machines', 'to', 'understand', 'human', 'language', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 1.1\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Paragraph to tokenize\n",
        "paragraph = \"Machine learning models power many NLP tasks. They learn patterns from data!\"\n",
        "\n",
        "# Sentence tokenization\n",
        "sentences = sent_tokenize(paragraph)\n",
        "\n",
        "# Word tokenization\n",
        "tokens = word_tokenize(paragraph)\n",
        "\n",
        "# Display results\n",
        "print(\"Original Paragraph:\\n\", paragraph)\n",
        "print(\"\\n -> Sentences:\")\n",
        "for i, sentence in enumerate(sentences, 1):\n",
        "    print(f\"{i}. {sentence}\")\n",
        "\n",
        "print(\"\\n -> Tokens:\")\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9zqAWivkbgU",
        "outputId": "751e4d2d-9f4b-422a-b275-3ec8f68dcbf3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Paragraph:\n",
            " Machine learning models power many NLP tasks. They learn patterns from data!\n",
            "\n",
            " -> Sentences:\n",
            "1. Machine learning models power many NLP tasks.\n",
            "2. They learn patterns from data!\n",
            "\n",
            " -> Tokens:\n",
            "['Machine', 'learning', 'models', 'power', 'many', 'NLP', 'tasks', '.', 'They', 'learn', 'patterns', 'from', 'data', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Part-of-Speech Tagging\n",
        "# Goal: Assign grammatical tags to each token.\n",
        "import nltk\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77Oaecojks8z",
        "outputId": "0866bb6d-0f81-499d-a793-a854d3e185e6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('enables', 'VBZ'), ('machines', 'NNS'), ('to', 'TO'), ('understand', 'VB'), ('human', 'JJ'), ('language', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2.1\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Paragraph from Exercise 1.1\n",
        "paragraph = \"Machine learning models power many NLP tasks. They learn patterns from data!\"\n",
        "\n",
        "# Tokenize the paragraph into words\n",
        "tokens = word_tokenize(paragraph)\n",
        "\n",
        "# Apply Part-of-Speech tagging\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "# Display POS tagged tokens\n",
        "print(\"POS Tags:\")\n",
        "for word, tag in pos_tags:\n",
        "    print(f\"{word:15} → {tag}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyd1If0ukzN8",
        "outputId": "ed7a97e7-fdfa-4c92-d236-f81fb60bed42"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags:\n",
            "Machine         → NN\n",
            "learning        → NN\n",
            "models          → NNS\n",
            "power           → NN\n",
            "many            → JJ\n",
            "NLP             → NNP\n",
            "tasks           → NNS\n",
            ".               → .\n",
            "They            → PRP\n",
            "learn           → VBP\n",
            "patterns        → NNS\n",
            "from            → IN\n",
            "data            → NN\n",
            "!               → .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Stemming\n",
        "# Goal: Reduce words to their root forms (may be non-dictionary).\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "words = [\"running\", \"runs\", \"ran\", \"easily\", \"fairly\"]\n",
        "print({w: stemmer.stem(w) for w in words})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WISqqBOplG8v",
        "outputId": "c9b77b8e-885a-4c6d-d1dc-196083cf6391"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'running': 'run', 'runs': 'run', 'ran': 'ran', 'easily': 'easili', 'fairly': 'fairli'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3.1\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Paragraph from Exercise 1.1\n",
        "paragraph = \"Machine learning models power many NLP tasks. They learn patterns from data!\"\n",
        "\n",
        "# Tokenize into words\n",
        "tokens = word_tokenize(paragraph)\n",
        "\n",
        "# Initialize stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Stem each token (excluding punctuation)\n",
        "stemmed_words = {word: stemmer.stem(word) for word in tokens if word.isalpha()}\n",
        "\n",
        "# Display results\n",
        "print(\"Stemming Results:\")\n",
        "for original, stemmed in stemmed_words.items():\n",
        "    print(f\"{original:12} → {stemmed}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCqbtQUVlhfc",
        "outputId": "08e45f07-446e-4a34-c11b-ed0b526949a1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming Results:\n",
            "Machine      → machin\n",
            "learning     → learn\n",
            "models       → model\n",
            "power        → power\n",
            "many         → mani\n",
            "NLP          → nlp\n",
            "tasks        → task\n",
            "They         → they\n",
            "learn        → learn\n",
            "patterns     → pattern\n",
            "from         → from\n",
            "data         → data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Stop-Word Filtering\n",
        "# Goal: Remove common, low-value words.\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = word_tokenize(text.lower())\n",
        "filtered = [w for w in tokens if w.isalpha() and w not in stop_words]\n",
        "print(filtered)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1Luyo7Ulu_S",
        "outputId": "30bfe97e-2828-4903-ba8b-2cf32a0ec008"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natural', 'language', 'processing', 'enables', 'machines', 'understand', 'human', 'language']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 4.1\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Paragraph from Exercise 1.1\n",
        "paragraph = \"Machine learning models power many NLP tasks. They learn patterns from data!\"\n",
        "\n",
        "# Tokenize and lowercase\n",
        "tokens = word_tokenize(paragraph.lower())\n",
        "\n",
        "# Load English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Filter out stopwords and non-alphabetic tokens\n",
        "filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "\n",
        "# Display results\n",
        "print(\"Filtered Tokens (no stop words):\")\n",
        "print(filtered_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mN-0RclKlyhz",
        "outputId": "c6a1330a-834c-40ff-f51d-790885a66c3a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Tokens (no stop words):\n",
            "['machine', 'learning', 'models', 'power', 'many', 'nlp', 'tasks', 'learn', 'patterns', 'data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Vocabulary Matching\n",
        "# Goal: Check tokens against a predefined vocabulary.\n",
        "\n",
        "vocab = {\"natural\", \"language\", \"machine\", \"data\", \"processing\"}\n",
        "tokens = [w.lower() for w in word_tokenize(text)]\n",
        "in_vocab = [w for w in tokens if w.isalpha() and w in vocab]\n",
        "print(\"In-vocab tokens:\", in_vocab)\n",
        "print(\"OOV tokens:\", [w for w in tokens if w.isalpha() and w not in vocab])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfBwkprIl5r5",
        "outputId": "718a09aa-2e54-4422-a2ac-db152b36072e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In-vocab tokens: ['natural', 'language', 'processing', 'language']\n",
            "OOV tokens: ['enables', 'machines', 'to', 'understand', 'human']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 5.1\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Paragraph from Exercise 1.1\n",
        "paragraph = \"Machine learning models power many NLP tasks. They learn patterns from data!\"\n",
        "\n",
        "# Define your own small vocabulary\n",
        "my_vocab = {\"machine\", \"learning\", \"data\", \"patterns\", \"models\", \"intelligence\"}\n",
        "\n",
        "# Tokenize and lowercase\n",
        "tokens = [w.lower() for w in word_tokenize(paragraph)]\n",
        "\n",
        "# Classify tokens\n",
        "in_vocab = [w for w in tokens if w.isalpha() and w in my_vocab]\n",
        "oov = [w for w in tokens if w.isalpha() and w not in my_vocab]\n",
        "\n",
        "# Display results\n",
        "print(\"In-vocab tokens:\", in_vocab)\n",
        "print(\"OOV tokens:\", oov)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4vvX42Sl7vM",
        "outputId": "227b0c69-04cd-40a2-c796-f7b4d9aff086"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In-vocab tokens: ['machine', 'learning', 'models', 'patterns', 'data']\n",
            "OOV tokens: ['power', 'many', 'nlp', 'tasks', 'they', 'learn', 'from']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Lemmatization\n",
        "# Goal: Convert words to their dictionary form.\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = [\"running\", \"better\", \"wolves\"]\n",
        "print({w: lemmatizer.lemmatize(w) for w in words})\n",
        "# For verbs:\n",
        "print(\"run (verb):\", lemmatizer.lemmatize(\"running\", pos='v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfCO-ExDmJMz",
        "outputId": "53ed8103-2448-4539-eb0c-cfdfddb4ca18"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'running': 'running', 'better': 'better', 'wolves': 'wolf'}\n",
            "run (verb): run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 6.1\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Paragraph from Exercise 1.1\n",
        "paragraph = \"Machine learning models power many NLP tasks. They learn patterns from data!\"\n",
        "\n",
        "# Tokenize and lowercase\n",
        "tokens = [w.lower() for w in word_tokenize(paragraph) if w.isalpha()]\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize using default (noun) and verb POS\n",
        "lemmas_default = {word: lemmatizer.lemmatize(word) for word in tokens}\n",
        "lemmas_verb = {word: lemmatizer.lemmatize(word, pos='v') for word in tokens}\n",
        "\n",
        "# Display results\n",
        "print(\"Lemmatization (Default - noun):\")\n",
        "for word, lemma in lemmas_default.items():\n",
        "    print(f\"{word:12} → {lemma}\")\n",
        "\n",
        "print(\"\\n Lemmatization (Verb POS):\")\n",
        "for word, lemma in lemmas_verb.items():\n",
        "    print(f\"{word:12} → {lemma}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXggMeEDmMM_",
        "outputId": "45a0e4b5-469a-41a8-80aa-dc2e03854c87"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatization (Default - noun):\n",
            "machine      → machine\n",
            "learning     → learning\n",
            "models       → model\n",
            "power        → power\n",
            "many         → many\n",
            "nlp          → nlp\n",
            "tasks        → task\n",
            "they         → they\n",
            "learn        → learn\n",
            "patterns     → pattern\n",
            "from         → from\n",
            "data         → data\n",
            "\n",
            " Lemmatization (Verb POS):\n",
            "machine      → machine\n",
            "learning     → learn\n",
            "models       → model\n",
            "power        → power\n",
            "many         → many\n",
            "nlp          → nlp\n",
            "tasks        → task\n",
            "they         → they\n",
            "learn        → learn\n",
            "patterns     → pattern\n",
            "from         → from\n",
            "data         → data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Dependency Parsing\n",
        "# Goal: Identify syntactic relationships between tokens.\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "    print(token.text, token.dep_, token.head.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F78TtjqzmX3U",
        "outputId": "cc4f017d-d080-45c0-c8f1-4e58ef00af60"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural compound Language\n",
            "Language compound Processing\n",
            "Processing nsubj enables\n",
            "enables ROOT enables\n",
            "machines nsubj understand\n",
            "to aux understand\n",
            "understand ccomp enables\n",
            "human amod language\n",
            "language dobj understand\n",
            ". punct enables\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 7.1\n",
        "import spacy\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sentence to parse\n",
        "sentence = \"They learn patterns from data\"\n",
        "\n",
        "# Process the sentence\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Print dependency information\n",
        "print(f\"{'Token':12} {'Dep':10} {'Head'}\")\n",
        "print(\"-\" * 30)\n",
        "for token in doc:\n",
        "    print(f\"{token.text:12} {token.dep_:10} {token.head.text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YK6mG7D6mam7",
        "outputId": "4d466308-b3ae-4698-dbf5-176736375205"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token        Dep        Head\n",
            "------------------------------\n",
            "They         nsubj      learn\n",
            "learn        ROOT       learn\n",
            "patterns     dobj       learn\n",
            "from         prep       patterns\n",
            "data         pobj       from\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Named-Entity Recognition (NER)\n",
        "# Goal: Extract real-world entities from text.\n",
        "doc = nlp(\"Google was founded in 1998 by Larry Page and Sergey Brin in California.\")\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxAtq1Ybmoyi",
        "outputId": "b2eac4b7-b87b-4780-a3c3-5a5fd5fe6e78"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google ORG\n",
            "1998 DATE\n",
            "Larry Page PERSON\n",
            "Sergey Brin PERSON\n",
            "California GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 8.1\n",
        "import spacy\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Text input with original + 2 custom sentences\n",
        "text = (\n",
        "    \"Google was founded in 1998 by Larry Page and Sergey Brin in California. \"\n",
        "    \"Apple Inc. launched the iPhone in 2007 during a keynote by Steve Jobs in San Francisco. \"\n",
        "    \"Barack Obama was elected President of the United States in 2008.\"\n",
        ")\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print named entities and their labels\n",
        "print(f\"{'Entity':30} {'Label'}\")\n",
        "print(\"-\" * 45)\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text:30} {ent.label_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2Kzk8zjmp4J",
        "outputId": "78d6760e-6fe2-44e7-af39-29190164ab09"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity                         Label\n",
            "---------------------------------------------\n",
            "Google                         ORG\n",
            "1998                           DATE\n",
            "Larry Page                     PERSON\n",
            "Sergey Brin                    PERSON\n",
            "California                     GPE\n",
            "Apple Inc.                     ORG\n",
            "iPhone                         ORG\n",
            "2007                           DATE\n",
            "Steve Jobs                     PERSON\n",
            "San Francisco                  GPE\n",
            "Barack Obama                   PERSON\n",
            "the United States              GPE\n",
            "2008                           DATE\n"
          ]
        }
      ]
    }
  ]
}