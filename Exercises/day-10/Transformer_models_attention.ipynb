{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "482a8a62",
   "metadata": {},
   "source": [
    "# Session 8: Transformer Models and Attention Mechanisms\n",
    "\n",
    "## üìö Learning Objectives\n",
    "By the end of this session, you will be able to:\n",
    "- Understand the architecture and components of Transformer models\n",
    "- Explain attention mechanisms and their role in NLP\n",
    "- Implement basic attention mechanisms from scratch\n",
    "- Use pre-trained Transformer models for various NLP tasks\n",
    "- Compare different Transformer architectures (BERT, GPT, T5)\n",
    "- Fine-tune Transformer models for specific applications\n",
    "\n",
    "## üéØ Session Overview\n",
    "1. **Introduction to Transformers** - Architecture and key innovations\n",
    "2. **Attention Mechanisms** - Self-attention and multi-head attention\n",
    "3. **Pre-trained Models** - BERT, GPT, T5, and others\n",
    "4. **Practical Implementation** - Using transformers library\n",
    "5. **Fine-tuning Techniques** - Task-specific adaptation\n",
    "6. **Performance Evaluation** - Benchmarking and optimization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b1c994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports for Transformer models and attention\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Transformers library\n",
    "try:\n",
    "    from transformers import (\n",
    "        AutoTokenizer, AutoModel, AutoModelForSequenceClassification,\n",
    "        BertTokenizer, BertModel, GPT2Tokenizer, GPT2Model,\n",
    "        T5Tokenizer, T5Model, pipeline\n",
    "    )\n",
    "    transformers_available = True\n",
    "    print(\"‚úÖ Transformers library loaded successfully\")\n",
    "except ImportError:\n",
    "    transformers_available = False\n",
    "    print(\"‚ùå Transformers library not available. Install with: pip install transformers\")\n",
    "\n",
    "# Other useful libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"üì¶ All imports completed!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5a80de",
   "metadata": {},
   "source": [
    "## Section 1: Understanding Transformer Architecture\n",
    "\n",
    "### 1.1 The Transformer Revolution\n",
    "\n",
    "**Key Innovations:**\n",
    "- **Self-Attention**: Allows the model to focus on different parts of the input\n",
    "- **Parallel Processing**: Unlike RNNs, Transformers can process all positions simultaneously\n",
    "- **Position Encoding**: Captures sequence order without recurrence\n",
    "- **Multi-Head Attention**: Multiple attention patterns in parallel\n",
    "\n",
    "### 1.2 Core Components\n",
    "1. **Encoder-Decoder Architecture** (Original Transformer)\n",
    "2. **Encoder-Only Models** (BERT-style)\n",
    "3. **Decoder-Only Models** (GPT-style)\n",
    "4. **Encoder-Decoder Models** (T5-style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3371a4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Attention Mechanism Implementation\n",
    "\n",
    "class SimpleAttention(nn.Module):\n",
    "    \"\"\"Simple attention mechanism implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim):\n",
    "        super(SimpleAttention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.attention = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, encoder_outputs):\n",
    "        # encoder_outputs shape: (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        # Calculate attention weights\n",
    "        attention_weights = self.attention(encoder_outputs)  # (batch_size, seq_len, 1)\n",
    "        attention_weights = F.softmax(attention_weights.squeeze(-1), dim=1)  # (batch_size, seq_len)\n",
    "        \n",
    "        # Apply attention weights\n",
    "        context_vector = torch.sum(encoder_outputs * attention_weights.unsqueeze(-1), dim=1)\n",
    "        \n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention mechanism\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.matmul(attention_weights, V)\\n        \\n        return context, attention_weights\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linear transformations and split into heads\n",
    "        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Apply attention\n",
    "        context, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # Final linear transformation\n",
    "        output = self.W_o(context)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Demonstrate attention mechanisms\n",
    "print(\"üîç ATTENTION MECHANISMS DEMONSTRATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create sample data\n",
    "batch_size = 2\n",
    "seq_length = 5\n",
    "hidden_dim = 8\n",
    "\n",
    "# Sample input embeddings\n",
    "sample_embeddings = torch.randn(batch_size, seq_length, hidden_dim)\n",
    "print(f\"Sample input shape: {sample_embeddings.shape}\")\n",
    "\n",
    "# Simple Attention\n",
    "simple_attention = SimpleAttention(hidden_dim)\n",
    "context_vector, attention_weights = simple_attention(sample_embeddings)\n",
    "\n",
    "print(f\"\\\\nüìä Simple Attention Results:\")\n",
    "print(f\"Context vector shape: {context_vector.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"Attention weights (first sequence): {attention_weights[0].detach().numpy()}\")\n",
    "\n",
    "# Multi-Head Attention\n",
    "num_heads = 4\n",
    "multi_head_attention = MultiHeadAttention(hidden_dim, num_heads)\n",
    "output, mh_attention_weights = multi_head_attention(sample_embeddings, sample_embeddings, sample_embeddings)\n",
    "\n",
    "print(f\"\\\\nüß† Multi-Head Attention Results:\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {mh_attention_weights.shape}\")\n",
    "\n",
    "# Visualize attention patterns\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Simple attention visualization\n",
    "axes[0].bar(range(seq_length), attention_weights[0].detach().numpy())\n",
    "axes[0].set_title('Simple Attention Weights')\n",
    "axes[0].set_xlabel('Position')\n",
    "axes[0].set_ylabel('Attention Weight')\n",
    "\n",
    "# Multi-head attention visualization (first head)\n",
    "im = axes[1].imshow(mh_attention_weights[0, 0].detach().numpy(), cmap='Blues')\n",
    "axes[1].set_title('Multi-Head Attention (Head 1)')\n",
    "axes[1].set_xlabel('Key Position')\n",
    "axes[1].set_ylabel('Query Position')\n",
    "plt.colorbar(im, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790a6031",
   "metadata": {},
   "source": [
    "## Section 2: Pre-trained Transformer Models\n",
    "\n",
    "### 2.1 Model Categories\n",
    "- **BERT (Bidirectional Encoder Representations from Transformers)**\n",
    "  - Encoder-only architecture\n",
    "  - Bidirectional context understanding\n",
    "  - Best for: Classification, NER, QA\n",
    "\n",
    "- **GPT (Generative Pre-trained Transformer)**\n",
    "  - Decoder-only architecture\n",
    "  - Autoregressive generation\n",
    "  - Best for: Text generation, completion\n",
    "\n",
    "- **T5 (Text-to-Text Transfer Transformer)**\n",
    "  - Encoder-decoder architecture\n",
    "  - Unified text-to-text framework\n",
    "  - Best for: Translation, summarization, QA\n",
    "\n",
    "### 2.2 Model Sizes and Variants\n",
    "- **Base models**: ~110M parameters\n",
    "- **Large models**: ~340M parameters\n",
    "- **XL/XXL models**: 1B+ parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312e08b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with Pre-trained Transformer Models\n",
    "\n",
    "def demonstrate_bert_usage():\n",
    "    \"\"\"Demonstrate BERT model usage for various tasks\"\"\"\n",
    "    if not transformers_available:\n",
    "        print(\"‚ùå Transformers library not available\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Load BERT model and tokenizer\n",
    "        model_name = \"bert-base-uncased\"\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        model = BertModel.from_pretrained(model_name)\n",
    "        \n",
    "        print(f\"‚úÖ Loaded BERT model: {model_name}\")\n",
    "        \n",
    "        # Sample texts for analysis\n",
    "        texts = [\n",
    "            \"The weather is beautiful today.\",\n",
    "            \"I love machine learning and artificial intelligence.\",\n",
    "            \"The stock market had a volatile day with significant price movements.\"\n",
    "        ]\n",
    "        \n",
    "        # Process texts and get embeddings\n",
    "        embeddings = []\n",
    "        attention_weights = []\n",
    "        \n",
    "        for text in texts:\n",
    "            # Tokenize\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            \n",
    "            # Get model outputs\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs, output_attentions=True)\n",
    "                \n",
    "                # Extract embeddings (use [CLS] token)\n",
    "                cls_embedding = outputs.last_hidden_state[0, 0, :].numpy()\n",
    "                embeddings.append(cls_embedding)\n",
    "                \n",
    "                # Extract attention weights from last layer, first head\n",
    "                attention = outputs.attentions[-1][0, 0, :, :].numpy()\n",
    "                attention_weights.append(attention)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\\\nüìä BERT Analysis Results:\")\n",
    "        print(f\"Embedding dimension: {len(embeddings[0])}\")\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            tokens = tokenizer.tokenize(text)\n",
    "            print(f\"\\\\nText {i+1}: {text}\")\n",
    "            print(f\"Tokens: {tokens}\")\n",
    "            print(f\"Embedding norm: {np.linalg.norm(embeddings[i]):.4f}\")\n",
    "        \n",
    "        # Visualize attention patterns\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        \n",
    "        for i in range(3):\n",
    "            tokens = tokenizer.tokenize(texts[i])\n",
    "            # Limit to actual token length for visualization\n",
    "            token_len = min(len(tokens), attention_weights[i].shape[0])\n",
    "            \n",
    "            im = axes[i].imshow(attention_weights[i][:token_len, :token_len], cmap='Blues')\n",
    "            axes[i].set_title(f'Attention Pattern {i+1}')\n",
    "            axes[i].set_xticks(range(token_len))\n",
    "            axes[i].set_yticks(range(token_len))\n",
    "            axes[i].set_xticklabels(tokens[:token_len], rotation=45)\n",
    "            axes[i].set_yticklabels(tokens[:token_len])\n",
    "            plt.colorbar(im, ax=axes[i])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return embeddings, attention_weights\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with BERT: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def demonstrate_gpt2_usage():\n",
    "    \"\"\"Demonstrate GPT-2 for text generation\"\"\"\n",
    "    if not transformers_available:\n",
    "        print(\"‚ùå Transformers library not available\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Load GPT-2 model\n",
    "        model_name = \"gpt2\"\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Set pad token (GPT-2 doesn't have one by default)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # Use pipeline for easier text generation\n",
    "        generator = pipeline('text-generation', \n",
    "                           model=model_name, \n",
    "                           tokenizer=tokenizer,\n",
    "                           max_length=100,\n",
    "                           num_return_sequences=2,\n",
    "                           temperature=0.7,\n",
    "                           pad_token_id=tokenizer.eos_token_id)\n",
    "        \n",
    "        print(f\"‚úÖ Loaded GPT-2 model: {model_name}\")\n",
    "        \n",
    "        # Sample prompts\n",
    "        prompts = [\n",
    "            \"Artificial intelligence will revolutionize\",\n",
    "            \"The future of natural language processing\",\n",
    "            \"Machine learning algorithms can help\"\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\\\nü§ñ GPT-2 Text Generation:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for prompt in prompts:\n",
    "            print(f\"\\\\nPrompt: '{prompt}'\")\n",
    "            print(\"-\" * 30)\n",
    "            \n",
    "            generations = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "            \n",
    "            for i, generation in enumerate(generations, 1):\n",
    "                generated_text = generation['generated_text']\n",
    "                new_text = generated_text[len(prompt):].strip()\n",
    "                print(f\"Generation {i}: {prompt}{new_text}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with GPT-2: {str(e)}\")\n",
    "\n",
    "def compare_model_outputs():\n",
    "    \"\"\"Compare outputs from different transformer models\"\"\"\n",
    "    if not transformers_available:\n",
    "        print(\"‚ùå Transformers library not available\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Sample text for comparison\n",
    "        text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "        \n",
    "        print(f\"\\\\nüîç Model Comparison for: '{text}'\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # BERT - for understanding/encoding\n",
    "        bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        bert_tokens = bert_tokenizer.tokenize(text)\n",
    "        print(f\"\\\\nBERT tokenization: {bert_tokens}\")\n",
    "        print(f\"BERT vocab size: {bert_tokenizer.vocab_size:,}\")\n",
    "        \n",
    "        # GPT-2 - for generation\n",
    "        gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "        gpt2_tokens = gpt2_tokenizer.tokenize(text)\n",
    "        print(f\"\\\\nGPT-2 tokenization: {gpt2_tokens}\")\n",
    "        print(f\"GPT-2 vocab size: {gpt2_tokenizer.vocab_size:,}\")\n",
    "        \n",
    "        # Compare tokenization differences\n",
    "        print(f\"\\\\nüìä Tokenization Comparison:\")\n",
    "        print(f\"BERT tokens: {len(bert_tokens)}\")\n",
    "        print(f\"GPT-2 tokens: {len(gpt2_tokens)}\")\n",
    "        \n",
    "        # Show token differences\n",
    "        max_len = max(len(bert_tokens), len(gpt2_tokens))\n",
    "        print(f\"\\\\nToken-by-token comparison:\")\n",
    "        for i in range(max_len):\n",
    "            bert_token = bert_tokens[i] if i < len(bert_tokens) else \"---\"\n",
    "            gpt2_token = gpt2_tokens[i] if i < len(gpt2_tokens) else \"---\"\n",
    "            print(f\"   {i+1:2d}: BERT='{bert_token:12s}' | GPT-2='{gpt2_token:12s}'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in model comparison: {str(e)}\")\n",
    "\n",
    "# Run demonstrations\n",
    "print(\"üöÄ TRANSFORMER MODELS IN ACTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# BERT demonstration\n",
    "print(\"\\\\n1Ô∏è‚É£ BERT Model Demonstration:\")\n",
    "bert_embeddings, bert_attention = demonstrate_bert_usage()\n",
    "\n",
    "# GPT-2 demonstration\n",
    "print(\"\\\\n2Ô∏è‚É£ GPT-2 Model Demonstration:\")\n",
    "demonstrate_gpt2_usage()\n",
    "\n",
    "# Model comparison\n",
    "print(\"\\\\n3Ô∏è‚É£ Model Comparison:\")\n",
    "compare_model_outputs()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
